<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Week 2 Lab | Biometry Lecture and Lab Notes</title>
  <meta name="description" content="4 Week 2 Lab | Biometry Lecture and Lab Notes" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Week 2 Lab | Biometry Lecture and Lab Notes" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Week 2 Lab | Biometry Lecture and Lab Notes" />
  
  
  

<meta name="author" content="Heather Lynch" />


<meta name="date" content="2020-01-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="week-2-lecture.html"/>
<link rel="next" href="week-3-lecture.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Biometry Lecture and Lab Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="week-1-lecture.html"><a href="week-1-lecture.html"><i class="fa fa-check"></i><b>1</b> Week 1 Lecture</a><ul>
<li class="chapter" data-level="1.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#reading-material"><i class="fa fa-check"></i><b>1.1</b> Reading Material</a></li>
<li class="chapter" data-level="1.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-outline"><i class="fa fa-check"></i><b>1.2</b> Basic Outline</a></li>
<li class="chapter" data-level="1.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#todays-agenda"><i class="fa fa-check"></i><b>1.3</b> Today’s Agenda</a></li>
<li class="chapter" data-level="1.4" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-probability-theory"><i class="fa fa-check"></i><b>1.4</b> Basic Probability Theory</a><ul>
<li class="chapter" data-level="1.4.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#intersection"><i class="fa fa-check"></i><b>1.4.1</b> Intersection</a></li>
<li class="chapter" data-level="1.4.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#union"><i class="fa fa-check"></i><b>1.4.2</b> Union</a></li>
<li class="chapter" data-level="1.4.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#complement"><i class="fa fa-check"></i><b>1.4.3</b> Complement:</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="week-1-lecture.html"><a href="week-1-lecture.html#multiple-events"><i class="fa fa-check"></i><b>1.5</b> Multiple events</a></li>
<li class="chapter" data-level="1.6" data-path="week-1-lecture.html"><a href="week-1-lecture.html#conditionals"><i class="fa fa-check"></i><b>1.6</b> Conditionals</a></li>
<li class="chapter" data-level="1.7" data-path="week-1-lecture.html"><a href="week-1-lecture.html#bayes-theorem"><i class="fa fa-check"></i><b>1.7</b> Bayes Theorem</a></li>
<li class="chapter" data-level="1.8" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-few-foundational-ideas"><i class="fa fa-check"></i><b>1.8</b> A few foundational ideas</a></li>
<li class="chapter" data-level="1.9" data-path="week-1-lecture.html"><a href="week-1-lecture.html#overview-of-univariate-distributions"><i class="fa fa-check"></i><b>1.9</b> Overview of Univariate Distributions</a></li>
<li class="chapter" data-level="1.10" data-path="week-1-lecture.html"><a href="week-1-lecture.html#what-can-you-ask-of-a-distribution"><i class="fa fa-check"></i><b>1.10</b> What can you ask of a distribution?</a><ul>
<li class="chapter" data-level="1.10.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#expected-value-of-a-random-variable"><i class="fa fa-check"></i><b>1.10.1</b> Expected Value of a Random Variable</a></li>
<li class="chapter" data-level="1.10.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#discrete-case"><i class="fa fa-check"></i><b>1.10.2</b> Discrete Case</a></li>
<li class="chapter" data-level="1.10.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#continuous-case"><i class="fa fa-check"></i><b>1.10.3</b> Continuous Case</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-brief-introduction-to-scientific-method"><i class="fa fa-check"></i><b>1.11</b> A Brief Introduction to Scientific Method</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="week-1-lab-handout.html"><a href="week-1-lab-handout.html"><i class="fa fa-check"></i><b>2</b> Week 1 Lab Handout</a><ul>
<li class="chapter" data-level="2.1" data-path="week-1-lab-handout.html"><a href="week-1-lab-handout.html#using-r-like-a-calculator"><i class="fa fa-check"></i><b>2.1</b> Using R like a calculator</a></li>
<li class="chapter" data-level="2.2" data-path="week-1-lab-handout.html"><a href="week-1-lab-handout.html#the-basic-data-structures-in-r"><i class="fa fa-check"></i><b>2.2</b> The basic data structures in R</a></li>
<li class="chapter" data-level="2.3" data-path="week-1-lab-handout.html"><a href="week-1-lab-handout.html#writing-functions-in-r"><i class="fa fa-check"></i><b>2.3</b> Writing functions in R</a></li>
<li class="chapter" data-level="2.4" data-path="week-1-lab-handout.html"><a href="week-1-lab-handout.html#writing-loops-and-ifelse"><i class="fa fa-check"></i><b>2.4</b> Writing loops and if/else</a></li>
<li class="chapter" data-level="2.5" data-path="week-1-lab-handout.html"><a href="week-1-lab-handout.html#a-short-diversion-bias-in-estimators"><i class="fa fa-check"></i><b>2.5</b> (A short diversion) Bias in estimators</a></li>
<li class="chapter" data-level="2.6" data-path="week-1-lab-handout.html"><a href="week-1-lab-handout.html#lesson-6-some-practice-writing-r-code"><i class="fa fa-check"></i><b>2.6</b> Lesson #6: Some practice writing R code</a></li>
<li class="chapter" data-level="2.7" data-path="week-1-lab-handout.html"><a href="week-1-lab-handout.html#a-few-final-notes"><i class="fa fa-check"></i><b>2.7</b> A few final notes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="week-2-lecture.html"><a href="week-2-lecture.html"><i class="fa fa-check"></i><b>3</b> Week 2 Lecture</a><ul>
<li class="chapter" data-level="3.1" data-path="week-2-lecture.html"><a href="week-2-lecture.html#hypothesis-testing-and-p-values"><i class="fa fa-check"></i><b>3.1</b> Hypothesis testing and p-values</a></li>
<li class="chapter" data-level="3.2" data-path="week-2-lecture.html"><a href="week-2-lecture.html#permutation-tests"><i class="fa fa-check"></i><b>3.2</b> Permutation tests</a></li>
<li class="chapter" data-level="3.3" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parameter-estimation"><i class="fa fa-check"></i><b>3.3</b> Parameter estimation</a></li>
<li class="chapter" data-level="3.4" data-path="week-2-lecture.html"><a href="week-2-lecture.html#method-1-non-parametric-bootstrap"><i class="fa fa-check"></i><b>3.4</b> Method #1: Non-parametric bootstrap</a></li>
<li class="chapter" data-level="3.5" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parametric-bootstrap"><i class="fa fa-check"></i><b>3.5</b> Parametric bootstrap</a></li>
<li class="chapter" data-level="3.6" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife"><i class="fa fa-check"></i><b>3.6</b> Jackknife</a></li>
<li class="chapter" data-level="3.7" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife-after-bootstrap"><i class="fa fa-check"></i><b>3.7</b> Jackknife-after-bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="week-2-lab.html"><a href="week-2-lab.html"><i class="fa fa-check"></i><b>4</b> Week 2 Lab</a><ul>
<li class="chapter" data-level="4.1" data-path="week-2-lab.html"><a href="week-2-lab.html#confidence-intervals"><i class="fa fa-check"></i><b>4.1</b> Confidence intervals</a></li>
<li class="chapter" data-level="4.2" data-path="week-2-lab.html"><a href="week-2-lab.html#testing-hypotheses-through-permutation"><i class="fa fa-check"></i><b>4.2</b> Testing hypotheses through permutation</a></li>
<li class="chapter" data-level="4.3" data-path="week-2-lab.html"><a href="week-2-lab.html#basics-of-bootstrap-and-jackknife"><i class="fa fa-check"></i><b>4.3</b> Basics of bootstrap and jackknife</a></li>
<li class="chapter" data-level="4.4" data-path="week-2-lab.html"><a href="week-2-lab.html#calculating-bias-and-standard-error"><i class="fa fa-check"></i><b>4.4</b> Calculating bias and standard error</a></li>
<li class="chapter" data-level="4.5" data-path="week-2-lab.html"><a href="week-2-lab.html#parametric-bootstrap-1"><i class="fa fa-check"></i><b>4.5</b> Parametric bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="week-3-lecture.html"><a href="week-3-lecture.html"><i class="fa fa-check"></i><b>5</b> Week 3 Lecture</a><ul>
<li class="chapter" data-level="5.1" data-path="week-3-lecture.html"><a href="week-3-lecture.html#overview-of-probability-distributions"><i class="fa fa-check"></i><b>5.1</b> Overview of probability distributions</a></li>
<li class="chapter" data-level="5.2" data-path="week-3-lecture.html"><a href="week-3-lecture.html#normal-gaussian-distribution"><i class="fa fa-check"></i><b>5.2</b> Normal (Gaussian) Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="week-3-lecture.html"><a href="week-3-lecture.html#standard-normal-distribution"><i class="fa fa-check"></i><b>5.3</b> Standard Normal Distribution</a></li>
<li class="chapter" data-level="5.4" data-path="week-3-lecture.html"><a href="week-3-lecture.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.4</b> Log-Normal Distribution</a></li>
<li class="chapter" data-level="5.5" data-path="week-3-lecture.html"><a href="week-3-lecture.html#intermission-central-limit-theorem"><i class="fa fa-check"></i><b>5.5</b> Intermission: Central Limit Theorem</a></li>
<li class="chapter" data-level="5.6" data-path="week-3-lecture.html"><a href="week-3-lecture.html#poisson-distribution"><i class="fa fa-check"></i><b>5.6</b> Poisson Distribution</a></li>
<li class="chapter" data-level="5.7" data-path="week-3-lecture.html"><a href="week-3-lecture.html#binomial-distribution"><i class="fa fa-check"></i><b>5.7</b> Binomial Distribution</a></li>
<li class="chapter" data-level="5.8" data-path="week-3-lecture.html"><a href="week-3-lecture.html#beta-distribution"><i class="fa fa-check"></i><b>5.8</b> Beta Distribution</a></li>
<li class="chapter" data-level="5.9" data-path="week-3-lecture.html"><a href="week-3-lecture.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9</b> Gamma Distribution</a></li>
<li class="chapter" data-level="5.10" data-path="week-3-lecture.html"><a href="week-3-lecture.html#some-additional-notes"><i class="fa fa-check"></i><b>5.10</b> Some additional notes:</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="week-3-lab-handout.html"><a href="week-3-lab-handout.html"><i class="fa fa-check"></i><b>6</b> Week 3 Lab Handout</a><ul>
<li class="chapter" data-level="6.1" data-path="week-3-lab-handout.html"><a href="week-3-lab-handout.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>6.1</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="6.2" data-path="week-3-lab-handout.html"><a href="week-3-lab-handout.html#exploring-the-univariate-distributions-with-r"><i class="fa fa-check"></i><b>6.2</b> Exploring the univariate distributions with R</a></li>
<li class="chapter" data-level="6.3" data-path="week-3-lab-handout.html"><a href="week-3-lab-handout.html#standard-deviation-vs.standard-error"><i class="fa fa-check"></i><b>6.3</b> Standard deviation vs. Standard error</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="week-4-lecture.html"><a href="week-4-lecture.html"><i class="fa fa-check"></i><b>7</b> Week 4 Lecture</a><ul>
<li class="chapter" data-level="7.1" data-path="week-4-lecture.html"><a href="week-4-lecture.html#short-digression-degrees-of-freedom"><i class="fa fa-check"></i><b>7.1</b> Short digression: Degrees of freedom</a></li>
<li class="chapter" data-level="7.2" data-path="week-4-lecture.html"><a href="week-4-lecture.html#t-distribution"><i class="fa fa-check"></i><b>7.2</b> t-distribution</a></li>
<li class="chapter" data-level="7.3" data-path="week-4-lecture.html"><a href="week-4-lecture.html#chi-squared-distribution"><i class="fa fa-check"></i><b>7.3</b> Chi-squared distribution</a></li>
<li class="chapter" data-level="7.4" data-path="week-4-lecture.html"><a href="week-4-lecture.html#f-distribution"><i class="fa fa-check"></i><b>7.4</b> F distribution</a></li>
<li class="chapter" data-level="7.5" data-path="week-4-lecture.html"><a href="week-4-lecture.html#parameter-estimation-1"><i class="fa fa-check"></i><b>7.5</b> Parameter estimation</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="week-4-lab.html"><a href="week-4-lab.html"><i class="fa fa-check"></i><b>8</b> Week 4 Lab</a></li>
<li class="chapter" data-level="9" data-path="week-5-lecture.html"><a href="week-5-lecture.html"><i class="fa fa-check"></i><b>9</b> Week 5 Lecture</a><ul>
<li class="chapter" data-level="9.1" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-single-sample-t-test"><i class="fa fa-check"></i><b>9.1</b> The single sample t test</a></li>
<li class="chapter" data-level="9.2" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-unpaired-two-sample-t-test"><i class="fa fa-check"></i><b>9.2</b> The unpaired two sample t test</a></li>
<li class="chapter" data-level="9.3" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-paired-two-sample-t-test"><i class="fa fa-check"></i><b>9.3</b> The paired two sample t test</a></li>
<li class="chapter" data-level="9.4" data-path="week-5-lecture.html"><a href="week-5-lecture.html#the-f-test"><i class="fa fa-check"></i><b>9.4</b> The F test</a></li>
<li class="chapter" data-level="9.5" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-proportions"><i class="fa fa-check"></i><b>9.5</b> Comparing two proportions</a></li>
<li class="chapter" data-level="9.6" data-path="week-5-lecture.html"><a href="week-5-lecture.html#comparing-two-distributions"><i class="fa fa-check"></i><b>9.6</b> Comparing two distributions</a></li>
<li class="chapter" data-level="9.7" data-path="week-5-lecture.html"><a href="week-5-lecture.html#a-bit-more-detail-on-the-binomial"><i class="fa fa-check"></i><b>9.7</b> A bit more detail on the Binomial</a></li>
<li class="chapter" data-level="9.8" data-path="week-5-lecture.html"><a href="week-5-lecture.html#side-note-about-the-wald-test"><i class="fa fa-check"></i><b>9.8</b> Side-note about the Wald test</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="week-5-lab.html"><a href="week-5-lab.html"><i class="fa fa-check"></i><b>10</b> Week 5 Lab</a><ul>
<li class="chapter" data-level="10.1" data-path="week-5-lab.html"><a href="week-5-lab.html#f-test"><i class="fa fa-check"></i><b>10.1</b> F-test</a></li>
<li class="chapter" data-level="10.2" data-path="week-5-lab.html"><a href="week-5-lab.html#comparing-two-proportions-1"><i class="fa fa-check"></i><b>10.2</b> Comparing two proportions</a></li>
<li class="chapter" data-level="10.3" data-path="week-5-lab.html"><a href="week-5-lab.html#comparing-two-distributions-1"><i class="fa fa-check"></i><b>10.3</b> Comparing two distributions</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="week-6-lecture.html"><a href="week-6-lecture.html"><i class="fa fa-check"></i><b>11</b> Week 6 Lecture</a></li>
<li class="chapter" data-level="12" data-path="week-6-lab.html"><a href="week-6-lab.html"><i class="fa fa-check"></i><b>12</b> Week 6 Lab</a></li>
<li class="chapter" data-level="13" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html"><i class="fa fa-check"></i><b>13</b> Week 7 Lecture/Lab</a><ul>
<li class="chapter" data-level="13.1" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#part-i-box-plots"><i class="fa fa-check"></i><b>13.1</b> PART I: Box plots</a></li>
<li class="chapter" data-level="13.2" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#part-ii-two-dimensional-data"><i class="fa fa-check"></i><b>13.2</b> PART II: Two-dimensional data</a></li>
<li class="chapter" data-level="13.3" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#part-iii-three-dimensional-data"><i class="fa fa-check"></i><b>13.3</b> PART III: Three-dimensional data</a></li>
<li class="chapter" data-level="13.4" data-path="week-7-lecturelab.html"><a href="week-7-lecturelab.html#part-iv-multiple-plots"><i class="fa fa-check"></i><b>13.4</b> PART IV: Multiple plots</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="week-8-lecture.html"><a href="week-8-lecture.html"><i class="fa fa-check"></i><b>14</b> Week 8 Lecture</a><ul>
<li class="chapter" data-level="14.1" data-path="week-8-lecture.html"><a href="week-8-lecture.html#the-aims-of-modelling-a-discussion-of-shmueli-2010"><i class="fa fa-check"></i><b>14.1</b> The aims of modelling – A discussion of Shmueli (2010)</a></li>
<li class="chapter" data-level="14.2" data-path="week-8-lecture.html"><a href="week-8-lecture.html#introduction-to-linear-models"><i class="fa fa-check"></i><b>14.2</b> Introduction to linear models</a></li>
<li class="chapter" data-level="14.3" data-path="week-8-lecture.html"><a href="week-8-lecture.html#linear-models-example-with-continuous-covariate"><i class="fa fa-check"></i><b>14.3</b> Linear models | example with continuous covariate</a></li>
<li class="chapter" data-level="14.4" data-path="week-8-lecture.html"><a href="week-8-lecture.html#resolving-overparameterization-using-contrasts"><i class="fa fa-check"></i><b>14.4</b> Resolving overparameterization using contrasts</a></li>
<li class="chapter" data-level="14.5" data-path="week-8-lecture.html"><a href="week-8-lecture.html#effect-codingtreatment-constrast"><i class="fa fa-check"></i><b>14.5</b> Effect coding/Treatment constrast</a></li>
<li class="chapter" data-level="14.6" data-path="week-8-lecture.html"><a href="week-8-lecture.html#helmert-contrasts"><i class="fa fa-check"></i><b>14.6</b> Helmert contrasts</a></li>
<li class="chapter" data-level="14.7" data-path="week-8-lecture.html"><a href="week-8-lecture.html#sum-to-zero-contrasts"><i class="fa fa-check"></i><b>14.7</b> Sum-to-zero contrasts</a></li>
<li class="chapter" data-level="14.8" data-path="week-8-lecture.html"><a href="week-8-lecture.html#polynomial-contrasts"><i class="fa fa-check"></i><b>14.8</b> Polynomial contrasts</a></li>
<li class="chapter" data-level="14.9" data-path="week-8-lecture.html"><a href="week-8-lecture.html#visualizing-hypotheses-for-different-coding-schemes"><i class="fa fa-check"></i><b>14.9</b> Visualizing hypotheses for different coding schemes</a></li>
<li class="chapter" data-level="14.10" data-path="week-8-lecture.html"><a href="week-8-lecture.html#orthogonal-vs.non-orthogonal-contrasts"><i class="fa fa-check"></i><b>14.10</b> Orthogonal vs. Non-orthogonal contrasts</a></li>
<li class="chapter" data-level="14.11" data-path="week-8-lecture.html"><a href="week-8-lecture.html#error-structure-of-linear-models"><i class="fa fa-check"></i><b>14.11</b> Error structure of linear models</a><ul>
<li class="chapter" data-level="14.11.1" data-path="week-8-lecture.html"><a href="week-8-lecture.html#independence-of-errors"><i class="fa fa-check"></i><b>14.11.1</b> Independence of errors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="week-8-lab.html"><a href="week-8-lab.html"><i class="fa fa-check"></i><b>15</b> Week 8 Lab</a><ul>
<li class="chapter" data-level="15.1" data-path="week-8-lab.html"><a href="week-8-lab.html#contrasts"><i class="fa fa-check"></i><b>15.1</b> Contrasts</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="week-9-lecture.html"><a href="week-9-lecture.html"><i class="fa fa-check"></i><b>16</b> Week 9 Lecture</a><ul>
<li class="chapter" data-level="16.1" data-path="week-9-lecture.html"><a href="week-9-lecture.html#correlation"><i class="fa fa-check"></i><b>16.1</b> Correlation</a></li>
<li class="chapter" data-level="16.2" data-path="week-9-lecture.html"><a href="week-9-lecture.html#hypothesis-testing---pearsons-r"><i class="fa fa-check"></i><b>16.2</b> Hypothesis testing - Pearson’s <em>r</em></a></li>
<li class="chapter" data-level="16.3" data-path="week-9-lecture.html"><a href="week-9-lecture.html#fishers-z"><i class="fa fa-check"></i><b>16.3</b> Fisher’s <span class="math inline">\(z\)</span></a></li>
<li class="chapter" data-level="16.4" data-path="week-9-lecture.html"><a href="week-9-lecture.html#regression"><i class="fa fa-check"></i><b>16.4</b> Regression</a><ul>
<li class="chapter" data-level="16.4.1" data-path="week-9-lecture.html"><a href="week-9-lecture.html#assumptions-of-regression"><i class="fa fa-check"></i><b>16.4.1</b> Assumptions of regression:</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="week-9-lecture.html"><a href="week-9-lecture.html#confidence-vs.prediction-intervals"><i class="fa fa-check"></i><b>16.5</b> Confidence vs. Prediction intervals</a></li>
<li class="chapter" data-level="16.6" data-path="week-9-lecture.html"><a href="week-9-lecture.html#how-do-we-know-if-our-model-is-any-good"><i class="fa fa-check"></i><b>16.6</b> How do we know if our model is any good?</a></li>
<li class="chapter" data-level="16.7" data-path="week-9-lecture.html"><a href="week-9-lecture.html#robust-regression"><i class="fa fa-check"></i><b>16.7</b> Robust regression</a></li>
<li class="chapter" data-level="16.8" data-path="week-9-lecture.html"><a href="week-9-lecture.html#robust-regression-1"><i class="fa fa-check"></i><b>16.8</b> Robust regression</a></li>
<li class="chapter" data-level="16.9" data-path="week-9-lecture.html"><a href="week-9-lecture.html#type-i-and-type-ii-regression"><i class="fa fa-check"></i><b>16.9</b> Type I and Type II Regression</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="week-9-lab.html"><a href="week-9-lab.html"><i class="fa fa-check"></i><b>17</b> Week 9 Lab</a></li>
<li class="chapter" data-level="18" data-path="week-10-lecture.html"><a href="week-10-lecture.html"><i class="fa fa-check"></i><b>18</b> Week 10 Lecture</a><ul>
<li class="chapter" data-level="18.1" data-path="week-10-lecture.html"><a href="week-10-lecture.html#an-example"><i class="fa fa-check"></i><b>18.1</b> An example</a></li>
<li class="chapter" data-level="18.2" data-path="week-10-lecture.html"><a href="week-10-lecture.html#generalized-linear-models"><i class="fa fa-check"></i><b>18.2</b> Generalized linear models</a></li>
<li class="chapter" data-level="18.3" data-path="week-10-lecture.html"><a href="week-10-lecture.html#logistic-regression"><i class="fa fa-check"></i><b>18.3</b> Logistic regression</a></li>
<li class="chapter" data-level="18.4" data-path="week-10-lecture.html"><a href="week-10-lecture.html#fitting-a-glm"><i class="fa fa-check"></i><b>18.4</b> Fitting a GLM</a></li>
<li class="chapter" data-level="18.5" data-path="week-10-lecture.html"><a href="week-10-lecture.html#poisson-regression"><i class="fa fa-check"></i><b>18.5</b> Poisson regression</a></li>
<li class="chapter" data-level="18.6" data-path="week-10-lecture.html"><a href="week-10-lecture.html#deviance"><i class="fa fa-check"></i><b>18.6</b> Deviance</a></li>
<li class="chapter" data-level="18.7" data-path="week-10-lecture.html"><a href="week-10-lecture.html#other-methods-loess-splines-gams"><i class="fa fa-check"></i><b>18.7</b> Other methods – LOESS, splines, GAMs</a><ul>
<li class="chapter" data-level="18.7.1" data-path="week-10-lecture.html"><a href="week-10-lecture.html#loess"><i class="fa fa-check"></i><b>18.7.1</b> LOESS</a></li>
<li class="chapter" data-level="18.7.2" data-path="week-10-lecture.html"><a href="week-10-lecture.html#splines"><i class="fa fa-check"></i><b>18.7.2</b> Splines</a></li>
<li class="chapter" data-level="18.7.3" data-path="week-10-lecture.html"><a href="week-10-lecture.html#gams"><i class="fa fa-check"></i><b>18.7.3</b> GAMs</a></li>
<li class="chapter" data-level="18.7.4" data-path="week-10-lecture.html"><a href="week-10-lecture.html#multiple-regression"><i class="fa fa-check"></i><b>18.7.4</b> Multiple regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="week-10-lab.html"><a href="week-10-lab.html"><i class="fa fa-check"></i><b>19</b> Week 10 Lab</a><ul>
<li class="chapter" data-level="19.1" data-path="week-10-lab.html"><a href="week-10-lab.html#discussion-of-challenger-analysis"><i class="fa fa-check"></i><b>19.1</b> Discussion of Challenger analysis</a><ul>
<li class="chapter" data-level="19.1.1" data-path="week-10-lab.html"><a href="week-10-lab.html#practice-fitting-models"><i class="fa fa-check"></i><b>19.1.1</b> Practice fitting models</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="week-10-lab.html"><a href="week-10-lab.html#weighted-linear-regression"><i class="fa fa-check"></i><b>19.2</b> Weighted linear regression</a></li>
<li class="chapter" data-level="19.3" data-path="week-10-lab.html"><a href="week-10-lab.html#logistic-regression-1"><i class="fa fa-check"></i><b>19.3</b> Logistic regression</a></li>
<li class="chapter" data-level="19.4" data-path="week-10-lab.html"><a href="week-10-lab.html#poisson-regression-1"><i class="fa fa-check"></i><b>19.4</b> Poisson regression</a></li>
<li class="chapter" data-level="19.5" data-path="week-10-lab.html"><a href="week-10-lab.html#getting-a-feel-for-deviance"><i class="fa fa-check"></i><b>19.5</b> Getting a feel for Deviance</a></li>
<li class="chapter" data-level="19.6" data-path="week-10-lab.html"><a href="week-10-lab.html#generalized-additive-models"><i class="fa fa-check"></i><b>19.6</b> Generalized Additive Models</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="week-11-lecture.html"><a href="week-11-lecture.html"><i class="fa fa-check"></i><b>20</b> Week 11 Lecture</a><ul>
<li class="chapter" data-level="20.0.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#variation-within-treatment-group"><i class="fa fa-check"></i><b>20.0.1</b> Variation within treatment group</a></li>
<li class="chapter" data-level="20.0.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#variation-among-treatment-group-means"><i class="fa fa-check"></i><b>20.0.2</b> Variation among treatment group means</a></li>
<li class="chapter" data-level="20.0.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#comparing-variance-components"><i class="fa fa-check"></i><b>20.0.3</b> Comparing variance components</a></li>
<li class="chapter" data-level="20.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#comparing-variance-components-1"><i class="fa fa-check"></i><b>20.1</b> Comparing variance components</a></li>
<li class="chapter" data-level="20.2" data-path="week-11-lecture.html"><a href="week-11-lecture.html#two-ways-to-estimate-variance"><i class="fa fa-check"></i><b>20.2</b> Two ways to estimate variance</a></li>
<li class="chapter" data-level="20.3" data-path="week-11-lecture.html"><a href="week-11-lecture.html#single-factor-anova"><i class="fa fa-check"></i><b>20.3</b> Single-factor ANOVA</a></li>
<li class="chapter" data-level="20.4" data-path="week-11-lecture.html"><a href="week-11-lecture.html#fixed-effects-vs.random-effects"><i class="fa fa-check"></i><b>20.4</b> Fixed effects vs. random effects</a></li>
<li class="chapter" data-level="20.5" data-path="week-11-lecture.html"><a href="week-11-lecture.html#post-hoc-tests"><i class="fa fa-check"></i><b>20.5</b> Post-hoc tests</a><ul>
<li class="chapter" data-level="20.5.1" data-path="week-11-lecture.html"><a href="week-11-lecture.html#tukeys-hsd"><i class="fa fa-check"></i><b>20.5.1</b> Tukey’s HSD</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="week-11-lab.html"><a href="week-11-lab.html"><i class="fa fa-check"></i><b>21</b> Week 11 Lab</a><ul>
<li class="chapter" data-level="21.1" data-path="week-11-lab.html"><a href="week-11-lab.html#rs-anova-functions"><i class="fa fa-check"></i><b>21.1</b> R’s ANOVA functions</a></li>
<li class="chapter" data-level="21.2" data-path="week-11-lab.html"><a href="week-11-lab.html#single-factor-anova-1"><i class="fa fa-check"></i><b>21.2</b> Single-factor ANOVA</a></li>
<li class="chapter" data-level="21.3" data-path="week-11-lab.html"><a href="week-11-lab.html#follow-up-analyses-to-anova"><i class="fa fa-check"></i><b>21.3</b> Follow up analyses to ANOVA</a></li>
<li class="chapter" data-level="21.4" data-path="week-11-lab.html"><a href="week-11-lab.html#more-practice-model-i-anova"><i class="fa fa-check"></i><b>21.4</b> More practice: Model I ANOVA</a></li>
<li class="chapter" data-level="21.5" data-path="week-11-lab.html"><a href="week-11-lab.html#more-practice-brief-intro-to-doing-model-ii-anova-in-r"><i class="fa fa-check"></i><b>21.5</b> More practice: Brief intro to doing Model II ANOVA in R</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="week-12-lecture.html"><a href="week-12-lecture.html"><i class="fa fa-check"></i><b>22</b> Week 12 Lecture</a><ul>
<li class="chapter" data-level="22.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#review-anova-with-one-factor"><i class="fa fa-check"></i><b>22.1</b> Review: ANOVA with one factor</a></li>
<li class="chapter" data-level="22.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#anova-with-more-than-one-factor"><i class="fa fa-check"></i><b>22.2</b> ANOVA with more than one factor</a></li>
<li class="chapter" data-level="22.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#two-way-anova-factorial-designs"><i class="fa fa-check"></i><b>22.3</b> Two-way ANOVA factorial designs</a></li>
<li class="chapter" data-level="22.4" data-path="week-12-lecture.html"><a href="week-12-lecture.html#why-bother-with-random-effects"><i class="fa fa-check"></i><b>22.4</b> Why bother with random effects?</a></li>
<li class="chapter" data-level="22.5" data-path="week-12-lecture.html"><a href="week-12-lecture.html#mixed-models"><i class="fa fa-check"></i><b>22.5</b> Mixed models</a></li>
<li class="chapter" data-level="22.6" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-designs"><i class="fa fa-check"></i><b>22.6</b> Unbalanced designs</a></li>
<li class="chapter" data-level="22.7" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-design-different-sample-sizes"><i class="fa fa-check"></i><b>22.7</b> Unbalanced design – Different sample sizes</a><ul>
<li class="chapter" data-level="22.7.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-i-sequential-sums-of-squares"><i class="fa fa-check"></i><b>22.7.1</b> Type I (sequential) sums of squares</a></li>
<li class="chapter" data-level="22.7.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#type-iii-marginal-sums-of-squares"><i class="fa fa-check"></i><b>22.7.2</b> Type III (marginal) sums of squares</a></li>
<li class="chapter" data-level="22.7.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#comparing-type-i-ii-and-iii-ss"><i class="fa fa-check"></i><b>22.7.3</b> Comparing type I, II, and III SS</a></li>
</ul></li>
<li class="chapter" data-level="22.8" data-path="week-12-lecture.html"><a href="week-12-lecture.html#unbalanced-design-missing-cells"><i class="fa fa-check"></i><b>22.8</b> Unbalanced design – Missing cells</a></li>
<li class="chapter" data-level="22.9" data-path="week-12-lecture.html"><a href="week-12-lecture.html#two-factor-nested-anova"><i class="fa fa-check"></i><b>22.9</b> Two factor nested ANOVA</a><ul>
<li class="chapter" data-level="22.9.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#potential-issues-with-nested-designs"><i class="fa fa-check"></i><b>22.9.1</b> Potential issues with nested designs</a></li>
</ul></li>
<li class="chapter" data-level="22.10" data-path="week-12-lecture.html"><a href="week-12-lecture.html#experimental-design"><i class="fa fa-check"></i><b>22.10</b> Experimental design</a><ul>
<li class="chapter" data-level="22.10.1" data-path="week-12-lecture.html"><a href="week-12-lecture.html#completely-randomized-design"><i class="fa fa-check"></i><b>22.10.1</b> Completely randomized design</a></li>
<li class="chapter" data-level="22.10.2" data-path="week-12-lecture.html"><a href="week-12-lecture.html#randomized-block-design"><i class="fa fa-check"></i><b>22.10.2</b> Randomized block design</a></li>
<li class="chapter" data-level="22.10.3" data-path="week-12-lecture.html"><a href="week-12-lecture.html#latin-square-design"><i class="fa fa-check"></i><b>22.10.3</b> Latin square design</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="week-12-lab.html"><a href="week-12-lab.html"><i class="fa fa-check"></i><b>23</b> Week 12 Lab</a><ul>
<li class="chapter" data-level="23.1" data-path="week-12-lab.html"><a href="week-12-lab.html#example-1-two-way-factorial-anova-in-r"><i class="fa fa-check"></i><b>23.1</b> Example #1: Two-way factorial ANOVA in R</a></li>
<li class="chapter" data-level="23.2" data-path="week-12-lab.html"><a href="week-12-lab.html#example-2-nested-design"><i class="fa fa-check"></i><b>23.2</b> Example #2: Nested design</a></li>
<li class="chapter" data-level="23.3" data-path="week-12-lab.html"><a href="week-12-lab.html#example-3-nested-design"><i class="fa fa-check"></i><b>23.3</b> Example #3: Nested design</a></li>
<li class="chapter" data-level="23.4" data-path="week-12-lab.html"><a href="week-12-lab.html#example-4-randomized-block-design"><i class="fa fa-check"></i><b>23.4</b> Example #4: Randomized Block Design</a></li>
<li class="chapter" data-level="23.5" data-path="week-12-lab.html"><a href="week-12-lab.html#example-5-nested-design"><i class="fa fa-check"></i><b>23.5</b> Example #5: Nested design</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="week-13-lecture.html"><a href="week-13-lecture.html"><i class="fa fa-check"></i><b>24</b> Week 13 Lecture</a><ul>
<li class="chapter" data-level="24.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-criticism"><i class="fa fa-check"></i><b>24.1</b> Model criticism</a></li>
<li class="chapter" data-level="24.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#residuals"><i class="fa fa-check"></i><b>24.2</b> Residuals</a></li>
<li class="chapter" data-level="24.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#leverage"><i class="fa fa-check"></i><b>24.3</b> Leverage</a></li>
<li class="chapter" data-level="24.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#influential"><i class="fa fa-check"></i><b>24.4</b> Influential</a></li>
<li class="chapter" data-level="24.5" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-residuals-leverage-and-influence"><i class="fa fa-check"></i><b>24.5</b> Comparing residuals, leverage, and influence</a></li>
<li class="chapter" data-level="24.6" data-path="week-13-lecture.html"><a href="week-13-lecture.html#if-our-goal-is-to-explain"><i class="fa fa-check"></i><b>24.6</b> If our goal is to explain…</a></li>
<li class="chapter" data-level="24.7" data-path="week-13-lecture.html"><a href="week-13-lecture.html#if-our-goal-is-to-predict"><i class="fa fa-check"></i><b>24.7</b> If our goal is to predict…</a></li>
<li class="chapter" data-level="24.8" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-two-models"><i class="fa fa-check"></i><b>24.8</b> Comparing two models</a></li>
<li class="chapter" data-level="24.9" data-path="week-13-lecture.html"><a href="week-13-lecture.html#likelihood"><i class="fa fa-check"></i><b>24.9</b> Likelihood</a></li>
<li class="chapter" data-level="24.10" data-path="week-13-lecture.html"><a href="week-13-lecture.html#nested-or-not"><i class="fa fa-check"></i><b>24.10</b> Nested or not?</a></li>
<li class="chapter" data-level="24.11" data-path="week-13-lecture.html"><a href="week-13-lecture.html#akaikes-information-criterion"><i class="fa fa-check"></i><b>24.11</b> Akaike’s Information Criterion</a></li>
<li class="chapter" data-level="24.12" data-path="week-13-lecture.html"><a href="week-13-lecture.html#bayesian-information-criterion"><i class="fa fa-check"></i><b>24.12</b> Bayesian Information Criterion</a></li>
<li class="chapter" data-level="24.13" data-path="week-13-lecture.html"><a href="week-13-lecture.html#comparing-lrt-and-aicbic"><i class="fa fa-check"></i><b>24.13</b> Comparing LRT and AIC/BIC</a></li>
<li class="chapter" data-level="24.14" data-path="week-13-lecture.html"><a href="week-13-lecture.html#model-weighting"><i class="fa fa-check"></i><b>24.14</b> Model weighting</a><ul>
<li class="chapter" data-level="24.14.1" data-path="week-13-lecture.html"><a href="week-13-lecture.html#stepwise-regression"><i class="fa fa-check"></i><b>24.14.1</b> Stepwise regression</a></li>
<li class="chapter" data-level="24.14.2" data-path="week-13-lecture.html"><a href="week-13-lecture.html#criticism-of-stepwise-regression"><i class="fa fa-check"></i><b>24.14.2</b> Criticism of stepwise regression</a></li>
<li class="chapter" data-level="24.14.3" data-path="week-13-lecture.html"><a href="week-13-lecture.html#criticism-of-data-dredging"><i class="fa fa-check"></i><b>24.14.3</b> Criticism of data dredging</a></li>
<li class="chapter" data-level="24.14.4" data-path="week-13-lecture.html"><a href="week-13-lecture.html#final-thoughts-on-model-selection"><i class="fa fa-check"></i><b>24.14.4</b> Final thoughts on model selection</a></li>
</ul></li>
<li class="chapter" data-level="24.15" data-path="week-13-lecture.html"><a href="week-13-lecture.html#week-13-faq"><i class="fa fa-check"></i><b>24.15</b> Week 13 FAQ</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="week-13-lab.html"><a href="week-13-lab.html"><i class="fa fa-check"></i><b>25</b> Week 13 Lab</a><ul>
<li class="chapter" data-level="25.1" data-path="week-13-lab.html"><a href="week-13-lab.html#part-1-model-selection-model-comparison"><i class="fa fa-check"></i><b>25.1</b> Part 1: Model selection / model comparison</a></li>
<li class="chapter" data-level="25.2" data-path="week-13-lab.html"><a href="week-13-lab.html#model-selection-via-step-wise-regression"><i class="fa fa-check"></i><b>25.2</b> Model selection via step-wise regression</a></li>
<li class="chapter" data-level="25.3" data-path="week-13-lab.html"><a href="week-13-lab.html#part-2-model-criticism"><i class="fa fa-check"></i><b>25.3</b> Part 2: Model criticism</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="week-14-lecture.html"><a href="week-14-lecture.html"><i class="fa fa-check"></i><b>26</b> Week 14 Lecture</a><ul>
<li class="chapter" data-level="26.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#what-does-multivariate-mean"><i class="fa fa-check"></i><b>26.1</b> What does ‘multivariate’ mean?</a></li>
<li class="chapter" data-level="26.2" data-path="week-14-lecture.html"><a href="week-14-lecture.html#multivariate-associations"><i class="fa fa-check"></i><b>26.2</b> Multivariate associations</a></li>
<li class="chapter" data-level="26.3" data-path="week-14-lecture.html"><a href="week-14-lecture.html#model-criticism-for-multivariate-analyses"><i class="fa fa-check"></i><b>26.3</b> Model criticism for multivariate analyses</a><ul>
<li class="chapter" data-level="26.3.1" data-path="week-14-lecture.html"><a href="week-14-lecture.html#transforming-your-data"><i class="fa fa-check"></i><b>26.3.1</b> Transforming your data</a></li>
</ul></li>
<li class="chapter" data-level="26.4" data-path="week-14-lecture.html"><a href="week-14-lecture.html#standardizing-your-data"><i class="fa fa-check"></i><b>26.4</b> Standardizing your data</a></li>
<li class="chapter" data-level="26.5" data-path="week-14-lecture.html"><a href="week-14-lecture.html#multivariate-outliers"><i class="fa fa-check"></i><b>26.5</b> Multivariate outliers</a></li>
<li class="chapter" data-level="26.6" data-path="week-14-lecture.html"><a href="week-14-lecture.html#brief-overview-of-multivariate-analyses"><i class="fa fa-check"></i><b>26.6</b> Brief overview of multivariate analyses</a></li>
<li class="chapter" data-level="26.7" data-path="week-14-lecture.html"><a href="week-14-lecture.html#manova-and-dfa"><i class="fa fa-check"></i><b>26.7</b> MANOVA and DFA</a></li>
<li class="chapter" data-level="26.8" data-path="week-14-lecture.html"><a href="week-14-lecture.html#scaling-or-ordination-techniques"><i class="fa fa-check"></i><b>26.8</b> Scaling or ordination techniques</a></li>
<li class="chapter" data-level="26.9" data-path="week-14-lecture.html"><a href="week-14-lecture.html#principal-components-analysis-pca"><i class="fa fa-check"></i><b>26.9</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="26.10" data-path="week-14-lecture.html"><a href="week-14-lecture.html#principal-components-analysis-pca-1"><i class="fa fa-check"></i><b>26.10</b> Principal components analysis (PCA)</a></li>
<li class="chapter" data-level="26.11" data-path="week-14-lecture.html"><a href="week-14-lecture.html#pca-in-r"><i class="fa fa-check"></i><b>26.11</b> PCA in R</a></li>
<li class="chapter" data-level="26.12" data-path="week-14-lecture.html"><a href="week-14-lecture.html#missing-data"><i class="fa fa-check"></i><b>26.12</b> Missing data</a></li>
<li class="chapter" data-level="26.13" data-path="week-14-lecture.html"><a href="week-14-lecture.html#imputing-missing-data"><i class="fa fa-check"></i><b>26.13</b> Imputing missing data</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="week-14-lab.html"><a href="week-14-lab.html"><i class="fa fa-check"></i><b>27</b> Week 14 Lab</a><ul>
<li class="chapter" data-level="27.1" data-path="week-14-lab.html"><a href="week-14-lab.html#missing-at-random---practice-with-glms"><i class="fa fa-check"></i><b>27.1</b> Missing at random - practice with GLMs</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Biometry Lecture and Lab Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="week-2-lab" class="section level1">
<h1><span class="header-section-number">4</span> Week 2 Lab</h1>
<div id="confidence-intervals" class="section level2">
<h2><span class="header-section-number">4.1</span> Confidence intervals</h2>
<p>Before getting too far, we need to circle back and make sure we understand what is meant by a confidence interval.</p>
<p>A 95th percentile confidence interval say “If I repeat this procedure 100 times using 100 different datasets, 95% of the time my confidence intervals will capture the true parameter”. It does NOT say that there is a 95% chance that the parameter is in the interval.</p>
<p><strong>Quiz time! (Don’t worry, not a real quiz)</strong></p>
<p><em>Important note</em>: This is an area where Aho is WRONG. I will not repeat Aho’s interpretation here because I think he’s just wrong. Aho is correct on only one point. It is true that ONCE THE 95th CI HAS BEEN CONSTRUCTED, it is no longer possible to assign a <span class="math inline">\(%\)</span> to the probability that that CI contains the true value or not. Because that CI, once created, either DOES or DOES NOT contain the true value. However, we often talk about the interval in the abstract. When we say “There is a 95<span class="math inline">\(%\)</span> chance that the interval contains the true value” what we mean is that there is a 95<span class="math inline">\(%\)</span> probability that a CI created using that methodology would contain the true value.</p>
<p>Do not let Week 2 pass by without fundamentally understanding the interpretation of a confidence interval.</p>
</div>
<div id="testing-hypotheses-through-permutation" class="section level2">
<h2><span class="header-section-number">4.2</span> Testing hypotheses through permutation</h2>
<p>We’ll start off by working through two examples from Phillip Good’s book “Introduction to Statistics Through Resampling Methods and R/S-PLUS”:</p>
<p>Example #1: Use permutation methods to test the null hypothesis that the treatment does not increase survival time (in other word: <span class="math inline">\(H_{0}\)</span>: No difference in survival between the treated and control groups):</p>
<p>Survival.treated=<span class="math inline">\(\{94,197,16,38,99,141,23 \}\)</span></p>
<p>Survival.control=<span class="math inline">\(\{52,104,146,10,51,30,40,27,46 \}\)</span></p>
<p>(Is this a one-tailed or a two-tailed test?)</p>
<p>Make sure that you understand what is being done here, as this example is very closely related to the problem set.</p>
<p>Example #2: Using the same data, provide a 95% confidence interval for the difference in mean survival days based on 1000 bootstrap samples</p>
<p>Note that these two approaches are very closely related. Do you see why either approach can be used to test the null hypothesis? (What is the null hypothesis here?)</p>
<p>Now we will do one slightly more complicated example from Phillip Good’s book “Permutation tests: A practical guide to resampling methods and testing hypotheses”:</p>
<p>Holmes and Williams (1954) studied tonsil size in children to verify a possible association with the virus . Test for an association between  status and tonsil size. (Note that you will need to come up with a reasonable test statistic.)</p>
<div class="figure">
<img src="Table2categories.png" />

</div>
<p>Now lets consider the full dataset, where tonsil size is divided into three categories. How would we do the test now? What is the new test statistic? (There are many options.) What ‘labels’ do you permute?</p>
<div class="figure">
<img src="Table3categories.png" />

</div>
</div>
<div id="basics-of-bootstrap-and-jackknife" class="section level2">
<h2><span class="header-section-number">4.3</span> Basics of bootstrap and jackknife</h2>
<p>To get started with bootstrap and jackknife techniques, we start by working through a very simple example. First we simulate some data</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x&lt;-<span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">9</span>,<span class="dt">by=</span><span class="dv">1</span>)</code></pre></div>
<p>This will constutute our “data”. Let’s print the result of sampling with replacement to get a sense for it…</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">table</span>(<span class="kw">sample</span>(x,<span class="dt">size=</span><span class="kw">length</span>(x),<span class="dt">replace=</span>T))</code></pre></div>
<pre><code>## 
## 1 2 5 6 7 9 
## 2 1 2 1 1 3</code></pre>
<p>Now we will write a little script to take bootstrap samples and calculate the means of each of these bootstrap samples</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">xmeans&lt;-<span class="kw">vector</span>(<span class="dt">length=</span><span class="dv">1000</span>)
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">1000</span>)
  {
  xmeans[i]&lt;-<span class="kw">mean</span>(<span class="kw">sample</span>(x,<span class="dt">replace=</span>T))
  }</code></pre></div>
<p>The actual number of bootstrapped samples is arbitrary <em>at this point</em> but there are ways of characterizing the precision of the bootstrap (jackknife-after-bootstrap) which might inform the number of bootstrap samples needed. <em>In practice</em>, people tend to pick some arbitrary but large number of bootstrap samples because computers are so fast that it is often easy to draw far more samples than are actually needed. When calculation of the statistic is slow (as might be the case if you are using the samples to construct a phylogeny, for example), then you would need to be more concerned with the number of bootstrap samples.</p>
<p>First, lets just look at a histogram of the bootstrapped means and plot the actual sample mean on the histogram for comparison</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(xmeans,<span class="dt">breaks=</span><span class="dv">30</span>,<span class="dt">col=</span><span class="st">&quot;pink&quot;</span>)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">mean</span>(x),<span class="dt">lwd=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="Week-2-Lab_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="calculating-bias-and-standard-error" class="section level2">
<h2><span class="header-section-number">4.4</span> Calculating bias and standard error</h2>
<p>From these we can calculate the bias and standard deviation for the mean (which is the “statistic”):</p>
<p><span class="math display">\[
\widehat{Bias_{boot}} = \left(\frac{1}{k}\sum^{k}_{i=1}\theta^{*}_{i}\right)-\hat{\theta}
\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">bias.boot&lt;-<span class="kw">mean</span>(xmeans)<span class="op">-</span><span class="kw">mean</span>(x)
bias.boot</code></pre></div>
<pre><code>## [1] -0.0168</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(xmeans,<span class="dt">breaks=</span><span class="dv">30</span>,<span class="dt">col=</span><span class="st">&quot;pink&quot;</span>)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">mean</span>(x),<span class="dt">lwd=</span><span class="dv">5</span>,<span class="dt">col=</span><span class="st">&quot;black&quot;</span>)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">mean</span>(xmeans),<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">col=</span><span class="st">&quot;yellow&quot;</span>)</code></pre></div>
<p><img src="Week-2-Lab_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p><span class="math display">\[
\widehat{s.e._{boot}} = \sqrt{\frac{1}{k-1}\sum^{k}_{i=1}(\theta^{*}_{i}-\bar{\theta^{*}})^{2}}
\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">se.boot&lt;-<span class="kw">sd</span>(xmeans)</code></pre></div>
<p>We can find the confidence intervals in two ways:</p>
<p>Method #1: Assume the bootstrap statistics are normally distributed</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">LL.boot&lt;-<span class="kw">mean</span>(xmeans)<span class="op">-</span><span class="fl">1.96</span><span class="op">*</span>se.boot <span class="co">#where did 1.96 come from?</span>
UL.boot&lt;-<span class="kw">mean</span>(xmeans)<span class="op">+</span><span class="fl">1.96</span><span class="op">*</span>se.boot
LL.boot</code></pre></div>
<pre><code>## [1] 2.693739</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">UL.boot</code></pre></div>
<pre><code>## [1] 6.272661</code></pre>
<p>Method #2: Simply take the quantiles of the bootstrap statistics</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">quantile</span>(xmeans,<span class="kw">c</span>(<span class="fl">0.025</span>,<span class="fl">0.975</span>))</code></pre></div>
<pre><code>##  2.5% 97.5% 
##   2.8   6.3</code></pre>
<p>Let’s compare this to what we would have gotten if we had used normal distribution theory. First we have to calculate the standard error:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">se.normal&lt;-<span class="kw">sqrt</span>(<span class="kw">var</span>(x)<span class="op">/</span><span class="kw">length</span>(x))
LL.normal&lt;-<span class="kw">mean</span>(x)<span class="op">-</span><span class="kw">qt</span>(<span class="fl">0.975</span>,<span class="kw">length</span>(x)<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>se.normal
UL.normal&lt;-<span class="kw">mean</span>(x)<span class="op">+</span><span class="kw">qt</span>(<span class="fl">0.975</span>,<span class="kw">length</span>(x)<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>se.normal
LL.normal</code></pre></div>
<pre><code>## [1] 2.334149</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">UL.normal</code></pre></div>
<pre><code>## [1] 6.665851</code></pre>
<p>In this case, the confidence intervals we got from the normal distribution theory are too wide.</p>
<p>Does it make sense why the normal distribution theory intervals are too wide? Because the original were were uniformly distributed, the data has higher variance than would be expected and therefore the standard error is higher than would be expected.</p>
<p>There are two packages that provide functions for bootstrapping, ‘boot’ and ‘boostrap’. We will start by using the ‘bootstrap’ package, which was originally designed for Efron and Tibshirani’s monograph on the bootstrap.</p>
<p>To test the main functionality of the ‘bootstrap’ package, we will use the data we already have. The ‘bootstrap’ function requires the input of a user-defined function to calculate the statistic of interest. Here I will write a function that calculates the mean of the input values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(bootstrap)
theta&lt;-<span class="cf">function</span>(x)
  {
    <span class="kw">mean</span>(x)
  }
results&lt;-<span class="kw">bootstrap</span>(<span class="dt">x=</span>x,<span class="dt">nboot=</span><span class="dv">1000</span>,<span class="dt">theta=</span>theta)
results</code></pre></div>
<pre><code>## $thetastar
##    [1] 3.6 4.9 5.0 4.2 4.8 5.3 4.7 5.2 4.2 4.0 4.9 5.4 6.1 5.7 5.5 5.2 4.4 3.3
##   [19] 3.2 3.3 5.1 3.5 5.7 4.0 3.7 5.8 3.6 5.3 4.3 5.4 5.9 2.0 3.9 4.5 4.2 3.3
##   [37] 4.3 3.7 6.1 4.9 4.4 3.4 4.5 3.4 4.3 4.1 5.1 5.0 4.4 3.4 5.3 2.7 4.7 4.0
##   [55] 4.1 3.6 5.1 2.8 5.7 4.8 5.5 5.1 3.2 3.8 5.4 4.8 5.1 3.9 4.4 4.4 3.5 4.5
##   [73] 3.6 4.6 5.1 4.7 4.9 3.4 5.6 4.6 4.6 4.9 4.1 5.6 5.2 4.7 3.6 5.3 5.2 5.1
##   [91] 4.3 6.4 3.7 4.1 3.4 4.6 4.2 4.9 3.6 3.0 3.3 4.8 4.2 4.3 4.6 3.3 3.4 4.0
##  [109] 4.2 3.2 5.0 5.2 5.0 3.2 3.7 4.0 2.9 4.5 4.8 2.9 3.8 4.0 4.2 3.4 3.3 4.0
##  [127] 6.0 4.0 4.1 3.1 2.8 4.4 5.4 5.2 3.2 4.8 4.1 5.6 3.3 3.4 4.4 3.6 5.0 4.4
##  [145] 5.9 6.0 5.1 4.3 4.3 3.3 3.9 3.7 4.6 4.0 4.5 4.3 6.3 3.6 4.1 3.9 4.7 6.0
##  [163] 3.6 4.7 3.1 5.5 4.0 4.9 5.8 5.4 4.4 6.2 5.8 2.9 4.9 5.1 4.5 4.0 4.4 4.5
##  [181] 3.9 4.9 3.0 5.0 6.0 6.4 4.6 3.4 5.1 2.2 4.9 5.2 3.3 4.7 4.2 4.8 2.7 3.6
##  [199] 4.4 5.0 5.7 3.5 3.8 4.5 5.5 6.2 4.3 3.2 4.4 4.5 4.2 3.8 4.7 4.2 3.9 4.5
##  [217] 3.3 4.4 4.9 4.5 3.4 3.9 4.8 2.9 4.5 6.4 4.2 4.9 5.5 3.7 4.8 4.5 3.3 4.3
##  [235] 4.6 5.1 5.4 4.6 4.1 4.8 5.1 4.5 3.4 4.7 4.1 5.1 4.7 5.5 4.6 4.9 5.1 2.9
##  [253] 2.9 4.8 4.3 5.6 5.4 4.0 4.1 4.7 4.5 5.5 5.5 3.7 4.0 2.8 4.5 3.3 5.3 5.5
##  [271] 4.3 3.7 5.8 5.1 3.6 2.5 4.4 6.3 4.7 5.4 3.7 4.4 3.8 3.5 6.4 3.5 4.9 2.7
##  [289] 5.4 3.1 3.9 4.4 5.5 3.9 6.1 5.4 4.5 5.2 5.8 4.3 4.2 4.1 4.2 4.4 3.6 4.9
##  [307] 5.3 5.4 4.9 4.2 5.5 5.1 5.0 5.7 4.3 5.4 5.4 2.4 5.4 3.7 3.2 2.6 2.9 4.0
##  [325] 3.4 4.5 6.0 3.8 5.5 4.0 4.2 4.4 6.6 4.5 5.4 3.5 4.7 4.3 4.2 5.2 4.3 5.0
##  [343] 4.7 4.7 5.6 4.4 4.6 4.8 3.0 4.3 6.1 5.8 3.8 4.9 5.5 4.3 2.8 2.3 5.9 4.3
##  [361] 3.5 5.1 4.3 5.6 4.1 4.3 3.4 4.7 5.9 4.6 4.1 5.0 4.4 3.9 3.9 4.1 4.5 5.7
##  [379] 4.3 5.6 6.2 4.7 3.0 4.7 4.8 4.4 4.1 4.1 5.2 4.8 7.0 5.3 3.2 3.5 5.2 4.8
##  [397] 4.2 4.5 4.2 3.7 5.6 3.7 3.9 6.1 4.4 4.5 4.0 6.3 5.2 4.8 4.6 6.3 5.2 3.6
##  [415] 4.4 5.3 5.9 4.1 3.5 4.5 6.6 5.8 5.7 4.9 3.2 5.4 2.7 3.3 3.6 5.4 5.6 3.5
##  [433] 2.6 5.4 4.8 5.6 4.1 4.0 4.6 4.5 4.1 4.0 5.4 3.1 4.6 4.4 3.3 5.5 4.7 5.7
##  [451] 5.1 3.7 5.0 4.1 6.7 2.3 3.7 4.7 4.0 6.1 5.7 4.0 2.6 3.9 1.9 3.1 3.2 4.8
##  [469] 4.2 4.6 4.3 4.2 3.9 4.7 4.0 4.7 4.3 5.1 4.7 5.1 4.8 4.1 6.3 4.1 5.9 4.9
##  [487] 6.1 4.7 4.3 5.1 3.6 3.6 5.3 3.2 6.2 4.9 4.4 4.5 3.7 3.9 4.7 5.0 3.7 4.9
##  [505] 5.2 4.0 5.5 5.5 5.3 4.2 3.7 4.8 3.4 3.7 3.4 5.7 4.5 4.5 5.3 4.9 3.9 3.5
##  [523] 3.3 3.9 5.3 3.9 4.6 4.5 5.9 6.5 3.3 4.9 4.0 5.5 3.4 3.7 3.0 5.3 4.2 4.5
##  [541] 4.9 5.1 4.9 4.5 3.0 4.7 4.5 4.3 5.8 4.0 5.1 5.5 3.7 4.8 5.6 4.8 4.6 4.1
##  [559] 4.6 5.2 4.2 6.2 4.6 3.7 5.3 5.1 3.5 4.0 4.7 5.1 4.7 4.6 4.1 6.5 4.0 3.9
##  [577] 5.0 5.3 4.3 2.9 4.1 4.7 5.1 4.2 4.1 3.3 4.4 4.2 3.7 4.3 5.8 4.5 5.3 4.5
##  [595] 5.7 5.8 3.8 4.7 4.0 5.3 3.6 4.5 5.0 4.8 4.5 4.3 6.3 5.1 4.4 4.4 4.6 4.4
##  [613] 5.9 5.8 3.7 4.6 3.9 4.8 3.7 5.4 3.6 3.7 5.1 4.8 3.8 3.0 4.6 7.3 5.3 4.4
##  [631] 3.8 7.1 4.9 4.4 1.9 4.0 3.6 3.1 3.3 4.6 5.1 4.9 4.1 5.5 5.9 5.1 3.7 3.0
##  [649] 5.8 4.2 4.1 6.0 3.1 5.0 4.6 3.3 4.0 3.8 5.0 5.2 5.0 5.4 3.8 4.1 4.2 4.5
##  [667] 3.8 5.2 3.2 4.6 6.5 5.1 3.4 5.5 3.2 3.3 4.3 4.5 4.3 4.0 5.0 4.3 4.6 5.4
##  [685] 4.5 6.8 3.8 4.6 5.6 3.5 4.2 4.8 5.3 4.1 2.7 5.1 3.8 4.8 6.2 4.5 6.0 5.4
##  [703] 4.6 4.6 5.5 4.2 5.1 1.8 4.1 4.9 3.1 3.7 2.9 2.9 4.6 3.6 5.2 5.3 3.7 5.1
##  [721] 3.5 4.9 4.0 4.2 7.1 5.2 4.3 6.4 3.9 3.7 4.9 5.1 4.0 4.9 4.4 5.1 2.8 5.2
##  [739] 3.5 4.0 3.9 5.3 3.9 5.0 4.5 4.5 2.8 4.9 4.3 5.2 4.2 3.8 5.0 3.6 5.0 3.7
##  [757] 4.7 4.6 4.2 4.4 4.8 5.6 4.3 5.5 3.6 4.1 3.4 3.1 5.4 6.2 5.1 5.3 4.8 3.9
##  [775] 6.5 4.3 5.1 4.6 3.8 5.0 3.7 2.4 3.2 4.3 3.5 5.6 4.7 4.0 5.4 3.9 4.0 5.2
##  [793] 3.1 3.0 4.7 1.8 6.2 5.6 5.0 4.1 3.7 2.4 4.2 4.2 5.3 3.9 5.1 3.8 4.4 4.7
##  [811] 4.6 3.4 4.4 3.0 5.6 4.9 3.5 5.4 4.1 4.0 5.8 5.3 6.0 4.8 4.7 5.0 4.0 3.4
##  [829] 4.6 3.3 3.8 4.1 5.5 2.7 4.8 4.7 3.8 5.4 4.4 4.5 4.9 4.3 3.7 3.8 5.4 5.6
##  [847] 3.9 4.8 2.5 4.0 4.1 3.8 5.5 3.6 3.3 5.0 4.2 5.1 4.7 4.2 3.1 5.2 5.1 5.0
##  [865] 2.8 4.3 4.5 5.0 4.8 3.9 5.0 2.6 7.1 3.7 3.3 4.7 5.5 5.1 3.5 3.7 3.6 5.0
##  [883] 4.3 4.7 6.5 3.7 4.4 5.3 3.3 4.4 4.3 4.3 3.4 3.8 4.8 2.4 3.2 5.0 5.5 4.2
##  [901] 4.5 5.9 5.4 4.3 3.5 3.7 4.7 5.5 4.1 5.4 4.1 5.5 4.1 4.2 4.5 3.5 4.9 3.7
##  [919] 4.6 5.8 4.8 5.0 5.1 4.5 3.5 4.0 3.9 5.6 3.3 4.7 5.2 6.0 4.2 3.1 5.0 3.8
##  [937] 2.7 4.7 6.9 4.9 2.3 5.1 3.9 3.3 4.3 4.4 4.7 2.6 2.6 4.8 4.3 4.8 6.2 3.6
##  [955] 5.5 4.8 4.0 4.4 5.8 3.1 6.4 3.9 5.4 4.6 4.5 3.5 3.8 5.8 4.6 5.3 4.7 4.2
##  [973] 4.8 4.3 4.9 3.8 5.6 4.4 6.0 5.2 3.9 4.3 4.8 5.8 3.8 2.4 5.0 4.0 4.0 4.3
##  [991] 4.0 3.9 3.5 5.3 5.4 4.4 3.8 3.8 5.2 4.0
## 
## $func.thetastar
## NULL
## 
## $jack.boot.val
## NULL
## 
## $jack.boot.se
## NULL
## 
## $call
## bootstrap(x = x, nboot = 1000, theta = theta)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">quantile</span>(results<span class="op">$</span>thetastar,<span class="kw">c</span>(<span class="fl">0.025</span>,<span class="fl">0.975</span>))</code></pre></div>
<pre><code>##  2.5% 97.5% 
##   2.7   6.3</code></pre>
<p>Notice that we get exactly what we got last time. This illustrates an important point, which is that the bootstrap functions are often no easier to use than something you could write yourself.</p>
<p>You can also define a function of the bootstrapped statistics (we have been calling this theta) to pull out immediately any summary statistics you are interested in from the bootstrapped thetas.</p>
<p>Here I will write a function that calculates the bias of my estimate of the mean (which is 4.5 [i.e. the mean of the number 0,1,2,3,4,5,6,7,8,9])</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">bias&lt;-<span class="cf">function</span>(x)
  {
  <span class="kw">mean</span>(x)<span class="op">-</span><span class="fl">4.5</span>
  }
results&lt;-<span class="kw">bootstrap</span>(<span class="dt">x=</span>x,<span class="dt">nboot=</span><span class="dv">1000</span>,<span class="dt">theta=</span>theta,<span class="dt">func=</span>bias)
results</code></pre></div>
<pre><code>## $thetastar
##    [1] 5.0 3.4 4.2 4.7 5.1 5.3 4.3 3.1 5.6 5.0 5.6 5.0 5.7 4.1 5.4 3.1 4.8 5.4
##   [19] 4.6 2.7 4.0 3.6 3.7 6.1 5.3 5.0 4.8 3.5 5.7 5.4 5.3 4.6 6.2 4.8 3.4 3.3
##   [37] 4.5 3.8 3.7 4.8 3.7 4.5 2.8 4.0 4.4 4.6 4.6 6.3 4.1 4.6 3.8 4.4 3.0 4.9
##   [55] 6.0 3.9 3.7 4.1 3.9 6.0 3.6 4.5 5.7 5.8 5.2 3.7 5.6 2.1 4.2 4.3 4.4 5.5
##   [73] 5.1 5.5 6.0 5.1 5.1 4.6 3.5 4.1 5.0 4.1 5.5 4.2 3.2 3.3 4.0 3.7 3.4 4.0
##   [91] 5.0 4.1 4.4 3.9 4.6 4.4 4.3 4.2 2.9 4.9 5.0 4.5 4.7 4.5 4.1 4.4 3.3 5.5
##  [109] 4.3 3.4 4.2 3.6 4.6 5.2 3.6 4.4 3.1 4.5 5.2 4.6 4.2 3.6 3.6 5.4 4.3 3.4
##  [127] 4.2 4.8 4.2 6.5 5.1 5.2 4.9 4.4 4.7 4.3 5.1 5.3 4.4 4.5 4.8 4.5 5.3 3.9
##  [145] 3.6 4.8 4.8 2.7 2.4 5.3 3.8 3.6 3.5 3.8 2.7 5.1 4.1 5.0 5.2 3.7 4.9 5.5
##  [163] 3.7 4.4 3.6 4.3 2.7 5.0 4.8 5.5 5.3 5.0 4.9 3.8 4.7 5.3 3.4 4.4 3.4 3.8
##  [181] 2.5 4.8 3.2 4.2 2.8 4.9 4.3 5.1 6.1 4.2 3.8 5.3 5.6 6.6 4.7 4.9 4.3 3.8
##  [199] 4.5 4.4 3.6 4.4 3.6 3.9 4.7 5.9 4.0 5.4 4.8 4.1 5.4 5.9 3.5 4.6 3.9 4.0
##  [217] 4.3 3.9 6.2 6.3 3.8 3.2 5.6 2.3 4.2 4.7 5.0 5.3 4.1 4.4 4.3 4.2 5.1 4.7
##  [235] 5.2 5.3 5.8 4.5 5.6 4.8 5.1 5.4 4.3 5.6 3.3 5.3 4.8 2.9 3.4 6.8 5.2 4.3
##  [253] 5.2 4.9 3.7 3.4 4.8 3.3 3.5 5.4 6.3 3.7 2.9 5.5 4.7 4.3 5.3 4.9 4.6 4.5
##  [271] 4.1 6.2 4.0 6.0 5.7 5.9 3.8 4.6 5.2 3.9 4.4 5.2 4.2 5.6 5.4 4.3 5.8 3.7
##  [289] 4.1 4.7 5.7 5.2 3.0 5.0 5.1 5.3 4.2 3.9 5.2 5.9 5.6 5.3 4.4 4.5 5.9 3.5
##  [307] 4.8 4.9 5.2 4.4 4.0 5.2 6.0 4.7 3.8 4.8 3.2 5.8 4.8 3.3 3.3 3.9 4.9 6.8
##  [325] 4.0 4.5 5.1 3.7 5.2 4.1 3.4 6.5 2.9 2.8 4.5 5.6 3.7 4.2 3.6 4.3 4.1 3.6
##  [343] 4.3 3.6 3.9 5.2 5.1 4.4 4.9 4.6 3.8 5.2 5.7 4.0 4.5 4.2 3.1 6.0 4.8 3.9
##  [361] 4.4 4.5 6.0 6.5 2.8 3.8 5.9 3.3 5.1 5.4 5.6 6.0 3.4 5.2 5.7 4.5 4.1 3.7
##  [379] 5.7 5.3 4.4 3.7 5.8 5.0 3.8 3.4 5.4 4.0 4.2 4.5 4.6 4.0 5.1 5.1 4.7 3.7
##  [397] 5.6 5.5 6.4 4.2 5.2 5.7 4.2 5.4 5.4 3.5 3.3 5.6 5.5 4.4 4.3 5.6 5.2 6.7
##  [415] 4.3 4.9 5.5 4.6 5.7 3.7 5.1 5.1 4.2 4.6 3.0 3.6 4.9 3.2 2.7 2.0 4.9 4.1
##  [433] 4.9 3.9 3.9 6.4 5.3 3.7 4.3 5.4 3.8 4.8 3.5 3.8 5.0 6.3 4.8 2.6 3.2 5.0
##  [451] 3.9 3.7 5.2 3.6 4.1 2.6 3.8 4.6 5.8 6.3 3.5 2.9 4.7 6.2 4.3 4.2 4.5 4.7
##  [469] 3.1 4.2 3.1 3.4 4.7 4.4 5.0 2.5 4.0 3.8 4.4 4.6 5.0 5.9 4.3 5.2 5.0 4.5
##  [487] 4.5 4.9 5.1 5.1 2.5 3.9 3.4 5.5 5.1 3.6 5.1 2.8 3.0 5.6 4.9 4.0 3.7 3.0
##  [505] 4.6 4.5 5.5 4.0 4.1 3.5 5.8 4.0 5.1 3.7 3.6 6.3 4.1 4.8 5.5 5.3 5.0 5.1
##  [523] 5.4 4.6 5.4 4.9 4.5 4.9 4.2 4.9 3.6 4.4 4.6 3.5 4.4 6.3 4.4 5.2 4.7 5.6
##  [541] 4.7 3.2 5.2 5.2 2.6 4.9 5.7 5.5 4.0 3.7 3.5 5.0 6.1 4.9 4.2 2.5 4.4 5.0
##  [559] 4.5 3.5 3.4 4.4 3.5 6.9 4.6 2.6 3.5 5.9 4.8 3.7 5.6 5.8 5.6 4.9 3.4 5.3
##  [577] 3.4 2.2 4.3 6.6 4.2 5.0 3.8 4.1 5.1 4.8 5.6 4.6 5.4 4.9 4.2 3.5 4.9 3.9
##  [595] 4.6 5.2 4.5 4.2 6.2 4.0 4.4 3.8 5.8 3.8 4.6 5.0 5.3 5.7 5.1 5.4 3.3 4.6
##  [613] 4.1 3.8 4.7 4.3 3.3 4.1 3.5 5.3 4.0 5.2 4.2 3.2 3.5 3.7 5.8 5.3 3.8 3.6
##  [631] 4.7 3.7 5.3 4.9 6.0 4.6 6.3 5.7 5.3 3.5 5.1 5.2 4.8 4.5 4.7 4.3 5.4 4.3
##  [649] 3.6 4.6 4.3 3.9 3.3 3.6 4.4 5.0 2.9 4.7 3.8 5.5 3.2 4.2 3.7 3.8 5.6 3.7
##  [667] 5.3 3.1 4.2 4.5 4.8 5.0 4.7 3.6 2.8 3.6 4.2 4.7 5.3 4.8 5.7 5.7 6.8 4.7
##  [685] 5.5 5.4 3.2 5.2 4.6 3.2 6.2 5.4 4.9 5.5 3.2 5.1 6.3 4.8 3.2 5.7 4.1 5.4
##  [703] 5.5 4.3 4.3 5.1 3.4 4.4 6.2 4.7 6.5 5.9 3.3 6.2 5.2 3.1 5.2 5.8 4.0 5.7
##  [721] 4.5 4.9 5.8 3.1 4.3 4.9 6.4 4.4 4.5 4.9 5.7 4.5 4.5 4.7 5.7 4.5 5.0 4.9
##  [739] 4.2 3.5 5.1 5.3 5.9 4.6 6.2 4.2 5.4 4.2 4.6 4.1 4.6 3.1 3.6 4.7 4.4 3.5
##  [757] 3.7 4.0 3.7 4.1 3.9 5.9 3.4 2.8 5.6 3.9 4.8 4.4 3.4 4.0 2.9 3.1 3.5 5.1
##  [775] 3.1 4.0 5.6 3.5 4.5 5.5 4.9 5.3 3.6 5.2 3.8 2.7 3.4 5.5 3.4 4.8 3.1 4.9
##  [793] 4.9 6.3 4.3 3.2 5.3 4.8 6.0 4.9 5.0 5.1 5.1 5.0 4.5 6.2 2.8 3.3 3.9 4.3
##  [811] 3.9 4.7 4.6 4.0 3.9 6.2 5.6 3.1 5.9 5.4 4.9 4.2 4.4 3.4 4.0 4.4 3.6 5.3
##  [829] 4.2 5.9 4.8 3.9 5.3 4.8 5.0 3.5 4.4 4.5 6.0 4.8 5.6 3.9 4.0 3.4 5.9 4.0
##  [847] 5.5 4.4 2.8 3.1 4.1 3.9 4.5 4.0 5.1 3.3 2.9 5.6 5.6 4.4 5.2 4.3 4.3 3.5
##  [865] 3.6 4.6 3.3 6.5 4.8 5.7 5.3 3.2 4.6 4.4 6.4 3.7 6.0 3.9 3.2 5.6 3.3 5.4
##  [883] 3.1 4.3 3.8 5.6 2.9 3.6 4.4 4.8 4.1 3.1 3.8 4.0 3.7 4.7 4.7 4.9 4.6 3.6
##  [901] 3.4 4.2 4.7 3.9 6.0 5.2 3.4 5.3 4.2 5.3 4.3 4.1 3.2 5.7 6.0 6.0 4.2 5.6
##  [919] 4.2 5.6 4.6 4.2 4.1 6.1 3.7 3.2 5.5 5.2 5.5 3.8 4.8 3.8 3.6 4.1 5.3 4.5
##  [937] 3.7 4.1 5.0 4.3 5.6 2.2 3.8 3.2 5.2 4.6 3.6 5.4 3.3 5.1 5.9 5.7 4.0 3.3
##  [955] 5.7 5.6 5.4 5.3 4.6 5.8 3.4 4.1 4.4 5.2 4.9 4.4 5.0 4.0 3.8 6.7 4.7 4.4
##  [973] 4.6 5.0 4.7 5.2 3.9 5.0 6.4 4.5 3.8 4.0 4.0 4.1 2.5 5.7 6.2 3.8 5.4 3.4
##  [991] 5.3 4.9 5.5 3.2 4.2 4.3 4.4 6.0 5.5 3.8
## 
## $func.thetastar
## [1] 0.0301
## 
## $jack.boot.val
##  [1]  0.575522388  0.462162162  0.322560976  0.182142857  0.008994709
##  [6]  0.010215054 -0.100835655 -0.290000000 -0.388414634 -0.466961652
## 
## $jack.boot.se
## [1] 1.010901
## 
## $call
## bootstrap(x = x, nboot = 1000, theta = theta, func = bias)</code></pre>
<p>Compare this to ‘bias.boot’ (our result from above). Why might it not be the same? Try running the same section of code several times. See how the value of the bias ($func.thetastar) jumps around? We should not be surprised by this because we can look at the jackknife-after-bootstrap estimate of the standard error of the function (in this case, that function is the bias) and we can see that it is not so small that we wouldn’t expect some variation in these values.</p>
<p>Remember, everything we have discussed today are estimates. The statistic as applied to your data will change with new data, as will the standard error, the confidence intervals - everything! All of these values have sampling distributions and are subject to change if you repeated the procedure with new data.</p>
<p>Note that we can calculate any function of <span class="math inline">\(\theta^{*}\)</span>. A simple example would be the 72nd percentile:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">perc72&lt;-<span class="cf">function</span>(x)
  {
  <span class="kw">quantile</span>(x,<span class="dt">probs=</span><span class="kw">c</span>(<span class="fl">0.72</span>))
  }
results&lt;-<span class="kw">bootstrap</span>(<span class="dt">x=</span>x,<span class="dt">nboot=</span><span class="dv">1000</span>,<span class="dt">theta=</span>theta,<span class="dt">func=</span>perc72)
results</code></pre></div>
<pre><code>## $thetastar
##    [1] 5.4 3.1 4.6 4.2 6.0 3.9 4.3 3.3 5.6 4.0 2.8 5.3 3.7 4.5 6.2 4.1 4.3 6.0
##   [19] 4.3 5.2 5.1 4.8 5.1 5.1 1.6 5.9 4.8 5.5 4.4 4.6 3.4 4.2 3.1 4.0 4.9 4.2
##   [37] 4.6 3.6 4.8 4.5 5.3 3.3 4.2 5.3 5.1 5.1 5.6 4.0 3.5 4.1 4.7 3.3 4.3 4.2
##   [55] 4.8 7.1 3.3 4.8 4.4 6.0 5.6 6.0 6.0 3.1 4.2 4.8 3.6 4.6 4.9 3.4 3.9 4.4
##   [73] 3.3 3.7 4.5 4.2 3.8 6.0 4.5 4.2 5.5 3.8 3.7 5.8 4.4 4.2 4.9 4.9 5.0 5.0
##   [91] 6.0 4.7 5.4 6.1 5.9 4.8 3.4 4.2 3.8 4.3 5.1 5.5 6.4 4.1 4.2 5.3 5.3 5.6
##  [109] 3.9 5.6 3.5 4.3 5.2 4.0 5.3 3.2 4.4 4.5 4.4 4.7 3.8 4.5 2.4 4.1 4.8 5.0
##  [127] 5.1 3.8 5.1 4.8 5.8 4.3 5.7 5.3 4.8 3.8 4.0 4.7 3.7 6.5 3.7 4.8 5.0 5.3
##  [145] 3.5 4.1 5.8 4.6 4.3 4.2 4.5 3.3 3.8 5.0 3.3 4.5 3.6 4.4 3.1 3.3 4.6 5.3
##  [163] 6.3 2.4 4.8 4.4 5.1 4.5 5.1 5.0 4.7 4.5 5.3 4.4 5.3 4.2 5.0 5.0 4.0 1.7
##  [181] 5.0 4.8 5.5 3.8 5.2 6.5 4.2 4.8 3.9 5.1 3.2 4.0 4.4 6.1 4.5 4.0 3.3 4.5
##  [199] 5.8 4.6 4.2 2.8 4.9 3.6 3.9 3.9 4.2 5.1 4.5 5.5 4.8 4.2 4.1 3.0 4.9 3.9
##  [217] 4.1 4.1 5.1 4.9 3.3 5.1 4.9 3.5 4.9 3.7 5.2 3.4 4.6 3.6 2.3 3.4 5.3 4.5
##  [235] 6.6 5.1 5.3 5.7 5.0 5.8 3.8 3.4 4.5 3.0 3.7 3.0 3.8 3.7 5.3 4.3 4.9 4.4
##  [253] 4.8 3.6 6.9 5.6 3.3 5.1 3.6 3.4 3.7 4.0 5.0 5.4 4.8 5.4 5.1 3.3 6.1 4.3
##  [271] 5.0 4.7 3.9 4.3 4.8 4.3 4.7 3.3 5.4 3.5 5.1 4.1 5.4 4.6 4.0 3.0 4.7 5.8
##  [289] 4.6 4.1 2.6 3.6 5.6 3.8 4.5 5.8 4.5 3.8 3.6 4.3 4.1 5.7 5.6 3.4 4.2 4.4
##  [307] 4.3 4.4 5.3 5.0 5.4 4.0 5.4 4.3 3.5 4.3 4.2 3.6 4.7 4.0 4.6 3.2 4.2 4.7
##  [325] 3.7 3.0 5.0 3.4 5.7 4.8 5.0 5.4 4.5 5.6 5.3 4.7 2.8 4.3 4.0 4.8 3.3 4.3
##  [343] 3.8 3.5 4.3 3.5 4.9 5.4 5.6 4.1 4.0 5.6 5.5 3.5 4.1 4.0 5.9 4.7 5.8 3.8
##  [361] 3.0 4.9 4.2 4.8 4.8 3.4 5.6 5.5 5.6 5.1 6.4 3.8 5.4 3.6 4.7 5.5 3.4 5.0
##  [379] 3.9 5.0 3.2 4.0 2.5 6.2 6.0 3.4 5.6 5.3 4.6 4.4 4.8 5.0 4.9 5.1 4.3 3.0
##  [397] 5.4 3.2 5.9 2.6 3.3 4.1 5.9 5.0 3.4 4.6 5.2 3.3 4.7 5.1 4.0 5.6 5.5 4.2
##  [415] 4.6 2.9 4.2 3.4 5.1 4.3 5.2 4.6 4.2 4.9 5.5 3.4 4.7 5.0 5.1 5.1 5.1 5.2
##  [433] 5.0 4.3 3.3 5.8 5.4 4.3 4.3 3.9 4.4 5.5 7.1 4.1 5.2 6.9 4.6 4.1 5.0 5.6
##  [451] 4.6 4.6 4.3 5.2 3.0 5.7 5.9 6.2 4.9 2.6 2.8 4.8 5.0 3.9 4.5 5.8 3.1 3.8
##  [469] 3.5 3.8 3.6 2.4 3.1 4.8 6.0 3.5 4.9 5.1 5.9 3.4 3.9 3.2 3.2 4.9 3.8 5.0
##  [487] 4.2 3.0 4.7 3.5 4.0 4.0 4.1 5.5 4.3 6.1 4.5 3.9 3.0 5.9 3.5 5.4 3.9 3.9
##  [505] 5.4 4.2 4.5 5.2 6.0 4.1 4.2 5.2 4.9 5.2 4.8 5.6 4.1 5.0 4.4 3.4 5.9 3.6
##  [523] 3.8 5.5 4.4 2.9 4.5 4.6 5.2 6.3 3.7 5.1 3.4 4.3 5.6 4.8 5.5 4.5 3.6 4.5
##  [541] 6.8 4.4 2.2 5.6 3.9 5.3 5.4 6.0 3.3 3.1 4.7 3.6 3.5 4.0 5.0 4.4 3.5 4.7
##  [559] 4.1 4.4 2.5 5.1 6.1 2.7 5.0 7.0 4.1 4.0 4.1 4.2 2.8 3.8 4.3 3.1 5.8 4.0
##  [577] 4.5 2.7 4.1 4.9 3.1 6.2 3.5 4.1 5.1 3.5 2.7 3.5 4.7 4.3 4.0 5.0 5.1 5.0
##  [595] 4.6 3.9 4.6 4.6 3.8 4.5 3.7 3.8 4.3 4.6 4.1 4.7 5.5 5.8 3.6 4.7 4.4 4.0
##  [613] 5.3 3.8 5.1 4.1 3.8 6.4 4.1 4.2 7.1 3.6 5.1 4.7 3.2 4.2 4.2 5.4 4.0 5.1
##  [631] 4.4 4.3 5.6 4.9 3.6 4.7 4.5 3.4 5.8 4.8 5.0 5.6 5.5 3.6 4.4 3.9 3.5 5.5
##  [649] 4.1 3.6 4.0 4.3 3.7 3.6 5.2 4.3 4.4 5.5 6.6 3.5 3.7 3.4 6.2 4.1 4.5 4.6
##  [667] 3.0 3.5 4.5 3.7 4.5 5.2 5.3 3.3 5.9 4.7 4.5 4.0 3.6 3.3 4.6 3.3 3.2 5.3
##  [685] 4.2 3.9 4.6 3.6 4.3 4.0 3.5 4.5 5.0 3.5 4.2 4.0 2.9 4.0 5.2 4.5 3.0 4.3
##  [703] 6.3 2.5 3.0 4.3 5.2 4.8 4.9 3.8 5.4 5.7 4.5 5.1 4.2 3.9 3.7 4.0 3.8 3.6
##  [721] 3.9 5.5 5.4 5.9 5.0 6.0 2.9 4.0 4.2 5.6 5.0 4.5 4.3 4.2 5.0 3.9 4.9 4.5
##  [739] 5.1 6.0 6.4 5.3 3.2 3.8 4.2 4.0 4.4 6.6 5.5 5.0 3.8 5.5 5.3 4.4 3.8 5.7
##  [757] 4.1 5.2 5.5 4.6 4.4 5.2 5.6 3.6 4.1 5.4 4.3 4.7 7.2 4.2 4.7 3.3 5.9 4.6
##  [775] 3.7 4.2 6.4 4.5 4.4 2.3 4.4 3.9 4.9 4.6 4.3 5.6 5.8 3.3 4.6 4.6 5.8 4.8
##  [793] 5.2 4.4 4.4 4.9 3.7 3.1 4.9 4.3 5.9 4.8 5.0 5.3 3.8 3.6 4.6 5.8 4.8 6.1
##  [811] 5.4 5.9 4.6 3.5 5.1 3.3 4.0 5.2 5.2 4.1 3.9 5.4 4.5 4.5 4.3 4.7 5.1 5.1
##  [829] 4.2 4.8 4.1 5.7 4.9 3.5 3.4 4.5 5.2 4.7 6.6 3.9 5.4 4.9 5.0 4.3 3.3 5.2
##  [847] 3.4 3.8 1.9 4.2 3.6 3.1 2.9 3.7 6.0 5.0 3.9 4.1 5.2 5.6 3.5 5.3 3.9 3.9
##  [865] 6.0 3.3 4.8 5.1 3.7 3.9 3.4 5.3 4.9 4.5 5.0 4.0 5.7 4.7 3.8 5.9 4.9 5.0
##  [883] 4.2 3.3 4.3 3.8 2.6 5.0 4.6 6.5 5.7 4.6 5.3 5.8 3.2 4.1 5.1 5.2 5.0 4.5
##  [901] 4.5 6.1 5.8 5.9 4.8 5.2 3.8 4.8 4.1 4.6 3.8 5.7 5.5 4.6 4.7 5.4 6.6 3.9
##  [919] 5.4 3.3 4.1 3.1 4.4 5.0 5.1 4.6 4.2 4.6 4.0 4.9 4.1 4.1 6.1 3.9 5.2 4.5
##  [937] 4.8 5.3 5.2 5.9 5.5 4.4 4.0 5.3 3.6 5.7 5.1 4.8 3.6 3.8 4.4 4.3 4.5 3.2
##  [955] 4.1 5.9 3.4 3.5 5.5 5.9 3.3 3.4 5.3 5.2 3.0 5.5 3.6 4.6 5.1 3.9 5.7 3.4
##  [973] 5.1 3.9 4.6 4.3 2.8 5.7 4.7 5.6 3.8 3.4 5.8 5.2 3.6 2.9 6.5 3.2 5.3 5.1
##  [991] 5.0 4.6 4.4 6.2 5.2 2.6 5.6 4.3 4.4 4.6
## 
## $func.thetastar
## 72% 
## 5.1 
## 
## $jack.boot.val
##  [1] 5.500 5.468 5.300 5.400 5.100 5.000 4.800 4.800 4.604 4.500
## 
## $jack.boot.se
## [1] 1.036439
## 
## $call
## bootstrap(x = x, nboot = 1000, theta = theta, func = perc72)</code></pre>
<p>On Tuesday we went over an example in which we bootstrapped the correlation coefficient between LSAT scores and GPA. To do that, we sampled pairs of (LSAT,GPA) data with replacement. Here is a little script that would do something like that using (X,Y) data that are independently drawn from the normal distribution</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">xdata&lt;-<span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">30</span>),<span class="dt">ncol=</span><span class="dv">2</span>)</code></pre></div>
<p>Everyone’s data is going to be different. With such a small sample size, it would be easy to get a positive or negative correlation by random change, but on average across everyone’s datasets, there should be zero correlation because the two columns are drawn independently.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n&lt;-<span class="dv">15</span>
theta&lt;-<span class="cf">function</span>(x,xdata)
  {
  <span class="kw">cor</span>(xdata[x,<span class="dv">1</span>],xdata[x,<span class="dv">2</span>])
  }
results&lt;-<span class="kw">bootstrap</span>(<span class="dt">x=</span><span class="dv">1</span><span class="op">:</span>n,<span class="dt">nboot=</span><span class="dv">50</span>,<span class="dt">theta=</span>theta,<span class="dt">xdata=</span>xdata) 
<span class="co">#NB: xdata is passed to the theta function, not needed for bootstrap function itself</span></code></pre></div>
<p>Notice the parameters that get passed to the ‘bootstrap’ function are: (1) the indexes which will be sampled with replacement. This is different that the raw data but the end result is the same because both the indices and the raw data get passed to the function ‘theta’ (2) the number of bootrapped samples (in this case 50) (3) the function to calculate the statistic (4) the raw data.</p>
<p>Lets look at a histogram of the bootstrapped statistics <span class="math inline">\(\theta^{*}\)</span> and draw a vertical line for the statistic as applied to the original data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(results<span class="op">$</span>thetastar,<span class="dt">breaks=</span><span class="dv">30</span>,<span class="dt">col=</span><span class="st">&quot;pink&quot;</span>)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">cor</span>(xdata[,<span class="dv">1</span>],xdata[,<span class="dv">2</span>]),<span class="dt">lwd=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="Week-2-Lab_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
<div id="parametric-bootstrap-1" class="section level2">
<h2><span class="header-section-number">4.5</span> Parametric bootstrap</h2>
<p>Let’s do one quick example of a parametric bootstrap. We haven’t introduced distributions yet (except for the Gaussian, or Normal, distribution, which is the most familiar), so lets spend a few minutes exploring the Gamma distribution, just so we have it to work with for testing out parametric bootstrap. All we need to know is that the Gamma distribution is a continuous, non-negative distribution that takes two parameters, which we call “shape” and “rate”. Lets plot a few examples just to see what a Gamma distribution looks like. (Note that the Gamma distribution can be parameterized by “shape” and “rate” OR by “shape” and “scale”, where “scale” is just 1/“rate”. R will allow you to use either (shape,rate) or (shape,scale) as long as you specify which you are providing.</p>
<p><img src="Week-2-Lab_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>Let’s generate some fairly sparse data from a Gamma distribution</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">original.data&lt;-<span class="kw">rgamma</span>(<span class="dv">10</span>,<span class="dv">3</span>,<span class="dv">5</span>)</code></pre></div>
<p>and calculate the skew of the data using the R function ‘skewness’ from the ‘moments’ package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(moments)
theta&lt;-<span class="kw">skewness</span>(original.data)
<span class="kw">head</span>(theta)</code></pre></div>
<pre><code>## [1] 0.20955</code></pre>
<p>What is skew? Skew describes how assymetric a distribution is. A distribution with a positive skew is a distribution that is “slumped over” to the right, with a right tail that is longer than the left tail. Alternatively, a distribution with negative skew has a longer left tail. Here we are just using it for illustration, as a property of a distribution that you may want to estimate using your data.</p>
<p>Lets use ‘fitdistr’ to fit a gamma distribution to these data. This function is an extremely handy function that takes in your data, the name of the distribution you are fitting, and some starting values (for the estimation optimizer under the hood), and it will return the parameter values (and their standard errors). We will learn in a couple weeks how R is doing this, but for now we will just use it out of the box. (Because we generated the data, we happen to know that the data are gamma distributed. In general we wouldn’t know that, and we will see in a second that our assumption about the shape of the data really does make a difference.)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(MASS)
fit&lt;-<span class="kw">fitdistr</span>(original.data,dgamma,<span class="kw">list</span>(<span class="dt">shape=</span><span class="dv">1</span>,<span class="dt">rate=</span><span class="dv">1</span>))
<span class="co"># fit&lt;-fitdistr(original.data,&quot;gamma&quot;)</span>
<span class="co"># The second version would also work.</span>
fit</code></pre></div>
<pre><code>##     shape       rate  
##   21.70207   38.67205 
##  ( 9.63180) (17.36300)</code></pre>
<p>Now lets sample with replacement from this new distribution and calculate the skewness at each step:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">results&lt;-<span class="kw">c</span>()
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">1000</span>)
  {
  x.star&lt;-<span class="kw">rgamma</span>(<span class="kw">length</span>(original.data),<span class="dt">shape=</span>fit<span class="op">$</span>estimate[<span class="dv">1</span>],<span class="dt">rate=</span>fit<span class="op">$</span>estimate[<span class="dv">2</span>])
  results&lt;-<span class="kw">c</span>(results,<span class="kw">skewness</span>(x.star))
  }
<span class="kw">head</span>(results)</code></pre></div>
<pre><code>## [1]  0.5617233  0.4855255  0.8723190 -0.2510972 -0.7146691  0.5216789</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(results,<span class="dt">breaks=</span><span class="dv">30</span>,<span class="dt">col=</span><span class="st">&quot;pink&quot;</span>,<span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="dt">freq=</span>F)</code></pre></div>
<p><img src="Week-2-Lab_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>Now we have the bootstrap distribution for skewness (the <span class="math inline">\(\theta^{*}\)</span> s), we can compare that to the equivalent non-parametric bootstrap:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">results2&lt;-<span class="kw">bootstrap</span>(<span class="dt">x=</span>original.data,<span class="dt">nboot=</span><span class="dv">1000</span>,<span class="dt">theta=</span>skewness)
results2</code></pre></div>
<pre><code>## $thetastar
##    [1] -0.584310600  0.496896534  0.360743890 -0.409449965 -0.614063986
##    [6] -0.487025126  0.102215426  0.682931648  0.583178560  0.168765647
##   [11] -0.038600117 -0.047610576 -0.233337509  1.168159526  0.729571753
##   [16]  0.392609575  0.231470575 -0.551957585 -0.431932245 -0.490279467
##   [21]  0.280976965  0.168174355  0.958698258  0.340126168  0.628050093
##   [26] -0.751154124 -0.042393035  0.500929916 -0.113512851  0.034554907
##   [31] -0.233494712 -0.012537256  0.420371488  0.459471783  0.176069245
##   [36]  0.429807383  0.394528980  0.960410092 -0.108918101  1.234054312
##   [41]  0.329619120  0.210749132 -1.141579977 -0.331045309 -0.070419662
##   [46]  0.425411498  0.160378828  0.708278338  0.702435659  0.694060357
##   [51]  0.379772311  0.082639477  0.505414161 -0.455046603  0.575463806
##   [56]  0.089446392  0.014051567  0.465907827 -0.378037720 -0.225311149
##   [61]  0.459987102 -0.166755292  0.566558030  0.847762940  0.632081769
##   [66] -0.156597265  0.012212813  0.908575968  0.322973416 -0.714711137
##   [71]  0.492834864 -0.534706567  0.086827339  0.892490874  0.129222463
##   [76] -0.091413763  0.906138305  0.670666844  0.495910461  0.351149571
##   [81]  0.095509744 -0.316152314 -0.132196791  0.116214239  1.188918398
##   [86] -0.312451771  0.553155659  0.520690111  0.151564450  1.095602906
##   [91]  0.480083133 -0.073186104 -0.073680952 -0.638840770  1.122738812
##   [96] -0.250161398 -0.034794401  0.159882281  0.480727538 -0.024462878
##  [101]  0.014546051 -0.601132674  0.673192875 -0.423677962  0.931878224
##  [106]  0.061900005 -0.127196668  0.629438510  1.000501634  0.308899927
##  [111]  0.056596946  0.386252865  0.008538002  0.179818088 -0.087634306
##  [116]  0.283190616 -0.789666765  0.257409231  0.111048494  0.180462367
##  [121]  0.119239327  0.398798141 -0.196816128  0.329312649  0.070613377
##  [126] -1.088659712  0.231283022  0.116415863  0.454745691  0.236218138
##  [131]  0.587989845  0.314426339 -0.548385525 -0.800044350  0.400047165
##  [136] -0.609688219  0.294859529  0.087417097  0.220204242 -0.084025003
##  [141]  0.403715591  0.501245604 -0.120718185  0.597709548  0.247123109
##  [146]  0.956451546  0.251847936  0.062661240  0.014336361  0.679328729
##  [151]  0.182880049  1.293270738 -0.010928548  0.567869665  0.491021072
##  [156] -0.199479862  0.447141556  0.379331018  0.506607130 -0.069035492
##  [161]  0.187468175 -0.567386832  0.154574812 -0.118255045  0.599263595
##  [166]  0.164692735 -0.388753658  0.541346079  0.735175175 -0.515535211
##  [171]  0.020220446  0.079694637  0.728567540  0.553460990  0.317365027
##  [176]  0.006642652  0.221730303  0.862080479 -0.184647966 -0.304079692
##  [181] -0.364612945  0.534427667  0.637733677  0.394543885  0.131634805
##  [186] -0.916052343 -0.172127298 -0.852765851  0.123729109  0.785946618
##  [191] -0.031232585 -0.529727216  0.220815420  0.209550037  0.967788526
##  [196]  0.185784465  0.326733414 -0.012374122 -0.075491841 -0.200952629
##  [201] -0.379499032  0.488295304  0.168588372  0.220348913  0.607752967
##  [206] -0.206046328  0.210430136  0.170397999  0.295256650  0.785345124
##  [211]  0.525663171  0.589585377 -0.819764910 -0.414334051  0.258681280
##  [216]  0.072029629  0.288966699  0.307293737 -0.255800931  0.248842681
##  [221] -0.366192160  0.372480084  0.268027883 -0.529469463 -0.032213554
##  [226] -0.356470470  0.163248252 -0.325158481  1.216960659 -0.154885364
##  [231]  0.297355539  0.789729383  0.351223015  0.167185744  0.675507053
##  [236]  0.583198802  0.368808943 -0.224078347  1.132255246 -0.116509648
##  [241]  0.014051567  0.489860945 -0.273945924  0.510347213  0.192216102
##  [246]  0.285668834  0.131660225  0.916628634  0.976484235  1.462398579
##  [251]  0.640220741  0.881074454 -0.015158034  0.334124762  0.702312597
##  [256]  0.396738594 -0.094016775  0.478729094  0.126746265  0.111876136
##  [261]  1.295366191  0.619177857 -0.274606480 -0.457086517 -0.967794865
##  [266]  0.307146138  0.155217164 -0.065832302  0.820410203  1.083398417
##  [271]  0.370941497  0.391106823 -0.025287096 -0.623769911 -0.710788444
##  [276]  0.910509338  0.478685651  0.572908073  0.501361725 -0.201092148
##  [281] -0.756141250  0.404992301 -0.599236581  0.277290136  0.148250127
##  [286] -0.752173391  0.794147368  0.282788444 -0.091195253  0.622353297
##  [291]  0.574942130  0.273473754  0.613914700 -0.667299543 -0.153706170
##  [296] -0.316834573  0.623968767 -0.097609129  0.719378529  0.379894823
##  [301] -0.077117585 -0.097366320 -0.452669976 -0.056883846  0.206798655
##  [306]  0.140904658  0.222744726  0.286859885  0.151365326  0.559898096
##  [311]  0.579241813  0.399199756  0.747174070  0.101213630  0.623797164
##  [316] -0.389656994 -0.136468760  0.030998335  0.824248710 -0.145612705
##  [321]  0.461653886  0.201647124  0.172170927  0.052815604  0.502300599
##  [326]  0.317840007 -0.228588932 -0.440749255  0.390339871  0.622087227
##  [331] -0.482813084  0.102934327  0.262786614 -0.380175206  0.729227060
##  [336]  0.100727436  0.232975441  0.902574378 -0.145258582 -0.059984860
##  [341] -0.273898009  0.230005085  0.461959970 -0.564897508  0.195157221
##  [346]  0.360050458  0.226321409  0.316680952  0.640220741 -0.375856546
##  [351]  0.229838019 -0.396464222  0.242766945  0.500571636  1.237682573
##  [356]  0.552365492  0.707605396  0.383454690  2.295770801  0.225935367
##  [361]  0.747610013  0.537632298 -0.007585074  0.206480235  0.100059307
##  [366]  0.077359225  0.208342158  0.010727805  0.207744811  0.157689212
##  [371] -0.559802013  0.515856499 -0.319284881 -0.029777485 -0.436815346
##  [376]  0.544705014  0.629518272  1.178252212  0.335021670 -0.303178490
##  [381]  0.708782701 -0.714791224 -0.150621040  0.125297407  0.772149300
##  [386] -0.120389919  0.185863188  0.091499774 -0.117489092  0.732360825
##  [391] -0.227294078  0.053586966  0.115130896  0.147746291  0.044723230
##  [396] -0.279235289 -0.010290203 -0.133254168  0.250416211 -0.460814543
##  [401]  0.075642643 -0.031646680 -0.360083699  0.584557963  0.531146310
##  [406]  0.912558422 -0.446502141  0.415461459 -0.378150696 -0.312537604
##  [411]  0.431368454 -0.070826929  0.591226571  0.521240828  0.129599534
##  [416]  0.475252409  0.143426586 -0.403122355  0.425784415  0.919117126
##  [421]  0.385850599 -0.073963186 -0.176346570  0.005534130  0.170536817
##  [426]  0.052890292 -0.748276063  0.098285225  0.955568673  0.403410378
##  [431]  0.092352136  1.029951221 -0.234155535  0.219477579  0.061500329
##  [436]  0.661598896  0.021834749  0.409723726  0.530897010  0.623797164
##  [441] -0.233335712 -0.154792053  0.513574558  0.232458919 -0.633798210
##  [446]  0.812649529  1.099714652  0.195626816  1.343341475 -0.005161731
##  [451]  0.177757541  0.474198118 -0.409124948  0.894227670  0.269044220
##  [456]  0.268825219 -0.006367433  0.196728526 -0.658065537 -0.815201253
##  [461]  0.379337850  0.247000236  0.206215772  0.027738876 -0.265944818
##  [466] -0.396464222 -0.557278731 -0.037527818  1.310776776 -0.422118425
##  [471]  0.569815680  0.761087386  0.198276289 -0.488075807  0.075646017
##  [476]  0.610336205  0.481016159  0.034820323  0.338077010  0.547924228
##  [481]  0.108726511 -0.342771066  0.294206995  0.140315711  0.117110714
##  [486]  0.194054416  0.244669778  0.269047572  0.183736555 -0.311071706
##  [491]  0.799961945 -0.296444212  0.792101716  0.016724721  0.559898096
##  [496]  0.307293737  0.253645084  0.501125769  0.364282760  0.047648182
##  [501]  0.044531960  0.750406935  0.415843530  0.596683962  0.340566474
##  [506] -0.525847144  0.810146115  0.040163601  0.996694228  0.482200254
##  [511]  0.625439702  0.187468175  0.032318483 -0.394653928 -0.229477342
##  [516]  0.264401977 -0.468847037 -0.010828083  0.436044516  0.485191823
##  [521]  0.165395932  0.591007834  0.083608793  0.247924733  0.413157160
##  [526]  0.074474088 -0.552771474 -0.061495030  0.905629700  0.719165291
##  [531]  0.569815680  0.281855047 -0.037238118  0.802047365 -0.196653039
##  [536]  0.436588983  0.303858367  0.478392472 -0.024760076  0.062752628
##  [541]  0.469420219  0.577870945 -0.340169418  0.135089263 -0.293456517
##  [546] -0.222421775  0.417143358  0.012561062  0.115769189  0.068190864
##  [551] -0.075855235 -0.516899832  0.323347892  0.234836389  1.166401061
##  [556] -0.101362321 -0.176268803 -0.041275596 -0.163411672 -0.761392840
##  [561] -0.991370396  0.356769626 -0.066310366 -0.522194650  0.612788177
##  [566]  0.183353026 -0.546843854  0.383568096  0.239233880 -0.072957541
##  [571]  0.073646000  0.049688570  0.421718625  0.288237951  0.248409414
##  [576]  1.259803351  0.373115584 -0.702517860 -0.216477658  1.023755713
##  [581]  0.462934393  0.395805791  0.853887442  0.024231784  0.362007623
##  [586] -0.067557788 -0.066354578 -0.258866092 -0.205270130  0.784391286
##  [591]  1.303500408  0.231283022 -0.595044989  0.058835962  0.277942940
##  [596] -0.651813801 -0.560174624  0.407295252 -0.410782971  0.317205775
##  [601]  0.476691096 -0.043892508  0.217078215  0.495218430 -0.229013762
##  [606]  1.071364315  0.421718625  0.017196806  0.013634270  0.531144903
##  [611]  0.230759317  0.880699581 -0.026868550  0.593150933  0.428384266
##  [616] -0.375494497 -0.143137534 -0.287645253  0.615621169  0.520733044
##  [621] -0.159063819 -0.522416999 -0.038986422  0.527485705  0.011449617
##  [626] -0.518773699  0.170983549 -0.137358913  0.479627129  1.153119078
##  [631] -0.253408686 -0.160021748 -0.039296317 -0.299509016 -0.688068823
##  [636]  0.377381506 -0.556388340  0.319891972 -0.645159116 -0.165766036
##  [641]  0.489389839 -0.270528735  0.006034065  0.045757135  0.380287844
##  [646] -0.341451427 -0.003854327 -0.483144907 -0.028913873 -0.343924954
##  [651] -0.236590525  0.279706562  0.584718744 -0.686853366  0.008629052
##  [656] -0.001423016 -0.563674575  1.241694645 -0.537183181  0.126665207
##  [661]  0.157675631 -0.037142860  0.799166622  0.303413590 -0.377927098
##  [666] -0.008060420 -0.006038365  0.068044265  0.492477012 -0.360704668
##  [671] -0.171424187 -0.111013333  0.170348513 -0.031554970 -0.552656876
##  [676] -0.149182361  0.038232695 -0.739794677 -0.338679337  0.266336095
##  [681] -0.989832920  0.133610945 -0.449388303  0.842223358  0.001590991
##  [686] -0.592141287 -0.015823674  0.674864060  0.239429873 -0.253063688
##  [691]  0.098409806  0.200846168 -0.285741643  1.552687053  0.044531960
##  [696]  0.093148682  0.586752945  0.952405059  1.344961813  0.048637908
##  [701]  0.568648908  0.343601522  0.828503957  0.312418501  0.932598350
##  [706]  0.168326053 -0.070312498 -0.099383773 -0.450459191 -0.150049330
##  [711]  0.234544751  0.869191629  0.322351855  0.399121915 -0.050237393
##  [716] -0.601768700  0.615239262  0.248280633  0.076051106  0.714734200
##  [721]  0.504540682  0.468997014  0.389476750  0.555420866  0.080872712
##  [726] -0.610593802  0.666130615  0.017916060  0.058256992  0.031322244
##  [731]  1.048021032 -0.297598960  0.046783883 -0.459150055 -0.607933115
##  [736]  1.143059157  0.312846373  0.539462237  0.629990701 -0.919211194
##  [741]  0.555201882  0.209216403 -0.326340251  1.557731306  0.144522419
##  [746] -0.431305636  0.399121915  0.289465628 -0.082346632 -0.221315915
##  [751] -0.128092094  1.020481548  0.490589016 -0.060648263  0.643124622
##  [756] -0.207942443 -0.261514553  1.498760160 -0.390542283  0.526979477
##  [761]  1.004319506 -0.072644850  0.362217354 -0.031646680  0.429255993
##  [766]  1.418645914  0.361318804  0.188353173  0.961778356 -0.609620260
##  [771]  0.375357657  0.094748223  0.273676218 -0.510599706  0.443031409
##  [776] -0.530400837  0.273676218  1.425026496 -0.392569363 -0.130091166
##  [781]  0.199319669  0.846080168 -0.216151363  0.039255112 -0.170190497
##  [786]  0.190803051  0.187553231  0.220152695 -0.023599780  0.574069155
##  [791]  0.646414770  1.613099924  0.845654110  0.374858769 -0.217374280
##  [796] -0.195297772 -0.425238010 -0.558362132  0.299825280  0.560316426
##  [801]  0.325874231  0.125743076 -0.152339962  0.145912806  0.944082849
##  [806] -0.207735166  0.820910604  0.670078840 -0.381952046  0.035960381
##  [811]  0.343881602  0.446092578 -0.264302416  1.019463865 -1.034653334
##  [816]  0.229200577 -0.130140820  0.196859791 -0.176932587 -0.088402620
##  [821]  0.995868456  0.544705014  0.159285047 -0.160021748 -0.500741023
##  [826] -0.430664378  1.059335485  0.541482916  0.708231512 -0.225316271
##  [831]  1.179459963  0.430286442 -0.223154820 -0.585943877  0.286815796
##  [836]  0.096979054  0.947482621  0.184257757 -0.404594932 -0.268348152
##  [841]  0.271579491  0.032069814 -0.139870284  0.584752357 -0.029720186
##  [846] -0.190461117  0.855631623 -0.130374604 -0.580818533 -0.305811467
##  [851] -0.018864267 -0.465983682  0.262892350 -0.249258048  0.379060415
##  [856] -0.636426236  0.375860718 -0.746616641  0.030679800 -0.098743112
##  [861]  0.199698752  0.070613377  0.275278232 -0.199664573  0.801319563
##  [866] -0.519428943  0.029095166 -0.010178079 -0.285294146 -0.060338247
##  [871] -0.299934837  0.632824051  0.807587958 -0.734862093 -0.062885886
##  [876] -0.396369963 -0.121987779  0.086159574 -0.739191512  0.465525004
##  [881]  0.547924228 -0.639304727  0.977273348 -0.106414022  0.892490874
##  [886]  0.284135572  0.280279984  0.078341234  0.001815884  0.116265851
##  [891]  0.189932864  0.430689914  0.467597245  0.220863042 -0.098601450
##  [896]  0.297097582 -0.621308098 -0.739191512  0.777876422 -0.170407052
##  [901]  0.159090649  0.343040953  1.079978737 -0.435152841 -0.907679258
##  [906]  0.273283490  0.829065510  0.586710446  0.520593505 -0.046986534
##  [911]  0.897778694 -0.366533687 -0.601487115 -0.111975352  0.493740079
##  [916] -0.047194747  0.221711644  1.142604361  0.084190692  0.963076163
##  [921]  0.900472221 -0.164304440  0.775921000  0.016015640 -0.119565208
##  [926]  0.353390363  1.101948369  0.469464163  1.766180266  0.977757288
##  [931] -0.534040640  0.804766660  0.266192508  0.458712645  0.006236778
##  [936]  1.156491940 -0.420233078  0.434695022  0.181419907 -0.214247642
##  [941]  0.137241414  0.069004694  0.150943077  0.821827470  0.378084779
##  [946] -0.311825732  0.437769531 -0.081834009  0.340547572 -0.586242036
##  [951] -0.258180502  0.249790754  0.033438439  0.330152574 -0.391765519
##  [956]  0.160239970 -0.016463789 -0.362931893  0.433322967 -0.341326708
##  [961] -0.242204546  0.531610617  0.115279778  0.433240540  0.743386505
##  [966]  0.703761986  1.089143896  0.287648200 -0.424089448  0.300944793
##  [971]  0.692772060 -0.735639651  0.535308122  0.961747594  0.607634541
##  [976]  0.281094073 -0.259969905  0.941221554  0.195528396  0.378900263
##  [981]  1.093171392  0.871986029  0.144009296  0.490531250  0.151555378
##  [986]  1.104600776  0.311178273  0.821622274  0.582177536 -0.441494841
##  [991]  0.064191225  0.376203518  0.897993395  0.591575775 -0.304599184
##  [996]  0.085241289  0.236725787  0.556333396 -0.234532513 -0.248026970
## 
## $func.thetastar
## NULL
## 
## $jack.boot.val
## NULL
## 
## $jack.boot.se
## NULL
## 
## $call
## bootstrap(x = original.data, nboot = 1000, theta = skewness)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(results,<span class="dt">breaks=</span><span class="dv">30</span>,<span class="dt">col=</span><span class="st">&quot;pink&quot;</span>,<span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="dt">freq=</span>F)
<span class="kw">hist</span>(results2<span class="op">$</span>thetastar,<span class="dt">breaks=</span><span class="dv">30</span>,<span class="dt">border=</span><span class="st">&quot;purple&quot;</span>,<span class="dt">add=</span>T,<span class="dt">density=</span><span class="dv">20</span>,<span class="dt">col=</span><span class="st">&quot;purple&quot;</span>,<span class="dt">freq=</span>F)</code></pre></div>
<p><img src="Week-2-Lab_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>What would have happened if we would have fit a normal distribution instead of a gamma distribution?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit2&lt;-<span class="kw">fitdistr</span>(original.data,dnorm,<span class="dt">start=</span><span class="kw">list</span>(<span class="dt">mean=</span><span class="dv">1</span>,<span class="dt">sd=</span><span class="dv">1</span>))</code></pre></div>
<pre><code>## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced

## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced

## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced

## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced

## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced

## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced

## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced

## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced

## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced

## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced

## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced

## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced

## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit2</code></pre></div>
<pre><code>##       mean          sd    
##   0.56114990   0.12020121 
##  (0.03801096) (0.02686921)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">results.norm&lt;-<span class="kw">c</span>()
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">1000</span>)
  {
  x.star&lt;-<span class="kw">rnorm</span>(<span class="kw">length</span>(original.data),<span class="dt">mean=</span>fit2<span class="op">$</span>estimate[<span class="dv">1</span>],<span class="dt">sd=</span>fit2<span class="op">$</span>estimate[<span class="dv">2</span>])
  results.norm&lt;-<span class="kw">c</span>(results.norm,<span class="kw">skewness</span>(x.star))
  }
<span class="kw">head</span>(results.norm)</code></pre></div>
<pre><code>## [1]  0.2075966 -0.1054708  0.6885978  0.5295741 -0.4246298 -0.2727695</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(results,<span class="dt">breaks=</span><span class="dv">30</span>,<span class="dt">col=</span><span class="st">&quot;pink&quot;</span>,<span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="dt">freq=</span>F)
<span class="kw">hist</span>(results.norm,<span class="dt">breaks=</span><span class="dv">30</span>,<span class="dt">col=</span><span class="st">&quot;lightgreen&quot;</span>,<span class="dt">freq=</span>F,<span class="dt">add=</span>T)
<span class="kw">hist</span>(results2<span class="op">$</span>thetastar,<span class="dt">breaks=</span><span class="dv">30</span>,<span class="dt">border=</span><span class="st">&quot;purple&quot;</span>,<span class="dt">add=</span>T,<span class="dt">density=</span><span class="dv">20</span>,<span class="dt">col=</span><span class="st">&quot;purple&quot;</span>,<span class="dt">freq=</span>F)</code></pre></div>
<p><img src="Week-2-Lab_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>All three methods (two parametric and one non-parametric) really do give different distributions for the bootstrapped statistic, so the choice of which method is best depends a lot on the situation, how much data you have, and what you might already know about the underlying distribution.</p>
<p>Jackknifing is just as easy at bootstrapping. Here we will do a trivial example for illustration. We will write a little function for the mean even though you could put the function in directly with ‘jackknife(x,mean)’</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">theta&lt;-<span class="cf">function</span>(x)
  {
  <span class="kw">mean</span>(x)
  }
x&lt;-<span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">9</span>,<span class="dt">by=</span><span class="dv">1</span>)
results&lt;-<span class="kw">jackknife</span>(<span class="dt">x=</span>x,<span class="dt">theta=</span>theta)
results</code></pre></div>
<pre><code>## $jack.se
## [1] 0.9574271
## 
## $jack.bias
## [1] 0
## 
## $jack.values
##  [1] 5.000000 4.888889 4.777778 4.666667 4.555556 4.444444 4.333333 4.222222
##  [9] 4.111111 4.000000
## 
## $call
## jackknife(x = x, theta = theta)</code></pre>
<p>Why do we not have to tell the ‘jackknife’ function how many replicates to do?</p>
<p>Let’s compare this with what we would have obtained from bootstrapping</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">results2&lt;-<span class="kw">bootstrap</span>(x,<span class="dv">1000</span>,theta)
<span class="kw">mean</span>(results2<span class="op">$</span>thetastar)<span class="op">-</span><span class="kw">mean</span>(x)  <span class="co">#this is the bias</span></code></pre></div>
<pre><code>## [1] -0.0232</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(results2<span class="op">$</span>thetastar)  <span class="co">#the standard deviation of the theta stars is the SE of the statistic (in this case, the mean)</span></code></pre></div>
<pre><code>## [1] 0.9158715</code></pre>
<p>Everything we have done to this point used the R package ‘bootstrap’ - now lets compare that with the R package ‘boot’. To avoid any confusion (a.k.a. masking) between the two packages, I recommend detaching the bootstrap package from the workspace with</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">detach</span>(<span class="st">&quot;package:bootstrap&quot;</span>)</code></pre></div>
<p>The ‘boot’ package is now recommended over the ‘bootstrap’ package, but they give the same answers and to some extent it is personal preference which one prefers to use.</p>
<p>We will still use the mean as the statistic of interest, but we will have to write a new function for it because the syntax of the ‘boot’ package is slightly different:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(boot)
theta&lt;-<span class="cf">function</span>(x,index)
  {
  <span class="kw">mean</span>(x[index])
  }
<span class="kw">boot</span>(x,theta,<span class="dt">R=</span><span class="dv">999</span>)</code></pre></div>
<pre><code>## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = x, statistic = theta, R = 999)
## 
## 
## Bootstrap Statistics :
##     original       bias    std. error
## t1*      4.5 0.0002002002   0.9047304</code></pre>
<p>One of the main advantages to the ‘boot’ package over the ‘bootstrap’ package is the nicer formatting of the output.</p>
<p>Going back to our original code, lets see how we could reproduce all of these numbers:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">table</span>(<span class="kw">sample</span>(x,<span class="dt">size=</span><span class="kw">length</span>(x),<span class="dt">replace=</span>T))</code></pre></div>
<pre><code>## 
## 1 3 4 5 7 9 
## 1 1 3 2 1 2</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">xmeans&lt;-<span class="kw">vector</span>(<span class="dt">length=</span><span class="dv">1000</span>)
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">1000</span>)
  {
  xmeans[i]&lt;-<span class="kw">mean</span>(<span class="kw">sample</span>(x,<span class="dt">replace=</span>T))
  }
<span class="kw">mean</span>(x)</code></pre></div>
<pre><code>## [1] 4.5</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">bias&lt;-<span class="kw">mean</span>(xmeans)<span class="op">-</span><span class="kw">mean</span>(x)
se.boot&lt;-<span class="kw">sd</span>(xmeans)
bias</code></pre></div>
<pre><code>## [1] -0.0481</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">se.boot</code></pre></div>
<pre><code>## [1] 0.9175174</code></pre>
<p>Why do our numbers not agree exactly with those of the boot package? This is because our estimates of bias and standard error are just estimates, and they carry with them their own uncertainties. That is one of the reasons we might bother doing jackknife-after-bootstrap.</p>
<p>The ‘boot’ package has a LOT of functionality. If we have time, we will come back to some of these more complex functions later in the semester as we cover topics like regression and glm.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="week-2-lecture.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="week-3-lecture.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
