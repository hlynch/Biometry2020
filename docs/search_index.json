[
["index.html", "Biometry Lecture and Lab Notes Preface", " Biometry Lecture and Lab Notes Heather Lynch 2019-11-13 Preface This eBook contains all of the lecture notes and lab exercises that we will do this semester in Biometry. While I have made every effort to cite sources where I’ve used “found” material, this eBook reflects my own personal notes drawn up over nearly a decade of work and some material may not properly identify the original sources used in drawing up my initial lecture notes. As I have moved this material into an online eBook, I have tried to better document material inspired by or drawn from other sources. If you find anything in these notes that is not properly cited or sourced, please let me know so it can be amended. Any mistakes are mine and mine alone. "],
["week-1-lecture.html", "1 Week 1 Lecture 1.1 Reading Material 1.2 Basic Outline 1.3 Today’s Agenda 1.4 Basic Probability Theory 1.5 Multiple events 1.6 Conditionals 1.7 Bayes Theorem 1.8 A few foundational ideas 1.9 Overview of Univariate Distributions 1.10 What can you ask of a distribution? 1.11 A Brief Introduction to Scientific Method", " 1 Week 1 Lecture 1.1 Reading Material Text Foundational and Applied Statistics for Biologists Using R Biostatistical Design and Analysis Using R Papers from prmiary literature 1.2 Basic Outline First half: - R - bootstrap, jackknife, and other randomization techniques - hypothesis testing - probability distributions - the “classic tests” of statistics - graphical analysis of data Second half: regression (incl. ANOVA, ANCOVA) model building model criticism non-linear regression multivariate regression Grading: 12 Problem sets (3% each, 36% total) 2 Quizzes (2% each, 4% total) Midterm (25%) Final (35%) Class Structure Lecture on Tuesday “Lab”&quot; on Thursday Problem sets are posted on Wednesdays (feel free to remind me via Slack if I forget), and are due before lecture next Tuesday. This deadline is very strict, no exceptions. Turn in what you have before 8:30 am on Tuesday, even if its not complete. Communication Use slack! Come to (both) office hours E-mail me Weekdays 9 am - 10 pm Sundays 8 pm - 10 pm 1.3 Today’s Agenda Basic probability theory An overview of univariate distributions Calculating the expected value of a random variable A brief introduction to the Scientific Method Introduction to statistical inference 1.4 Basic Probability Theory A bag with a mix of regular and peanut M&amp;Ms Each M&amp;M has two traits: Color and Type \\[ \\sum_{all \\: colors} P(color) = 1 \\] \\[ \\sum_{all \\: types} P(types) = ? \\] Basic Probability Theory | Intersection Pull one M&amp;M out of the bag \\[ P(Green \\: AND \\: Peanut) = P(Green \\cap Peanut) = P(Green) \\cdot P(Peanut) \\] This is called a Joint Probability: \\(P(Green,Peanut)\\) Union: \\[ \\begin{align*} P(Green \\: OR \\: Peanut) &amp;= P(Green \\cup Peanut) \\\\ &amp;= P(Green) + P(Peanut) - P(Green \\cap Peanut) \\end{align*} \\] Complement: \\[ P(Green^c) = 1 - P(Green) \\] 1.5 Multiple events Pull 2 M&amp;Ms out of the bag \\[ P (Green \\: AND \\: THEN \\: Blue) = P(Green) \\cdot P(Blue) \\] What if we didn’t care about the order? 1.6 Conditionals \\[ P(A \\mid B) = P(A \\: conditional \\: on \\: B) \\] \\[ \\begin{align*} P(A,B) = P(A \\cap B) &amp;= P(A \\mid B) \\cdot P(B) \\\\ &amp;= P(B \\mid A) \\cdot P(A) \\end{align*} \\] 1.7 Bayes Theorem \\[ P(A \\mid B) \\cdot P(B) = P(B \\mid A) \\cdot P(A) \\] \\[ P(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}\\] \\[ P(parameters \\mid data) \\cdot P(data) = P(data \\mid parameters) \\cdot P(parameters) \\] \\[ P(parameters \\mid data) = \\frac{P(data \\mid parameters) \\cdot P(parameters)}{P(data)}\\] 1.8 A few foundational ideas There are a few statistics (a statistic is just something calculated from data) that we will need to know right at the beginning. For illustration purposes, lets assume we have the following (sorted) series of data points: (1,3,3,4,7,8,13) There are three statistics relating the “central tendancy”: the mean (the average value; 5.57), the mode (the most common value; 3), and the median (the “middle” value; 4). We often denote the mean of a variable with a bar, as in \\(\\bar{x}\\). There are also two statistics relating to how much variation there is in the data. The variance measures the average squared distance between each point and the mean. For reasons that we will discuss in lab, we estimate the variance using the following formula \\[ \\mbox{variance}_{unbiased} = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2} \\] rather than the more intuitive \\[ \\mbox{variance}_{biased} = \\frac{1}{n}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2} \\] Keep in mind that variance measures a distance squared. So if your data represent heights in m, than the variance will have units \\(m^{2}\\) or square-meters. The standard deviation is simply the square-root of variance, and is often denoted by the symbol \\(\\sigma\\). \\[ \\sigma = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}} \\] If you were handed a distribution and you were asked to measure a characteristic “fatness” for the distribution, your estimate would be approximately \\(\\sigma\\). Note that \\(\\sigma\\) has the same units as the original data, so if your data were in meters, \\(\\sigma\\) would also be in meters. We won’t get to Normal distributions properly until Week 3, but we will need one fact about the “Standard Normal Distribution” now. The Standard Normal distribution is a Normal (or Gaussian, bell-shaped) distribution with mean equal to zero and standard deviation equal to 1. 68\\(\\%\\) of the probability is contained within 1 standard deviation of the mean (so from -\\(\\sigma\\) to +\\(\\sigma\\)), and 95\\(\\%\\) of the probability is contained within 2 standard deviations of the mean (so from -2\\(\\sigma\\) to +2\\(\\sigma\\)). (Actually, 95\\(\\%\\) is contained with 1.96 standard deviations, so sometimes we will use the more precise 1.96 and sometimes you will see this rounded to 2.) 1.9 Overview of Univariate Distributions Discrete Distributions - Binomial - Multinomial - Poisson - Geometric Continuous Distributions - Normal/Gaussian - Beta - Gamma - Student’s t - \\(\\chi^2\\) 1.10 What can you ask of a distribution? Probability Density Function: \\(P(x_1&lt;X&lt;x_2)\\) (continuous distributions) Probability Mass Function: \\(P(X=x_1)\\) (discrete distributions) Cumulative Density Function (CDF): What is \\(P(X \\le X^*)\\)? Quantiles of the distributions: What is \\(X^{*}\\) if \\(P(X \\le X^{*})=0.37\\)? Sample from the distribution: With a large enough sample, the histogram will come very close to the underlying PDF. Note that the CDF is the integration of the PDF, and the PDF is the derivative of the CDF, so if you have one of these you can always get the other. Likewise, you can always get from the quantiles to the CDF (and then to the PDF). These three things are all equally informative about the shape of the distribution. Expected Value of a Random Variable In probability theory the expected value of a random variable is the weighted average of all possible values that this random variable can take on. The weights used in computing this average correspond to the probabilities in case of a discrete random variable, or densities in case of continious random variable. Expected Value of a Random Variable | Discrete Case \\[ X = \\{X_1, X_2,...,X_k\\} \\\\ E[X] = \\sum_{i=1}^n{X_i \\cdot P(X_i)}\\] Example: Draw numbered balls with numbers 1, 2, 3, 4 and 5 with probabilities 0.1, 0.1, 0.1, 0.1, 0.6. \\[ \\begin{align*} E[X] &amp;= (0.1 \\cdot 1) + (0.1 \\cdot 2) + (0.1 \\cdot 3) + (0.1 \\cdot 4) + (0.6 \\cdot 5) \\\\ &amp;=4 \\end{align*}\\] Expected Value of a Random Variable | Continuous Case \\[ E[X] = \\int_{-\\infty}^{\\infty}{X \\cdot f(X)dX}\\] 1.11 A Brief Introduction to Scientific Method INDUCTIVE reasoning: A set of specific observations -&gt; A general principle Example: I observe a number of elephants and they were all gray. Therefore, all elephants are gray. DEDUCTIVE reasoning: A general principle -&gt; A set of predictions or explanations Example: All elephants are gray. Therefore, I predict that this new (as yet undiscovered) species of elephant will be gray. QUESTION: If this new species of elephant is green, what does this do to our hypothesis that all elephants are gray? Some terminology: Null Hypothesis: A statement encapsulating “no effect” Alternative Hypothesis: A statement encapsulating “an effect”&quot; Fisher: Null hypothesis only Neyman and Pearson: H0 and H1, Weigh risk of of false positive against the false negative We use a hyprid approach Hypothesis can only be rejected, they can never be accepted! “Based on the data obtained, we reject the null hypothesis that…” or “Based on the data obained, we fail to reject the null hypothesis that…” More terminology Population: Entire collection of individuals a researcher is interested in. Model: Mathematical description of the quantity of interest. It combines a general description (functional form) with parameters (population parameters) that take specific values. Population paramater: Some measure of a population (mean, standard deviation, range, etc.). Because populations are typically very large this quantity is unknown (and usually unknowable). Sample: A subset of the population selected for the purposes of making inference for the popualtion. Sample Statistic: Some measure of this sample that is used to infer the true value of the associated populatipn parameter. An example: Population: Fish density in a lake Sample: You do 30 net tows and count all the fish in each tow Model: \\(Y_i \\sim Binom(p,N)\\) The basic outline of statistical inference sample(data) -&gt; sample statistics -&gt; ESTIMATOR -&gt; population parameter -&gt; underlying distribution Estimators are imperfect tools Bias: The expected value \\(\\neq\\) population parameter Not consistent: As \\(n \\to \\infty\\) sample statistic \\(\\neq\\) population parameter Variance "],
["week-1-lab-handout.html", "2 Week 1 Lab Handout 2.1 Using R like a calculator 2.2 The basic data structures in R 2.3 Writing functions in R 2.4 Writing loops and if/else 2.5 (A short diversion) Bias in estimators 2.6 Lesson #6: Some practice writing R code 2.7 A few final notes", " 2 Week 1 Lab Handout In lab today, we will cover just a few of the basic elements of using R. If you are not already fluent with R, you should work through all of Logan Chapter 1, as there are many important elements covered in that chapter that we will not have time to go through in lab. I will assume everyone is using RStudio to run R - exercises and associated code will be written accordingly. For the purposes of lab, we will be entering all of the commands directly on the command line at the prompt. In general, however, you should get into the habit of writing scripts with all of your code. This will allow you to save your work and go back and easily use code that you have written in the past. When you are doing actual analyses for publication, it is essential that you have all of your code in well commented scripts that could be understood by another researcher in your field. All of your analysis should be fully reproducible long after the paper is published. 2.1 Using R like a calculator R can be used like a basic calculator with commands entered at the R prompt ‘&gt;’ 3+5 ## [1] 8 3*5 ## [1] 15 (The “##” is the html editor’s mechanism for indicating R output. The [1] is something that actually appears in the R output. It indicates that what you see is the first element of the output and in this case it can be ignored.) While R is fairly clever about the order of operations 3*5+7 ## [1] 22 it is good practice to be explicit (3*5)+7 ## [1] 22 Notice that the following two expressions are equivalent 4^(1/2) ## [1] 2 sqrt(4) ## [1] 2 The former expression uses the ^ to signify an exponent, whereas the latter uses the built-in R function sqrt() for the square-root. Note the difference between -4^(1/2) ## [1] -2 and (-4)^(1/2) ## [1] NaN In the first instance, R does the sqrt() and THEN applied the negative, whereas in the second case, it is trying to take the square-root of a negative number and it spits back NaN for ‘Not a Number’. To use scientific notation, use e. 2.2e3 ## [1] 2200 For large numbers, R automatically uses scientific notation although the threshold for scientific notation is something you can change using the R function ‘options’ (advanced use only - don’t worry about it for now). 2.2e3*5e5 ## [1] 1.1e+09 2.2 The basic data structures in R There are several basic types of data structures in R. VECTORS: One-dimensional arrays of numbers, character strings, or logical values (T/F) FACTORS: One-dimensional arrays of factors (Stop - Let’s discuss factors) DATA FRAMES: Data tables in which the various columns may be of different type MATRICES: In R, matrices can only be 2-dimensional, anything higher dimension is called an array (see below). Matrix elements are typically (but not necessarily) numerical, but the key difference from a data frame is that every element has to have the same type. Some functions, like the transpose function t(), only work on matrices. ARRAYS: higher dimensional matrices are called arrays in R LISTS: lists can contain any type of object as list elements. You can have a list of numbers, a list of matrices, a list of characters, etc., or any combination of the above. Vectors: Vectors can be column vectors or row vectors but we are almost always talking about column vectors which are defined with a c(). One example of a vector would be a sequence of numbers. There are many ways to generate sequences in R. Lets say you want to define an object x as the following sequence of numbers (1,2,3,4,5,6,7) You could do this this long way x&lt;-c(1,2,3,4,5,6,7) Notice here I have used the &lt;- to “assign” the column vector (hence “c”) of values to the variable x. I could also do this using x&lt;-1:7 or I could explicitly use the seq() function as follows x&lt;-seq(from=1,to=7) The sequence function actually has three inputs, but I have left the last off because the default is that you want to step in increments of 1. The full version would be x&lt;-seq(from=1,to=7,by=1) Make sure this works by printing out the value for x x Try changing it up a little with x&lt;-seq(from=1,to=16,by=3) STOP: Spend a few minutes making vectors and using some of the basic R commands. What happens if you pass a vector to one of R’s built-in functions? R can do a host of logical operations. x&lt;7 ## [1] TRUE TRUE FALSE FALSE FALSE FALSE We can turn that into a binary vector in at least two ways as.numeric(x&lt;7) ## [1] 1 1 0 0 0 0 or 1*(x&lt;7) ## [1] 1 1 0 0 0 0 The former forces R to return the values of x as a numerical vector, and by default False maps to 0 and True to 1. The latter version does the same thing, by multiplying the logical vector by a number. This trick comes in handy all the time. For example, if you want to know how many values of x are less than 7, you can simply do the following sum(as.numeric(x&lt;7)) ## [1] 2 You can also ask which elements satisfy certain criteria. In other words, you can type which(x&lt;7) ## [1] 1 2 This is telling you that the first and second elements of the vector are less than 7. We can take a random set of numbers y&lt;-c(4,8,6,3,6,9,2) and sort them sort(y) ## [1] 2 3 4 6 6 8 9 or reverse sort them rev(sort(y)) ## [1] 9 8 6 6 4 3 2 To pull up the help file for the R command ‘sort’: ?sort STOP: Let’s take this opportunity to go through all the elements of an R help file. We can also print out the rank of each value rank(y) ## [1] 3.0 6.0 4.5 2.0 4.5 7.0 1.0 Notice that ties got averaged. Elements of vectors in R are addressed using [] as follows First lets make a vector z z&lt;-seq(from=1,to=15,by=2) We can find the 4th element by simply typing z[4] ## [1] 7 or we can find the 3rd and 4th elements by typing z[c(3,4)] ## [1] 5 7 In this more complicated case, we create a vector of the indices we want, and feed that into the brackets. We can do the opposite as well, instead of pulling out a set of elements you want, you can excise a set of elements and print everything else. In other words, if you wanted all the elements BUT element 3, you would use the minus sign z[-3] ## [1] 1 3 7 9 11 13 15 Factors: To explore factors, we will use the dataset Prestige.csv I posted on Blackboard. To simply everything that follows, I will set the working directory to my own folder for this week’s lab. This will allow me to reference files within this folder without the entire file name. Load the data Prestige&lt;-read.csv(&quot;~/Dropbox/Biometry/Week 1 Preliminary Material/Week 1 Lab/Prestige.csv&quot;) We can look at the entire data set by typing the name at the command prompt, but we can also just look at the first few lines using the ‘head’ function head(Prestige) ## X education income women prestige census type ## 1 gov.administrators 13.11 12351 11.16 68.8 1113 prof ## 2 general.managers 12.26 25879 4.02 69.1 1130 prof ## 3 accountants 12.77 9271 15.70 63.4 1171 prof ## 4 purchasing.officers 11.42 8865 9.11 56.8 1175 prof ## 5 chemists 14.62 8403 11.68 73.5 2111 prof ## 6 physicists 15.64 11030 5.13 77.6 2113 prof or the last few lines using the ‘tail’ function tail(Prestige) ## X education income women prestige census type ## 97 train.engineers 8.49 8845 0.00 48.9 9131 bc ## 98 bus.drivers 7.58 5562 9.47 35.9 9171 bc ## 99 taxi.drivers 7.93 4224 3.59 25.1 9173 bc ## 100 longshoremen 8.37 4753 0.00 26.1 9313 bc ## 101 typesetters 10.00 6462 13.58 42.2 9511 bc ## 102 bookbinders 8.55 3617 70.87 35.2 9517 bc We can also get the dimensions of the data set using the ‘dim’ function dim(Prestige) ## [1] 102 7 or use the ‘length’ function to figure out the length of one of the columns length(Prestige[,1]) ## [1] 102 Factors are character labels which take fixed values. First just look at the data. Prestige ## X education income women prestige census type ## 1 gov.administrators 13.11 12351 11.16 68.8 1113 prof ## 2 general.managers 12.26 25879 4.02 69.1 1130 prof ## 3 accountants 12.77 9271 15.70 63.4 1171 prof ## 4 purchasing.officers 11.42 8865 9.11 56.8 1175 prof ## 5 chemists 14.62 8403 11.68 73.5 2111 prof ## 6 physicists 15.64 11030 5.13 77.6 2113 prof ## 7 biologists 15.09 8258 25.65 72.6 2133 prof ## 8 architects 15.44 14163 2.69 78.1 2141 prof ## 9 civil.engineers 14.52 11377 1.03 73.1 2143 prof ## 10 mining.engineers 14.64 11023 0.94 68.8 2153 prof ## 11 surveyors 12.39 5902 1.91 62.0 2161 prof ## 12 draughtsmen 12.30 7059 7.83 60.0 2163 prof ## 13 computer.programers 13.83 8425 15.33 53.8 2183 prof ## 14 economists 14.44 8049 57.31 62.2 2311 prof ## 15 psychologists 14.36 7405 48.28 74.9 2315 prof ## 16 social.workers 14.21 6336 54.77 55.1 2331 prof ## 17 lawyers 15.77 19263 5.13 82.3 2343 prof ## 18 librarians 14.15 6112 77.10 58.1 2351 prof ## 19 vocational.counsellors 15.22 9593 34.89 58.3 2391 prof ## 20 ministers 14.50 4686 4.14 72.8 2511 prof ## 21 university.teachers 15.97 12480 19.59 84.6 2711 prof ## 22 primary.school.teachers 13.62 5648 83.78 59.6 2731 prof ## 23 secondary.school.teachers 15.08 8034 46.80 66.1 2733 prof ## 24 physicians 15.96 25308 10.56 87.2 3111 prof ## 25 veterinarians 15.94 14558 4.32 66.7 3115 prof ## 26 osteopaths.chiropractors 14.71 17498 6.91 68.4 3117 prof ## 27 nurses 12.46 4614 96.12 64.7 3131 prof ## 28 nursing.aides 9.45 3485 76.14 34.9 3135 bc ## 29 physio.therapsts 13.62 5092 82.66 72.1 3137 prof ## 30 pharmacists 15.21 10432 24.71 69.3 3151 prof ## 31 medical.technicians 12.79 5180 76.04 67.5 3156 wc ## 32 commercial.artists 11.09 6197 21.03 57.2 3314 prof ## 33 radio.tv.announcers 12.71 7562 11.15 57.6 3337 wc ## 34 athletes 11.44 8206 8.13 54.1 3373 &lt;NA&gt; ## 35 secretaries 11.59 4036 97.51 46.0 4111 wc ## 36 typists 11.49 3148 95.97 41.9 4113 wc ## 37 bookkeepers 11.32 4348 68.24 49.4 4131 wc ## 38 tellers.cashiers 10.64 2448 91.76 42.3 4133 wc ## 39 computer.operators 11.36 4330 75.92 47.7 4143 wc ## 40 shipping.clerks 9.17 4761 11.37 30.9 4153 wc ## 41 file.clerks 12.09 3016 83.19 32.7 4161 wc ## 42 receptionsts 11.04 2901 92.86 38.7 4171 wc ## 43 mail.carriers 9.22 5511 7.62 36.1 4172 wc ## 44 postal.clerks 10.07 3739 52.27 37.2 4173 wc ## 45 telephone.operators 10.51 3161 96.14 38.1 4175 wc ## 46 collectors 11.20 4741 47.06 29.4 4191 wc ## 47 claim.adjustors 11.13 5052 56.10 51.1 4192 wc ## 48 travel.clerks 11.43 6259 39.17 35.7 4193 wc ## 49 office.clerks 11.00 4075 63.23 35.6 4197 wc ## 50 sales.supervisors 9.84 7482 17.04 41.5 5130 wc ## 51 commercial.travellers 11.13 8780 3.16 40.2 5133 wc ## 52 sales.clerks 10.05 2594 67.82 26.5 5137 wc ## 53 newsboys 9.62 918 7.00 14.8 5143 &lt;NA&gt; ## 54 service.station.attendant 9.93 2370 3.69 23.3 5145 bc ## 55 insurance.agents 11.60 8131 13.09 47.3 5171 wc ## 56 real.estate.salesmen 11.09 6992 24.44 47.1 5172 wc ## 57 buyers 11.03 7956 23.88 51.1 5191 wc ## 58 firefighters 9.47 8895 0.00 43.5 6111 bc ## 59 policemen 10.93 8891 1.65 51.6 6112 bc ## 60 cooks 7.74 3116 52.00 29.7 6121 bc ## 61 bartenders 8.50 3930 15.51 20.2 6123 bc ## 62 funeral.directors 10.57 7869 6.01 54.9 6141 bc ## 63 babysitters 9.46 611 96.53 25.9 6147 &lt;NA&gt; ## 64 launderers 7.33 3000 69.31 20.8 6162 bc ## 65 janitors 7.11 3472 33.57 17.3 6191 bc ## 66 elevator.operators 7.58 3582 30.08 20.1 6193 bc ## 67 farmers 6.84 3643 3.60 44.1 7112 &lt;NA&gt; ## 68 farm.workers 8.60 1656 27.75 21.5 7182 bc ## 69 rotary.well.drillers 8.88 6860 0.00 35.3 7711 bc ## 70 bakers 7.54 4199 33.30 38.9 8213 bc ## 71 slaughterers.1 7.64 5134 17.26 25.2 8215 bc ## 72 slaughterers.2 7.64 5134 17.26 34.8 8215 bc ## 73 canners 7.42 1890 72.24 23.2 8221 bc ## 74 textile.weavers 6.69 4443 31.36 33.3 8267 bc ## 75 textile.labourers 6.74 3485 39.48 28.8 8278 bc ## 76 tool.die.makers 10.09 8043 1.50 42.5 8311 bc ## 77 machinists 8.81 6686 4.28 44.2 8313 bc ## 78 sheet.metal.workers 8.40 6565 2.30 35.9 8333 bc ## 79 welders 7.92 6477 5.17 41.8 8335 bc ## 80 auto.workers 8.43 5811 13.62 35.9 8513 bc ## 81 aircraft.workers 8.78 6573 5.78 43.7 8515 bc ## 82 electronic.workers 8.76 3942 74.54 50.8 8534 bc ## 83 radio.tv.repairmen 10.29 5449 2.92 37.2 8537 bc ## 84 sewing.mach.operators 6.38 2847 90.67 28.2 8563 bc ## 85 auto.repairmen 8.10 5795 0.81 38.1 8581 bc ## 86 aircraft.repairmen 10.10 7716 0.78 50.3 8582 bc ## 87 railway.sectionmen 6.67 4696 0.00 27.3 8715 bc ## 88 electrical.linemen 9.05 8316 1.34 40.9 8731 bc ## 89 electricians 9.93 7147 0.99 50.2 8733 bc ## 90 construction.foremen 8.24 8880 0.65 51.1 8780 bc ## 91 carpenters 6.92 5299 0.56 38.9 8781 bc ## 92 masons 6.60 5959 0.52 36.2 8782 bc ## 93 house.painters 7.81 4549 2.46 29.9 8785 bc ## 94 plumbers 8.33 6928 0.61 42.9 8791 bc ## 95 construction.labourers 7.52 3910 1.09 26.5 8798 bc ## 96 pilots 12.27 14032 0.58 66.1 9111 prof ## 97 train.engineers 8.49 8845 0.00 48.9 9131 bc ## 98 bus.drivers 7.58 5562 9.47 35.9 9171 bc ## 99 taxi.drivers 7.93 4224 3.59 25.1 9173 bc ## 100 longshoremen 8.37 4753 0.00 26.1 9313 bc ## 101 typesetters 10.00 6462 13.58 42.2 9511 bc ## 102 bookbinders 8.55 3617 70.87 35.2 9517 bc Notice that the third column assigns a type of professional status to the different occupations. We can have R list all those by printing just the last column. We do that by using the $ followed by the name of that column: Prestige$type ## [1] prof prof prof prof prof prof prof prof prof prof prof prof prof prof ## [15] prof prof prof prof prof prof prof prof prof prof prof prof prof bc ## [29] prof prof wc prof wc &lt;NA&gt; wc wc wc wc wc wc wc wc ## [43] wc wc wc wc wc wc wc wc wc wc &lt;NA&gt; bc wc wc ## [57] wc bc bc bc bc bc &lt;NA&gt; bc bc bc &lt;NA&gt; bc bc bc ## [71] bc bc bc bc bc bc bc bc bc bc bc bc bc bc ## [85] bc bc bc bc bc bc bc bc bc bc bc prof bc bc ## [99] bc bc bc bc ## Levels: bc prof wc Notice that in addition to just listing that column, R also tells you what all the factor values are. We can do this with numerical values too, but be careful because R will interpret the numerical values as characters: IMPORTANT: By default, R will rank factors alphabetically. R will do this also when doing modeling and it is almost never what you want. In this case, you likely want to think of the factors arranged as bc\\(&lt;\\)wc\\(&lt;\\)prof. To do this you: Prestige$type&lt;-factor(Prestige$type,levels=c(&quot;bc&quot;,&quot;wc&quot;,&quot;prof&quot;)) levels(Prestige$type) ## [1] &quot;bc&quot; &quot;wc&quot; &quot;prof&quot; Data frames: R has a special object called a dataframe which is, as the name suggests, designed to hold data. Unlike a matrix, in which all the elements have to be the same type (typically numbers), dataframes are more like spreadsheets - each column can be its own datatype. So you can have a column of numbers associated with a second column of treatment types (character). Let’s make a data frame to play around with, which we will make the ranking of the top three girls and boys names for 2010. our.data.frame&lt;-data.frame(rank=seq(1:3),boys.names=c(&quot;Jacob&quot;,&quot;Ethan&quot;,&quot;Michael&quot;), girls.names=c(&quot;Isabella&quot;,&quot;Sophia&quot;,&quot;Emma&quot;)) our.data.frame ## rank boys.names girls.names ## 1 1 Jacob Isabella ## 2 2 Ethan Sophia ## 3 3 Michael Emma Now while I would encourage everyone to use the command line at all times, its worth pointing out that R does have a very basic data editor. To change a value in our.data.frame using the data editor, use the command ‘fix’ fix(our.data.frame) Note that the command ‘edit’ looks like it should do the same thing but it does not. In fact, ‘edit’ does not change the original data frame but it makes a changed copy which must be assigned another name. In the following example, the changes are stored in new.data.frame. new.data.frame&lt;-edit(our.data.frame) NOTE: R allows you to ‘attach’ a dataframe to a workspace so that you can refer to the individual columns without having to type in the name of the dataframe. I think this is terrible practice and makes your code impossible to read by your future self. Matrices: You make a matrix as follows (here we populate the matrix with a sequence from 1:12): test.matrix&lt;-matrix(1:12,nrow=3,ncol=4) test.matrix ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 Notice that in general, you do not need to include the label names for input parameters to functions. This gives the same answer: test.matrix&lt;-matrix(1:12,3,4) test.matrix ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 but I highly suggest leaving all labels for clarity of coding. R indexes matrices by ROW THEN COLUMN. So, for example, try test.matrix[2,3] ## [1] 8 R has all the functions you could ever want for matrix algebra, such as transposing: trans.test.matrix&lt;-t(test.matrix) See what happens when you try 1-test.matrix Notice that R automatically translates the 1 into a matrix of 1s such that the calculation makes sense. Arrays: Arrays are just higher dimensional matrices and since we will not use them much, I won’t get into details here. Lists: A list is a one-dimensional structure of potentially heterogeneous data types. list.1&lt;-list(data=seq(1,15),mat1=test.matrix,mat2=trans.test.matrix) We can reference elements of the list by name list.1$data ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 or by position list.1[[2]] ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 Notice that list indexing requires double brackets. 2.3 Writing functions in R Using R, you are not limited to functions that have been written for you, you can write your own functions! The basic template is straightforward: square&lt;-function(x) { x*x } We can use our function now as follows: square(5) ## [1] 25 You can also have more than one argument for an R function: product&lt;-function(x,y) { x*y } product(3,5) ## [1] 15 A few notes about using R. What makes R special is not the base package but the “Contributed packages” which make up the bulk of R’s utility. We will be using a variety of these contributed packages along the way, so you need to feel comfortable downloading them from the web. I have posted a handout on Blackboard to cover this. 2.4 Writing loops and if/else The R language is very good at doing operations on vectors or matrices, and when possible, this is always the preferred method of doing an operation mulitple times. However, sometimes this is not possible and you have to write a loop to perform some operation on elements taken one at a time. There are two different kinds of loops in R. A ‘for loop’ executes once for each step through the looping index. The basic syntax for a ‘for loop’ in R is: for (i in 1:6) { print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 The indexing variable does not need to be called “i”, it could be anything. What follows the “in” can be any sequence of numbers; they need not be consecutive. What appears inside the brackets is the chunk of code that will be executed at each iteration. This code can, but need not, actually use the indexing variable. Another example illustrating these points is: for (blah in c(1,3,5,6)) { print(4+blah) } ## [1] 5 ## [1] 7 ## [1] 9 ## [1] 10 A ‘while loop’ is open ended; it will execute the loop indefinitely until the ‘while’ condition is no longer met. The basic syntax for a ‘while loop’ in R is: i=1 while(i &lt;= 8) { y &lt;- i*i i &lt;- i + 1 # What would happen if we left this line out? print(y) } ## [1] 1 ## [1] 4 ## [1] 9 ## [1] 16 ## [1] 25 ## [1] 36 ## [1] 49 ## [1] 64 Sometimes, you want to make R check some condition before executing a command. An ‘if’ statement will check a statement and execute a chunk of code if the statement evaluates to TRUE. If the statement evaluates to FALSE, the code is simply skipped. An if/else statement allows a second chunk of code to be executed as an alternative to the first. The syntax for each is as follows: a&lt;-3 if (a&lt;4) print(&quot;Hello&quot;) ## [1] &quot;Hello&quot; if (a &lt; 4) { print(&quot;Hello&quot;) } else print(&quot;Goodbye&quot;) ## [1] &quot;Hello&quot; if (a &gt; 4) { print(&quot;Hello&quot;) } else print(&quot;Goodbye&quot;) ## [1] &quot;Goodbye&quot; 2.5 (A short diversion) Bias in estimators Now we will stop for a short digression about how to calculate the population variance (i.e. the variamce assuming the data I have is from the entire population) and how to estimate the sample variance (i.e. the variance assuming what I have is a sample from the population, and I want to infer the variance of the underlying but unknown population), since we can now use R to convince ourselves that the naive estimator for variance is biased. The population variance is the variance of a population which, by definition, means that every single individual of interest has been measured. Remember, in this case there is no inference going on. When we have measured every single individual of interest, all we can do (statistically) is describe that population. The population variance describes the variation in the quantity of interest for that population you have completely sampled). \\[ \\sum^{n}_{i=1}{\\frac{(Y_{i}-\\bar{Y})^{2}}{n}} \\] The sample variance answers the question “If this data I have come from a larger population, and I want to use these data to estimate the population variance in that larger population, what is the best unbiased estimator for that (unknown) population variance?” The formula for the sample variance (think of it like “the estimate of the variance from the sample”): \\[ \\sum^{n}_{i=1}{\\frac{(Y_{i}-\\bar{Y})^{2}}{n-1}} \\] We can see this in practice using a little simulation. Type the following into an R script and run it in R: n.iter&lt;-100 data&lt;-rnorm(n.iter,0,2) sum&lt;-0 for (j in 1:length(data)) { sum&lt;-sum+((data[j]-mean(data))*(data[j]-mean(data))) } population.variance&lt;-sum/length(data) sample.variance&lt;-sum/(length(data)-1) What is the ratio of the sample variance to the population variance? Are either close to what we know the true variance to be? What happens if we change n.iter to 1000? Do the values get closer to the correct value? What happens to the ratio of the sample variance to the population variance? What does the R function var() give you? 2.6 Lesson #6: Some practice writing R code We will be using a cloud-seeding dataset from: Simpson, Alsen, and Eden. (1975). A Bayesian analysis of a multiplicative treatment effect in weather modification. Technometrics 17, 161-166. The data consist of data on the amount of rainfall (in acre-feet) from unseeded clouds vs. those seeded with silver nitrate. Here and throughout I have assumed the data resides in a folder called Biometry on a thumb drive called TEACHING drive but you will have to change the code according to your own file structure. (I am using a Mac, but getting the pathname correct I have included a .txt file and a .csv file to show you the differences in inputting your data: Method 1: cloud.data&lt;-read.table(&quot;~/Dropbox/Biometry/Week 1 Preliminary Material/Week 1 Lab/clouds.txt&quot;) Notice that that doesn’t work because the headers have become part of the data. cloud.data&lt;-read.table(&quot;~/Dropbox/Biometry/Week 1 Preliminary Material/Week 1 Lab/clouds.txt&quot;, header=T) Remember that we need to add the “header=T” or it will assume the headers are actually the first line of data. Method 2: cloud.data&lt;-read.table(&quot;~/Dropbox/Biometry/Week 1 Preliminary Material/Week 1 Lab/clouds.csv&quot;, header=T) This doesn’t work because R does know what the delimiter is. You have to specify the delimiter: cloud.data&lt;-read.table(&quot;~/Dropbox/Biometry/Week 1 Preliminary Material/Week 1 Lab/clouds.csv&quot;, header=T,sep=&quot;,&quot;) or use the command ‘read.csv’ which automatically assumes its comma delimited. cloud.data&lt;-read.csv(&quot;~/Dropbox/Biometry/Week 1 Preliminary Material/Week 1 Lab/clouds.csv&quot;, header=T) There are two ways to refer to the first column of data. Because we have column headers, we can refer to them by name using the “$” as follows: cloud.data$Unseeded_Clouds ## [1] 1202.6 830.1 372.4 345.5 321.2 244.3 163.0 147.8 95.0 87.0 ## [11] 81.2 68.5 47.3 41.1 36.6 29.0 28.6 26.3 26.1 24.4 ## [21] 21.7 17.3 11.5 4.9 4.9 1.0 but we can also just ask for a specific column of the data, in this case the first column cloud.data[,1] ## [1] 1202.6 830.1 372.4 345.5 321.2 244.3 163.0 147.8 95.0 87.0 ## [11] 81.2 68.5 47.3 41.1 36.6 29.0 28.6 26.3 26.1 24.4 ## [21] 21.7 17.3 11.5 4.9 4.9 1.0 Note that you can always print the data using just the name cloud.data ## Unseeded_Clouds Seeded_Clouds ## 1 1202.6 2745.6 ## 2 830.1 1697.8 ## 3 372.4 1656.0 ## 4 345.5 978.0 ## 5 321.2 703.4 ## 6 244.3 489.1 ## 7 163.0 430.0 ## 8 147.8 334.1 ## 9 95.0 302.8 ## 10 87.0 274.7 ## 11 81.2 274.7 ## 12 68.5 255.0 ## 13 47.3 242.5 ## 14 41.1 200.7 ## 15 36.6 198.6 ## 16 29.0 129.6 ## 17 28.6 119.0 ## 18 26.3 118.3 ## 19 26.1 115.3 ## 20 24.4 92.4 ## 21 21.7 40.6 ## 22 17.3 32.7 ## 23 11.5 31.4 ## 24 4.9 17.5 ## 25 4.9 7.7 ## 26 1.0 4.1 or, if its easier, can use the data editor as described above. Lets calculate the variance of each treatment. For now, I will do this step-by-step, defining intermediate variables along the way. For simplicity, I redefine the two columns worth of data as “A” and “B”: A&lt;- cloud.data$Unseeded_Clouds mean.A&lt;-mean(A) diff.from.mean.A&lt;- A-mean.A n.A&lt;-length(A) # Here I am just calculating the sample size to use in next line s2.A&lt;-sum(diff.from.mean.A^2)/(n.A-1) s2.A ## [1] 77521.26 Redo the calculation for the Seeded clouds to get “s2.B”. We could have saved ourselves a lot of effort by using the R function “var”: s2.A&lt;-var(A) s2.A ## [1] 77521.26 Is the variance for the Seeded clouds the same as the Unseeded clouds? How close (to equal) is close enough? What is the null hypothesis? 2.7 A few final notes I mentioned at the outset that all of your code should be kept in a script (some kind of text file; it could be a .R file but it could be a simple .txt file) and that your code should be clearly commented. Comments can be added to code using the # sign. For example a&lt;-3+5 #This is a comment everything after the # is not executed by R and is simply for your use in understanding the code. **Short digression on brackets and good coding practices. "],
["week-2-lecture.html", "3 Week 2 Lecture 3.1 Hypothesis testing and p-values 3.2 Permutation tests 3.3 Parameter estimation 3.4 Method #1: Non-parametric bootstrap 3.5 Parametric bootstrap 3.6 Jackknife 3.7 Jackknife-after-bootstrap", " 3 Week 2 Lecture 3.1 Hypothesis testing and p-values There are few topics in statistics more controversial than the various philosophies behind null hypothesis testing. Over the next two weeks we will learn about the two paradigms (Fisher vs. Neyman-Pearson), the hybrid approach mostly commonly used in ecology, and criticisms of the whole enterprise. Bayesian statistics takes an entirely different approach than either Fisher or Neyman-Pearson, and the Bayesian approach resolves many of the inconsistencies involved with frequentist statistics, but at the expense of increased computation (and the use of prior information…). We frame decision-making in terms of a null and an alternative hypothesis. \\(H_{0}\\) vs. \\(H_{A}\\) To take Karl Popper’s famous example: \\(H_{0}\\): There are no vultures in the park. \\(H_{A}\\): There are vultures in the park. Note that the data may reject the null hypothesis (for example, finding vultures in the park), or the data may fail to reject the null hypothesis, but it can never prove the null hypothesis. We cannot prove there are no vultures in the park. We can only say that we were not able to find any vultures in the park, and therefore cannot reject the null hypothesis. Fisher’s original context for developing significance testing was agricultural experiments that could be easily replicated. Fisher’s threshold of 0.05 was an arbitrary threshold for an effect to be considered worthy of continued experimentation. Any experiment that failed to reach this threshold would not be pursued. Experiments that gave “significant” results would be subject to additional experiments. These additional experiments may prove the original effect to be a fluke (and experiments would cease) or the additional experiments may provide confirmatory evidence that the effect was real. Null hypothesis testing (as I will teach it) involves 6 steps. Step #1: Specify a null hypothesis \\(H_{0}\\) (Note that I do not include specification of the alternative hypothesis \\(H_{A}\\) here. While the alternative hypothesis is useful as a mental construct, the basic approach deals only with \\(H_{0}\\) and does not require a \\(H_{A}\\). Step #2: Specific an appropriate test statistic T. A test statistic is some summary of your data that pertains to the null hypothesis. For testing simple hypotheses, there are test statistics known to be ideal in certain situations. However, even in these simple cases, there are other test statistics that could be used. In more complex situations, YOU will have to determine the most appropriate test statistic. generic=T=f(X) specific \\(T^{*}\\)=T(\\(X_{1}\\),\\(X_{2}\\),…,\\(X_{n}\\)) We will introduce many more test statistics in the weeks to come but a simple example of a test statistic would be the use of \\(\\bar{X}\\)̅ (the mean of your sample) as a measure of the mean of the normally distributed population from which the sample was derived. Step #3: Determine the distribution of the test statistic under the null hypothesis \\(H_{0}\\). A test statistic is a statistical quantity that has a statistical distribution. \\(f(T│H_{0})\\) Notice that this is the probability of obtaining the test statistic T GIVEN the null distribution, it is NOT \\(f(H_{0}│T)\\). The test statistic and its distribution under the null hypothesis is the statistical test. Test = Test statistic + Distribution of test statistic under \\(H_{0}\\) Step #4: Collect data and calculate T* Collect data by taking random samples from your population and calculate the test statistic from the sample data. Step #5: Calculate a p-value Calculate the probability that you would get a value for the test statistic as large or larger than that obtained with the data under the null hypothesis \\(P(T^{*}│H_{0})\\)=p-value Step #6: Interpret the p-value Use the p-value to determine whether to reject the null hypothesis (or, alternatively, to decide that the null hypothesis cannot be rejected) These steps apply for both parametric and non-parametric statistics. Here we are introducing hypothesis testing through the lens of randomization procedures, but the same steps will be used again when we get into statistics involving parametric distributions (i.e. statistical distributions of known form and described by a finite number of parameters) and their properties. As you will see in a few weeks, most standard statistical tests involve a test statistic with a known distribution under the null hypothesis; here the distribution under the null hypothesis needs to be generated by randomization (randomization test). (We are starting with the randomization-based procedures because there is no math involved and it is more intuitive.) The basic idea underlying all statistical tests: What is the probability that I would get a test statistic as large or larger (as produced by the data) if the null hypothesis was true (this is the “p-value”). To answer this question we need (1) a test statistic and (2) a distribution under the null hypothesis. p-value = \\(P(data|H_{0})\\) Remember – the p-value is a statement about the probability of getting your data if the null hypothesis were true. It is NOT a statement about the probability that the null hypothesis is true. This logic can go wrong!! Example: If a person is an American, he is probably not a member of Congress. This person is a member of Congress. Therefore he is probably not an American. Let’s draw a null distribution. In order to interpret the statistical test, we need to know whether we want a one-tailed test or a two-tailed test. In a one-tailed test, we would reject the null hypothesis only if the test statistic is larger than expected under in the null in one direction (5\\(%\\) in one tail). In a two-tailed test, we would reject the null if the test statistic is larger in either direction (2.5\\(%\\) in both tails). EXAMPLE: Let’s say I’m looking at the change in auto accident mortalities after a ban is enacted on driving while texting. We would expect that auto accident mortality would decrease after a ban on texting while driving. Let’s say, for arguments sake, that our test statistic T is the change in accident deaths \\(H_{0}\\): T=0 (no change in deaths) \\(H_{A}\\): T&lt;0 (decline in deaths) Another possible formulation of the null and alternative hypotheses is \\(H_{0}\\): T=0 (no change in deaths) \\(H_{A}\\): T \\(\\neq\\) 0 (increase or decline in deaths) Why does it matter? Consider the first case. To reject the null hypothesis, you would have to show that the measured decline T^*was so large as to be very unlikely to have occurred by random chance assuming there was no true change in death rate. Therefore, you would require \\(P(T \\geq T^{*}│H_{0})&lt;0.05\\) to be true for you to decide to reject the null hypothesis. This is a one-tailed test. Consider the second case. To reject the null hypothesis, you would accept values of \\(T^{*}\\) as significant if they were either very large or very small, and would divide the 5% critical region between the two tails \\(P(T \\geq T^{*}│H_{0})&lt;0.025\\) \\(P(T \\leq T^{*}│H_{0})&lt;0.025\\) Notice that it now becomes a more stringent test. If \\(T^{*}\\) is large, it now has to be even larger to qualify as “significant”. This is a two-tailed test. TWO KEY POINTS: If you are using a one-tailed test, you have to be willing to accept a result that is opposite in sign of what was expected as being PURELY BY CHANCE!! In other words, if traffic deaths went UP after the texting ban, you would have to be willing to accept that that was by pure chance and you would then fail to reject the null hypothesis of NO CHANGE. This is in fact what happened, by the way: Texting bans actually increase traffic deaths – WHY? Before using the more “lenient” one-tailed test, make sure you really believe that results opposite to what you expect are only random You cannot do a one-tailed test, find the answer to have the wrong sign and then do a two-tailed test. While probably quite common, this is not statistically valid. You cannot use the data to generate the test! Not all tests are created equal!! Tests differ in their power to detect differences, and their efficiency. The balance between power and efficiency depends on the specific situation; we will discuss this more next week. We are going to introduce the idea of hypothesis testing through the practice of permutation tests, because it allows us to get into the flow of testing hypotheses without the burden of a lot of complicated mathematics. Moreover, in doing so, we introduce the more general concept of “generative models”, which generate outcomes through simulation. for example, we can think of the statement \\(X \\sim Pois(\\lambda)\\) as a generative model because it allows us to generate datasets that follow the distribution \\(Pois(\\lambda)\\). If we had a dataset and we wanted to know whether it came from a Poisson distribution, we could imagine generating lots of datasets using a generative model (i.e. drawn from \\(Pois(\\lambda)\\)) and asking ourselves whether any of the generated datasets look anything like the dataset we have. In the same way, we can think about testing a null hypothesis \\(H_{0}\\) by generating data under that null hypothesis, calculating some test statistics from that generated data, and asking whether our geneterated test statsitics “look like” the test statistic obtained from our real data. If not, then we can reject the null hypothesis. Simulations like this are enormously powerful tools for testing hypotheses and are often far more intuitive than the alternative “parametric” statistical tests we will learn in Weeks 3 and 4. 3.2 Permutation tests Let’s say we have two random samples drawn from possibly different probability distributions F and G, \\(F \\rightarrow z=\\{z_{1},z_{2},...,z_{n}\\}\\) \\(G \\rightarrow y=\\{y_{1},y_{2},...,y_{m}\\}\\) Having observed z and y, we wish to test the null hypothesis \\(H_{0}\\) of no difference between F and G, \\(H_{0}:F=G\\). Note that the equality F=G means that the two distributions are exactly the same across their entire distribution, not just that their means are the same. If \\(H_{0}\\) is true, than there is no probabilistic difference between drawing random values from F and drawing random values from G. What are some possible test statistics that we might use in this case? There are many test statistics that we could use to test this null hypothesis but lets use the difference in means as the test statistic. If there is a large difference in their means, than we can probably reject the null hypothesis that they represent the same underlying distribution. \\(T=E[F]-E[G]=\\bar{x}-\\bar{y}\\) The way to do this is to lump all the data together with their “label” (and here I will use the example from the problem set and test whether males and females have different blood sugar distributions), and to randomly permute the labels so that the gender identity of the data points is randomized. In other words, if there are 10 males and 15 females, then you would randomly select 10 of the blood sugar levels and label them “male” and the remaining 15 would be labeled “female”. NOTE: We are not sampling with replacement here. We are simply permuting the gender labels to “erase” the correlation between gender and blood sugar. We then calculate the mean of the “fake male” group and the mean of the “fake female” group and take the difference. That is the result of ONE permutation. If we do that many many times (say, 10000 times) then the distribution of those differences reflects the distribution under the null hypothesis of no correlation between gender and blood sugar. We will do an example like this in the problem set. 3.3 Parameter estimation Hypothesis testing is a bit like the game 50-questions (Are you red? Are you blue? Each question is a null hypothesis to be rejected…). Parameter estimation appears at first glance more direct, it just asks “What are you?”, and in doing so it estimates the value of a parameter (mean growth rate of a fish population, for example) and provides a measure of uncertainty about that estimate. As a reminder, estimators are tools that produce estimates of population statistics from sample statistics. The basic outline of “statistical inference”: Data = sample \\(\\rightarrow\\) sample statistics \\(\\rightarrow\\) ESTIMATOR \\(\\rightarrow\\) population parameters We generally use the word “statistic” when discussing the data, and “parameter” when discussing the underlying distribution. An “estimator” or “point estimate” is a statistic (that is, a function of the data) that is used to infer the value of an unknown parameter in a statistical model. If the parameter is denoted \\(\\theta\\) then the estimator is typically written by adding a “hat” over the symbol: \\(\\hat{\\theta}\\). Being a function of the data, the estimator is itself a random variable; a particular realization of this random variable is called the “estimate”. Sometimes the words “estimator” and “estimate” are used interchangeably, but I will try and be consistent in using the word “estimator” for the function in the generic, and the word “estimate” for the result of applying that function to the data at hand. Example: \\[ X \\sim N(\\mu,\\sigma^{2}) \\] We define the “estimator” for \\(\\mu\\) as \\[ \\frac{1}{n}\\sum_{i=1}^{n}X_{i} \\] Therefore, the “estimate” \\(\\hat{\\mu}\\) is \\[ \\hat{\\mu}=\\bar{X} =\\frac{1}{n}\\sum_{i=1}^{n}X_{i} \\] Estimators are IMPERFECT tools. Bias: As \\(n \\rightarrow \\infty\\), sample statistic does not converge to the population parameter Standard error: Each individual estimate may be too low or too high from the true value (this can occur even if the long run average value is correct, i.e. unbiased) Why are estimators associated with a standard error? If you were to do your experiment all over again, say 1000 times, the value of your estimate would be different each time. Your 1000 estimates would have a statistical distribution with some spread, and the spread of these 1000 estimates is quantified by the standard error. How do we estimate the bias and variance (related to standard error) of an estimator? While there are other methods that we will discuss in a few weeks, now we are going to introduce the idea through two non-parametric approaches: bootstrap and jackknife. First we need to stop and discuss what it means to sample from an empirical distribution. Let’s say I have a bunch of lotto balls in an urn \\(X=\\{X_{1},X_{2},X_{3},...,X_{n}\\}\\) and I want to draw sets of 5 lotto numbers from that urn. I can sample with replacement or without replacement. If you sample with replacement, we may get some numbers more than once. It also means that if you draw n balls out of an urn with n numbers, there are some numbers you will never draw. STOP: Do you understand sample-with-replacement and sample-without-replacement? 3.4 Method #1: Non-parametric bootstrap The basic idea behind bootstrap sampling is that even if we don’t know what the distribution is that underlies the data, we can “pull ourselves up by our bootstraps” and generate the distribution by resampling WITH REPLACEMENT from the data itself. Say we have original data drawn from an unknown distribution G \\(X=\\{X_{1},X_{2},X_{3},...,X_{n}\\}\\) \\[ X \\sim G() \\] We don’t know the underlying distribution, but we can substitute the empirical distribution \\(\\hat{G}\\) which is defined by \\(\\{X_{1},X_{2},X_{3},...,X_{n}\\}\\). In other words, we model the underlying “true” unknown distribution as a multinomial where every value in X is given a probability \\(\\frac{1}{n}\\) of occurring. Let’s say we want to compute a statistic of the probability distribution \\(\\theta=f(G)\\), which could be the mean or the median or the standard error of the standard deviation (anything at all!!). BTW: \\(\\theta\\) is analogous to the test statistic T used for hypothesis testing, and it will be used in the same way. However, I will use the symbol \\(\\theta\\) to be consistent with the Efron and Tibshirani and other literature on the bootstrap. The “plug-in” principle states that for every parameter of the underlying distribution, we can estimate that function by simply plugging in the empirical distribution \\[ \\hat{\\theta}=f(\\hat{G})=f(X) \\] This is exactly what we would do intuitively. If we have a bunch of numbers and we want to know the mean of the distribution from whence they came, we would use as the best estimate the mean of those numbers. The “plug-in” principle simply formalizes the idea that these summary statistics can be used to make inference about the generating distribution. In the development to follow, we will assume that we have NO other information about a distribution other than a single sample from that distribution. Summary statistics are easy enough to compute, but we don’t have any way of knowing how accurate those summary statistics might be. The bootstrap gives us a way to calculate the accuracy of our summary statistics \\(\\hat{\\theta}\\). The bootstrap works NO MATTER HOW COMPLICATED THE FUNCTION, IT IS COMPLETELY AUTOMATIC, AND REQUIRES NO THEORETICAL CALCULATIONS. First we need the idea of a bootstrap sample. A bootstrap sample is any sample drawn randomly WITH REPLACEMENT from the empirical distribution. BOOTSTRAP = SAMPLE WITH REPLACEMENT \\(X^{*}=\\{\\mbox{n values drawn with replacement from } X\\}\\) n = size of the bootstrap sample = size of the original dataset We draw k such bootstrap samples: \\[ X_{1}^{*}=\\{\\mbox{n values drawn with replacement from } X\\} \\] \\[ X_{2}^{*}=\\{\\mbox{n values drawn with replacement from } X\\} \\] etc. \\[ X_{k}^{*}=\\{\\mbox{n values drawn with replacement from } X\\} \\] Important: Because we are sampling WITH REPLACEMENT, some of the original values will be represented more than once in any given bootstrap sample and others not at all. We calculate our statistic of interest on each bootstrap sample: \\[ \\theta_{1}^{*}=f(X_{1}^{*}) \\] \\[ \\theta_{2}^{*}=f(X_{2}^{*}) \\] etc. \\[ \\theta_{k}^{*}=f(X_{k}^{*}) \\] We will number the different bootstrap sample statistics as \\[ \\theta_{1}^{*},θ_{2}^{*},θ_{3}^{*},...,θ_{k}^{*} \\] k = number of bootstrap samples, you can choose the number of bootstrap samples, more sample = better estimates Now that we have our collection of k bootstrapped estimates of the statistic, what do we do with them? Remember: The goal was to calculate the bias and standard error of our estimator. \\[ \\widehat{Bias_{boot}}=\\left( \\frac{1}{k}\\sum_{i=1}^{k}\\theta_{i}^{*}\\right)-\\hat{\\theta} \\] In other words, the Bias of our estimator is simply the mean of the bootstrapped sample statistics minus the statistic as calculated for the original data. (For unbiased estimators, our estimate of bias goes to zero as the sample size n gets very large.) We can also use these bootstrapped statistics to calculate the standard error of the estimator: \\[ \\widehat{se_{boot}}=\\sqrt{\\frac{1}{k-1}\\sum_{i=1}^{k}(\\theta_{i}^{*}-\\bar{\\theta^{*}})^{2}} \\] This is just the standard deviation of the distribution of \\(\\theta\\). This is a really important point that is worth dwelling on for a bit. Our uncertainty about the value is captured by how much variation there is when I draw a different sub-sample of the data, which mimics re-doing the experiment altogether. In this case, I call the standard deviation of those \\(\\theta^{*}\\) values a standard error, because they represent my uncertainty (my potential error) about \\(\\hat{\\theta}\\). Do not confuse standard deviation and standard error. A standard deviation is a statistic (something calculated from data) about the spread of the data. A standard error is the standard deviation of my estimates, and therefore is a measure of how uncertain I am about my estimate. We will work through 2 examples, one using hand calculations, and one using R pseudocode. Example 1: Data: 10 pennies that the students have Test statistic: Median Lets say we are trying to find the median age of all pennies in circulation. We can’t figure this out exactly, because we can’t collect all the pennies in circulation, but we each have a sample of 10 pennies. The median age of the pennies in our sample is a reasonable estimate for the median age of all pennies in circulation. What is our uncertainty about that number? How far might our estimate of the median age be from the true median age? In this case, we don’t know the underlying distribution of penny ages. (Let’s brainstorm this for a bit. Do we have any guesses what this distribution might look like? What might be a reasonable distribution to describe the shape of penny age?) Let’s use bootstrapped samples to calculate the s.e. associated with that estimate. Procedure: 1. Sample WITH REPLACEMENT a group of 10 pennies. (To sample with replacement you will have to sample one penny, write down the age, and repeat that 10 times.) 2. Calculate the median age from that sample of pennies. 3. Repeat Over time you will gather a collection of median estimates, each one of which was calculated using a different bootstrapped dataset. They can be used to calculate the Bias and the Variance of the estimator. We actually have two primary mechanisms for generating confidence intervals for the statistic. Method #1: We can use the following normal approximation: \\[ \\hat{\\theta^{*}} \\sim N(\\hat{\\theta},\\hat{se}^{2}) \\] STOP: How do we construct a confidence interval from this? \\[ \\hat{\\theta}_{LL}=\\hat{\\theta}-1.96*\\hat{se} \\] \\[ \\hat{\\theta}_{UL}=\\hat{\\theta}+1.96*\\hat{se} \\] Remember that 95\\(\\%\\) of the probability for a Standard Normal distribution lies between (-1.96\\(\\sigma\\),+1.96\\(\\sigma\\))? Here we are using the same principle, capturing 95\\(\\%\\) of the probability of the distribution by assuming the distribution of \\(\\theta^{*}\\) is Normal and pulling out the lower limit and the upper limit lying 1.95 times the standard deviation below and above (respectively) the estimate. OR Method #2: we could do away with normal approximations altogether and simply take the quantiles directly from the distribution of \\(\\hat{\\theta}^{*}\\): \\[ \\theta_{LL} = \\mbox{2.5th percentile of } \\hat{\\theta}^{*} \\] \\[ \\theta_{UL} = \\mbox{97.5th percentile of } \\hat{\\theta}^{*} \\] Notice that (by construction) 95\\(%\\) of the \\(\\hat{\\theta}^{*}\\) values fall in the interval \\((\\theta_{LL},\\theta_{UL})\\). NB: If you are going to go through the trouble of doing the bootstrap sampling, I don’t know why you would make a normal approximation at the very end to construct the CIs. I recommend Method #2. Example 2: Knowing how to draw bootstrap replicates gets more complicated when you have multivariate datasets. For example, lets start with a dataset comparing average LSAT scores and GPA for the incoming classes for 15 law schools Lets say we want to estimate the true correlation coefficient between LSAT scores and GPA. We haven’t covered this yet, but one estimator for the true correlation coefficient is Pearson’s product moment correlation coefficient r \\[ r=\\frac{cov(a,b)}{\\sqrt{var(a)×var(b)}} \\] Therefore, in this case \\[ \\hat{r} = \\frac{cov(LSAT,GPA)}{\\sqrt{var(LSAT)*var(GPA)}} \\] (In R, we would write this as r.est = cor.test(LSAT,GPA)$estimate.) If LSAT and GPA both come from a normal distribution, then we could use the theory of normal distributions to calculate the standard error of \\(\\hat{r}\\). (We will learn this in Week 9.) But, we know LSAT and GPA can’t be from normal distributions. At the very least, GPA is bounded on (0,4), so it cannot be Normally distributed. So, how do we calculate the standard error of \\(\\hat{r}\\)? Here we sample with replacement from the bivariate PAIRS of data. In other words, we sample \\[ X_{1}^{*}=(LSAT_{i},GPA_{i}), \\mbox{where i=sample with replacement 1...n} \\] \\[ X_{2}^{*}=(LSAT_{i},GPA_{i}), \\mbox{where i=sample with replacement 1...n} \\] and so forth, and then calculate the correlation of each simulated dataset. Question: Why not sample with replacement from the two datasets independently? What question would that be answering? If we do this many times, say k=10,000 times, then we can draw a histogram of these bootstrapped correlation coefficients. We can calculate the standard error of our estimate for the correlation coefficient \\[ \\hat{se}_{boot} = \\sqrt{\\left(\\frac{1}{k-1}\\right)\\sum_{i=1}^{k}(r_{i}^{*}-\\bar{r^{*}})^{2}} \\] Therefore, using R, we would calculate the parametric correlation coefficient as: r.est ± 1.96*s.e.boot (VERSION 1) Even better, we can calculate the 95th percentile confidence interval of this distribution: quantile(all.cor,c(0.025,0.975)) (VERSION 2) Note that while VERSION 1 is common, VERSION 2 is preferred because there is no guarantee that the distribution of bootstrap statistics is even vaguely Normal. Bootstrapping can deal with even more complex cases, and is particularly useful when dealing with spatial or temporal autocorrelation. Take for instance a time series of hormone levels: If you wanted to do some time series analysis of this data, say to calculate the correlation between each datapoint and the last datapoint, you would have a difficult time doing so because of the complex temporal autocorrelation. Bootstrap can help in this case, but its not at all obvious how to bootstrap from this time series and preserve the essential temporal autocorrelation structure of the data. One approach would be to do a “moving blocks” bootstrap. This is more advanced, but it makes the point that a) bootstrap can be enormously useful in a variety of complicated analyses and b) you have to think carefully about what to sample in order to preserve the essential element of the data. R has numerous functions for doing bootstrapping, although bootstrapping is so easy its often just as easy (and more transparent) to simply write your own code to do it. We will go over some examples in lab. Note that the procedure we have described is called the non-parametric bootstrap estimate because it is based only on the non-parametric empirical distribution G ̂. If we had assumed some kind of distributional form for G, it would be considered a parametric bootstrap. 3.5 Parametric bootstrap The parametric bootstrap is similar to the non-parametric bootstrap except that instead of drawing our bootstrap samples from the original data, we fit a distribution to the data first, and then draw our samples from that. We haven’t covered how to fit a distribution to data yet, nor have we introduced any of the univariate distributions, so I won’t show you how to do a parametric bootstrap now but we’ll get some practice in the Week 3 problem set. Why would we ever do a parametric bootstrap? We might use a parametric distribution if our original sample size was so small that we did not think it could “stand in” for the underlying parametric distribution. For example, if your dataset for coin age just so happens not to have any coins made in 1990, you may be uncomfortable having all your bootstrapped datasets also be missing coins made in 1990. (Remember: Bootstrapping is, in some way, supposed to mimic redoing your experiment. Do you really think that you’d never get a coin made in 1990?) To get around this problem, you might do a parametric bootstrap. Note that, if you use MLEs to get the parameters for the parametric bootstrap, those parameter estimates assume large sample sizes (the formula are asymptotically correct for large sample sizes) and so you have to be a little caution that your parametric bootstrap might not be capturing the true underlying distribution. While parametric bootstrap is often done when sample sizes are too small, occasionally it may also be used when you have some strong theoretical justification for a particular distribution but the statistics you are interested in have no simple formula. (In other words, maybe the distribution is known, but the statistical properties of the specific parameter you are interested in is not known but could be derived through parametric bootstrapping.) 3.6 Jackknife Jackknifing is another method of assessing bias and standard error of sample statistics. Jackknife can also be used to establish the influence of each datapoint in your dataset. The procedure simply involves leaving out each datapoint and recalculating the statistic of interest. If your dataset involves the set \\[ \\{x_{1},x_{2},x_{3}\\} \\] then the jackknife samples are \\[ \\{x_{1},x_{2}\\},\\{x_{1},x_{3}\\},\\{x_{2},x_{3}\\} \\] The traditional notation is that the estimate based on the dataset when the ith element is removed is (\\(\\widehat{\\theta_{(i)}}\\)). The jackknife estimate of bias is given by \\[ \\widehat{Bias_{jack}}=(n-1)(\\hat{\\theta_{(.)}}-\\hat{\\theta}) \\] where \\[ \\hat{\\theta}_{(.)}=\\frac{1}{n}\\sum_{i=1}^{n}\\hat{\\theta}_{(i)} \\] You can convince yourself of this formula by working out the case where \\(\\hat{\\theta}\\) is the mean. You can also see intuitively why you would have to multiply the jackknife estimate of bias by (n-1) since the deviation of the jackknifed samples from the full sample is much smaller than the standard deviation of the bootstrapped samples. (DOES EVERYONE SEE WHY?) The jackknife estimate of standard error is given by \\[ \\hat{se}_{jack}=\\sqrt{\\frac{n-1}{n}\\sum_{i=1}^{n}(\\hat{\\theta}_{(i)}-\\hat{\\theta}_{(.)})^{2}} \\] With the pennies example, we had 10 pennies and we have only 10 possible jackknifed samples. Do you see why? Note that while bootstrapping can involve simulating an arbitrarily large number of pseudosamples (k), there are only n possible jackknife replicates for a dataset of size n. Exercise: Use your pennies to calculate \\(\\widehat{Bias}_{jack}\\) and \\(\\widehat{se}_{jack}\\). Both bootstrap and jackknife can estimate the standard error of a statistic, and in this way, their use can often be interchangeable. However, the jackknife can ONLY compute the bias and standard error whereas the bootstrap calculates the entire distribution of the statistic from which the standard error can be inferred. Bootstrapping is often more computer intensive, but with modern computers this is hardly a drawback. 3.7 Jackknife-after-bootstrap Jackknife-after-bootstrap is one method of assessing the standard error of bootstrap statistics. For example, jackknife-after-bootstrap can give us the standard error of the bootstrap standard error: \\[ \\widehat{se}_{jack}(\\widehat{se}_{boot}) \\] To do this there are two steps: Leave out data point i and use the remaining data in a bootstrap analysis to calculate (s.e.) ̂_(boot(i)) Define \\[ \\widehat{se}_{jack}(\\widehat{se}_{boot})=\\sqrt{\\left(\\frac{n-1}{n}\\right)\\sum_{i=1}^{n}(\\hat{se}_{boot(i)}-\\hat{se}_{boot(.)})^{2}} \\] Notice that because there are always some bootstrap samples that do not include i, you do not actually have to do any extra computation to do jackknife-after-bootstrap, but the precise details of using the bootstrap samples you already have are a bit complicated. In R, this can be done using the ‘jack.after.boot’ function in the package “boot”. Discuss: Why are hypothesis testing and parameter estimation two sides of the same coin? "],
["week-2-lab.html", "4 Week 2 Lab 4.1 Confidence intervals 4.2 Testing hypotheses through permutation 4.3 Basics of bootstrap and jackknife 4.4 Calculating bias and standard error 4.5 Parametric bootstrap", " 4 Week 2 Lab 4.1 Confidence intervals Before getting too far, we need to circle back and make sure we understand what is meant by a confidence interval. A 95th percentile confidence interval say “If I repeat this procedure 100 times using 100 different datasets, 95% of the time my confidence intervals will capture the true parameter”. It does NOT say that there is a 95% chance that the parameter is in the interval. Quiz time! (Don’t worry, not a real quiz) Important note: This is an area where Aho is WRONG. I will not repeat Aho’s interpretation here because I think he’s just wrong. Aho is correct on only one point. It is true that ONCE THE 95th CI HAS BEEN CONSTRUCTED, it is no longer possible to assign a \\(%\\) to the probability that that CI contains the true value or not. Because that CI, once created, either DOES or DOES NOT contain the true value. However, we often talk about the interval in the abstract. When we say “There is a 95\\(%\\) chance that the interval contains the true value” what we mean is that there is a 95\\(%\\) probability that a CI created using that methodology would contain the true value. Do not let Week 2 pass by without fundamentally understanding the interpretation of a confidence interval. 4.2 Testing hypotheses through permutation We’ll start off by working through two examples from Phillip Good’s book “Introduction to Statistics Through Resampling Methods and R/S-PLUS”: Example #1: Use permutation methods to test the null hypothesis that the treatment does not increase survival time (in other word: \\(H_{0}\\): No difference in survival between the treated and control groups): Survival.treated=\\(\\{94,197,16,38,99,141,23 \\}\\) Survival.control=\\(\\{52,104,146,10,51,30,40,27,46 \\}\\) (Is this a one-tailed or a two-tailed test?) Make sure that you understand what is being done here, as this example is very closely related to the problem set. Example #2: Using the same data, provide a 95% confidence interval for the difference in mean survival days based on 1000 bootstrap samples Note that these two approaches are very closely related. Do you see why either approach can be used to test the null hypothesis? (What is the null hypothesis here?) Now we will do one slightly more complicated example from Phillip Good’s book “Permutation tests: A practical guide to resampling methods and testing hypotheses”: Holmes and Williams (1954) studied tonsil size in children to verify a possible association with the virus . Test for an association between status and tonsil size. (Note that you will need to come up with a reasonable test statistic.) Now lets consider the full dataset, where tonsil size is divided into three categories. How would we do the test now? What is the new test statistic? (There are many options.) What ‘labels’ do you permute? 4.3 Basics of bootstrap and jackknife To get started with bootstrap and jackknife techniques, we start by working through a very simple example. First we simulate some data x&lt;-seq(0,9,by=1) This will constutute our “data”. Let’s print the result of sampling with replacement to get a sense for it… table(sample(x,size=length(x),replace=T)) ## ## 0 1 2 3 4 8 ## 1 2 1 1 2 3 Now we will write a little script to take bootstrap samples and calculate the means of each of these bootstrap samples xmeans&lt;-vector(length=1000) for (i in 1:1000) { xmeans[i]&lt;-mean(sample(x,replace=T)) } The actual number of bootstrapped samples is arbitrary at this point but there are ways of characterizing the precision of the bootstrap (jackknife-after-bootstrap) which might inform the number of bootstrap samples needed. In practice, people tend to pick some arbitrary but large number of bootstrap samples because computers are so fast that it is often easy to draw far more samples than are actually needed. When calculation of the statistic is slow (as might be the case if you are using the samples to construct a phylogeny, for example), then you would need to be more concerned with the number of bootstrap samples. First, lets just look at a histogram of the bootstrapped means and plot the actual sample mean on the histogram for comparison hist(xmeans,breaks=30,col=&quot;pink&quot;) abline(v=mean(x),lwd=2) 4.4 Calculating bias and standard error From these we can calculate the bias and standard deviation for the mean (which is the “statistic”): \\[ \\widehat{Bias_{boot}} = \\left(\\frac{1}{k}\\sum^{k}_{i=1}\\theta^{*}_{i}\\right)-\\hat{\\theta} \\] bias.boot&lt;-mean(xmeans)-mean(x) bias.boot ## [1] -0.0436 hist(xmeans,breaks=30,col=&quot;pink&quot;) abline(v=mean(x),lwd=5,col=&quot;black&quot;) abline(v=mean(xmeans),lwd=2,col=&quot;yellow&quot;) \\[ \\widehat{s.e._{boot}} = \\sqrt{\\frac{1}{k-1}\\sum^{k}_{i=1}(\\theta^{*}_{i}-\\bar{\\theta^{*}})^{2}} \\] se.boot&lt;-sd(xmeans) We can find the confidence intervals in two ways: Method #1: Assume the bootstrap statistics are normally distributed LL.boot&lt;-mean(xmeans)-1.96*se.boot #where did 1.96 come from? UL.boot&lt;-mean(xmeans)+1.96*se.boot LL.boot ## [1] 2.667862 UL.boot ## [1] 6.244938 Method #2: Simply take the quantiles of the bootstrap statistics quantile(xmeans,c(0.025,0.975)) ## 2.5% 97.5% ## 2.6 6.3 Let’s compare this to what we would have gotten if we had used normal distribution theory. First we have to calculate the standard error: se.normal&lt;-sqrt(var(x)/length(x)) LL.normal&lt;-mean(x)-qt(0.975,length(x)-1)*se.normal UL.normal&lt;-mean(x)+qt(0.975,length(x)-1)*se.normal LL.normal ## [1] 2.334149 UL.normal ## [1] 6.665851 In this case, the confidence intervals we got from the normal distribution theory are too wide. Does it make sense why the normal distribution theory intervals are too wide? Because the original were were uniformly distributed, the data has higher variance than would be expected and therefore the standard error is higher than would be expected. There are two packages that provide functions for bootstrapping, ‘boot’ and ‘boostrap’. We will start by using the ‘bootstrap’ package, which was originally designed for Efron and Tibshirani’s monograph on the bootstrap. To test the main functionality of the ‘bootstrap’ package, we will use the data we already have. The ‘bootstrap’ function requires the input of a user-defined function to calculate the statistic of interest. Here I will write a function that calculates the mean of the input values. library(bootstrap) ## Warning: package &#39;bootstrap&#39; was built under R version 3.5.2 theta&lt;-function(x) { mean(x) } results&lt;-bootstrap(x=x,nboot=1000,theta=theta) results ## $thetastar ## [1] 5.2 5.7 5.0 3.6 4.6 3.9 5.8 6.4 3.7 4.8 4.8 4.9 4.4 3.4 2.9 5.0 6.2 ## [18] 3.3 3.9 4.3 5.1 5.9 4.3 4.1 3.7 5.3 4.6 3.4 4.1 4.0 5.9 4.9 4.7 4.6 ## [35] 4.2 4.0 3.4 3.3 4.0 3.4 5.2 4.7 4.6 6.2 5.1 5.1 4.8 4.2 4.1 5.0 6.4 ## [52] 4.6 5.0 5.9 4.4 2.8 4.3 6.2 4.1 5.8 5.2 4.8 4.1 3.8 5.2 5.6 4.8 4.0 ## [69] 5.2 3.9 5.3 3.9 4.1 3.1 5.7 3.7 4.6 4.2 5.1 4.1 5.2 3.8 5.3 4.6 5.9 ## [86] 5.9 2.1 3.5 5.2 5.3 4.5 5.0 4.2 6.2 4.0 4.7 4.7 2.8 4.9 5.0 3.6 5.2 ## [103] 4.1 6.0 6.3 4.6 4.7 4.8 2.4 5.1 5.8 4.0 4.1 4.7 4.6 5.5 5.0 4.4 5.0 ## [120] 3.5 4.5 4.9 5.4 2.6 3.6 5.3 3.9 4.7 4.8 4.6 3.4 4.2 5.1 3.0 4.1 4.4 ## [137] 3.3 5.0 3.7 5.3 5.6 4.6 4.4 3.8 4.8 4.9 4.3 4.7 3.8 3.8 3.9 4.8 3.1 ## [154] 4.3 5.2 4.3 3.7 3.2 4.2 4.7 4.1 3.3 2.9 6.1 4.4 4.3 6.1 5.1 4.0 4.2 ## [171] 3.1 4.9 5.5 4.4 5.5 4.4 4.6 5.1 2.9 3.4 3.0 4.2 4.3 4.6 3.6 4.1 5.6 ## [188] 4.5 4.2 3.8 3.2 4.1 3.6 5.1 5.6 5.7 5.4 4.7 4.3 4.6 4.8 4.6 4.7 3.9 ## [205] 4.7 6.4 5.6 3.8 2.7 4.8 4.5 5.2 4.4 5.1 5.5 4.3 5.2 5.6 5.5 6.2 2.2 ## [222] 4.5 4.3 4.9 5.1 4.9 3.0 4.0 5.4 5.8 4.4 3.8 4.7 5.4 4.4 4.8 5.3 2.5 ## [239] 4.4 5.1 5.1 3.5 3.2 3.2 5.2 5.3 4.2 5.0 5.2 4.7 4.9 4.1 4.0 4.3 5.7 ## [256] 3.4 4.6 4.0 2.9 4.0 2.5 2.6 4.2 3.2 4.1 6.0 4.4 4.9 4.8 3.8 3.1 4.5 ## [273] 3.3 4.7 3.8 6.6 5.5 6.1 4.1 4.7 4.0 4.3 5.1 3.1 5.9 5.2 5.4 3.2 4.2 ## [290] 3.9 6.6 5.0 4.2 3.9 7.3 3.2 4.8 5.2 4.0 4.6 5.0 3.2 5.9 5.8 4.3 5.3 ## [307] 5.4 3.9 4.4 4.2 4.0 3.1 5.7 4.0 2.5 3.8 5.0 5.4 3.0 5.1 4.2 5.8 3.9 ## [324] 4.9 4.2 4.2 4.7 5.0 4.4 5.0 3.8 3.5 4.8 3.0 3.7 5.5 4.4 3.5 4.3 4.3 ## [341] 4.6 5.2 5.0 4.3 4.0 5.3 3.8 5.1 3.4 3.6 3.6 4.7 3.3 4.3 5.4 4.0 4.2 ## [358] 4.9 5.9 5.3 5.1 4.6 4.1 4.0 4.6 4.0 5.0 4.6 5.9 5.2 4.6 4.8 4.1 3.4 ## [375] 4.3 4.6 6.3 4.6 4.0 3.7 4.1 5.1 3.3 4.6 4.4 3.5 4.8 4.6 3.6 4.7 5.5 ## [392] 4.6 5.4 4.7 5.5 4.7 5.2 3.4 5.7 3.7 4.8 5.6 3.9 3.9 4.2 4.0 4.2 5.0 ## [409] 4.3 5.5 4.6 4.2 4.0 3.9 5.0 5.9 3.0 3.3 3.9 4.0 4.9 3.6 3.4 6.3 3.8 ## [426] 3.7 3.6 3.4 4.5 6.4 3.0 4.6 4.0 4.8 3.5 5.4 4.5 4.6 4.8 4.8 3.7 5.1 ## [443] 4.9 3.7 3.5 5.4 4.4 4.3 5.1 3.4 2.8 3.6 5.0 4.6 5.0 5.5 5.1 3.8 4.5 ## [460] 4.4 3.8 4.1 5.2 4.2 4.7 3.7 4.4 3.8 4.6 6.1 5.5 4.6 2.7 5.0 3.0 4.4 ## [477] 4.4 3.9 1.9 5.2 5.8 3.5 5.5 4.6 6.0 5.3 4.7 5.0 4.8 5.9 5.6 4.5 6.4 ## [494] 4.9 5.2 4.8 4.7 3.6 4.1 3.4 5.0 5.8 6.0 5.4 4.2 4.4 5.2 3.8 4.5 5.3 ## [511] 4.5 4.8 4.6 3.1 3.6 3.7 3.2 6.1 4.8 6.2 4.7 5.0 4.3 4.2 6.7 3.7 3.7 ## [528] 3.8 4.7 4.2 4.2 5.0 4.1 4.9 4.3 4.0 5.4 4.4 5.0 5.0 4.1 4.4 5.5 5.2 ## [545] 5.0 3.2 3.9 6.1 3.0 3.0 4.9 3.9 2.6 5.7 4.0 4.0 4.0 4.2 4.8 4.1 4.9 ## [562] 4.2 4.4 3.4 3.2 4.7 4.8 4.1 6.2 5.1 6.0 4.6 5.6 3.2 4.8 4.0 2.0 3.5 ## [579] 5.6 5.3 3.8 6.7 6.0 4.9 2.8 4.3 3.8 3.2 5.5 4.7 5.1 3.2 4.5 3.7 4.8 ## [596] 5.0 3.4 2.9 3.7 3.4 4.4 5.4 4.0 3.4 4.6 4.8 3.9 2.9 3.7 4.5 3.6 4.0 ## [613] 4.1 2.8 5.2 5.2 4.3 4.2 3.8 5.0 3.7 3.8 5.7 6.3 4.2 6.5 5.8 4.9 4.3 ## [630] 4.2 4.0 4.8 5.2 4.9 6.1 4.8 5.4 4.9 4.7 3.0 4.3 4.8 5.1 5.2 4.6 3.3 ## [647] 4.8 4.3 4.6 4.8 4.4 6.1 5.3 4.0 4.5 4.6 4.4 3.8 4.0 4.1 5.6 3.9 3.6 ## [664] 5.7 4.7 4.3 4.2 5.2 4.1 5.1 3.6 4.1 5.3 4.4 6.0 4.5 4.4 5.5 4.6 4.3 ## [681] 5.5 3.9 4.8 5.1 4.5 5.6 4.2 4.7 4.7 3.7 6.9 4.5 3.9 4.3 4.4 4.3 5.5 ## [698] 2.4 5.0 3.8 4.8 3.5 4.5 2.6 4.9 5.0 5.5 3.9 4.0 3.7 5.1 4.5 4.3 4.4 ## [715] 3.3 6.3 4.9 2.8 5.1 6.1 5.1 2.8 4.9 6.1 5.8 4.2 4.8 6.3 6.0 4.8 3.2 ## [732] 4.0 4.9 5.2 5.6 4.6 5.5 6.5 5.9 4.8 6.0 5.2 5.2 4.6 5.0 3.6 3.9 4.5 ## [749] 4.0 4.6 5.5 4.7 3.7 3.2 5.5 5.1 4.0 5.0 6.5 3.4 5.7 4.9 4.8 4.7 5.3 ## [766] 4.4 2.9 3.8 4.2 4.7 5.0 5.0 4.9 3.7 3.5 5.6 3.5 2.8 5.0 5.8 5.5 4.8 ## [783] 4.7 5.3 4.4 5.1 3.2 5.5 3.9 5.0 4.8 4.2 5.1 6.3 5.2 4.2 5.1 3.8 5.3 ## [800] 3.6 3.8 4.1 5.9 4.7 4.1 3.6 2.9 3.2 5.1 5.9 4.1 4.8 5.0 6.6 4.0 5.4 ## [817] 3.8 3.6 4.1 4.4 3.7 3.9 4.5 4.4 3.6 6.3 4.4 4.6 4.9 4.2 2.5 5.0 5.2 ## [834] 5.4 4.6 5.1 4.1 4.2 4.1 3.0 4.9 2.8 4.6 4.5 5.2 4.5 4.9 4.7 4.3 4.6 ## [851] 4.3 4.3 3.7 4.3 4.0 3.1 2.8 5.7 5.1 3.9 3.4 3.2 5.6 5.2 3.4 3.7 4.8 ## [868] 5.4 4.3 5.5 3.2 3.9 4.9 4.9 5.1 3.2 3.7 4.8 5.3 4.9 4.5 3.6 4.7 5.9 ## [885] 7.0 4.6 3.3 3.8 5.1 5.0 3.9 5.5 4.4 3.6 4.2 4.5 4.5 4.2 5.1 4.7 4.8 ## [902] 5.1 4.8 3.8 4.8 3.3 3.0 5.0 5.7 4.4 5.2 5.9 4.4 5.0 3.7 3.8 4.3 4.2 ## [919] 4.9 4.1 6.1 4.6 4.6 4.4 4.1 5.1 6.6 4.6 6.3 5.1 5.8 5.8 5.6 4.7 4.3 ## [936] 4.7 5.8 3.2 4.8 4.3 5.8 4.4 6.4 3.5 5.4 5.4 3.1 5.4 5.0 3.7 3.8 4.1 ## [953] 4.1 5.8 4.1 4.9 4.8 5.6 5.0 4.1 3.5 2.9 4.2 6.5 4.6 3.5 3.3 3.5 4.1 ## [970] 5.4 3.3 5.6 5.7 3.5 5.3 4.1 5.0 4.1 5.1 3.8 3.4 4.2 4.5 4.4 5.0 4.5 ## [987] 4.3 5.8 3.9 5.7 5.4 7.0 5.8 3.7 4.9 4.7 4.6 4.7 3.7 3.3 ## ## $func.thetastar ## NULL ## ## $jack.boot.val ## NULL ## ## $jack.boot.se ## NULL ## ## $call ## bootstrap(x = x, nboot = 1000, theta = theta) quantile(results$thetastar,c(0.025,0.975)) ## 2.5% 97.5% ## 2.8 6.3 Notice that we get exactly what we got last time. This illustrates an important point, which is that the bootstrap functions are often no easier to use than something you could write yourself. You can also define a function of the bootstrapped statistics (we have been calling this theta) to pull out immediately any summary statistics you are interested in from the bootstrapped thetas. Here I will write a function that calculates the bias of my estimate of the mean (which is 4.5 [i.e. the mean of the number 0,1,2,3,4,5,6,7,8,9]) bias&lt;-function(x) { mean(x)-4.5 } results&lt;-bootstrap(x=x,nboot=1000,theta=theta,func=bias) results ## $thetastar ## [1] 4.9 3.1 3.8 4.4 6.4 3.8 4.2 4.3 5.0 4.5 5.4 3.5 4.8 4.5 3.8 3.5 4.9 ## [18] 5.7 3.9 4.1 4.4 3.4 4.4 4.9 4.4 3.7 3.0 4.2 3.3 5.0 5.8 4.3 3.4 5.6 ## [35] 4.1 3.9 3.5 6.4 2.3 4.3 1.9 3.7 4.4 2.8 5.0 4.4 4.5 4.0 3.5 5.2 4.7 ## [52] 3.2 3.4 3.2 4.5 5.3 4.5 4.9 4.3 6.2 4.8 5.1 5.4 3.8 5.0 3.3 5.5 6.3 ## [69] 4.0 3.3 3.7 7.2 4.3 2.9 5.1 4.5 5.6 3.9 5.1 3.8 4.2 5.3 4.2 5.3 5.5 ## [86] 4.8 3.4 3.4 3.6 3.4 4.9 3.2 3.5 4.0 4.8 4.4 3.9 4.7 4.6 4.2 4.4 3.3 ## [103] 3.9 4.6 3.7 4.5 3.0 3.5 5.7 3.0 2.7 3.3 6.1 5.5 5.3 4.2 3.8 3.9 4.4 ## [120] 4.9 3.7 5.0 4.9 4.5 4.9 5.0 4.8 2.6 4.1 5.5 4.1 3.8 4.3 6.2 4.2 5.6 ## [137] 4.7 3.8 3.9 5.3 4.4 4.5 6.8 5.8 3.9 4.4 4.1 5.4 4.1 4.9 4.8 4.2 3.8 ## [154] 3.1 5.3 5.2 3.1 5.1 4.0 3.8 3.4 5.2 5.3 4.0 4.2 4.6 3.9 4.5 3.7 4.8 ## [171] 4.5 4.7 5.0 3.0 5.2 4.0 5.9 4.2 5.2 3.4 3.3 4.1 6.1 3.9 4.6 4.4 3.8 ## [188] 5.0 3.9 5.2 4.9 3.6 5.2 5.3 5.8 5.7 4.5 5.1 5.0 4.6 3.4 3.4 3.5 4.5 ## [205] 4.2 4.0 4.7 4.4 5.8 4.1 4.0 5.0 3.6 3.9 5.8 2.1 3.8 4.7 3.6 4.0 4.9 ## [222] 3.8 5.3 4.5 5.4 4.2 3.8 4.5 3.6 2.5 3.6 5.7 4.2 5.1 5.4 4.7 1.8 5.1 ## [239] 3.6 4.9 3.0 5.1 4.2 4.7 5.1 2.5 5.0 5.2 4.0 4.1 3.9 4.9 3.0 5.4 4.7 ## [256] 5.3 2.8 3.9 4.0 6.6 5.1 4.2 5.3 5.8 5.5 3.3 5.2 4.6 4.3 4.9 4.4 4.5 ## [273] 4.8 3.6 5.4 4.3 5.3 4.4 5.1 2.0 5.2 4.0 5.2 4.7 4.5 6.1 3.9 4.1 3.9 ## [290] 4.6 4.5 5.1 5.4 3.8 4.4 4.7 4.4 4.0 3.8 3.6 3.8 3.9 3.4 5.4 3.5 3.7 ## [307] 5.7 3.9 4.5 5.2 4.4 4.6 4.0 4.4 5.9 4.1 3.9 4.6 3.6 3.0 4.5 5.3 4.6 ## [324] 4.4 5.3 5.1 4.2 5.8 2.9 5.1 6.9 5.2 4.7 4.1 3.1 3.5 4.3 4.9 4.5 4.1 ## [341] 4.2 5.6 4.9 6.1 5.0 4.9 6.2 4.5 3.9 4.7 4.1 4.2 6.0 3.4 5.5 4.9 5.4 ## [358] 4.8 4.3 3.8 4.4 3.2 3.6 4.4 4.5 5.0 3.9 5.5 4.2 5.0 2.9 4.3 5.1 3.5 ## [375] 5.3 5.3 4.2 3.1 3.5 4.1 5.3 4.5 4.8 3.1 4.9 3.9 4.0 4.4 4.6 4.6 4.6 ## [392] 5.0 4.2 3.9 5.4 4.6 4.9 4.6 4.9 6.1 4.3 5.2 4.5 3.9 4.5 3.0 3.9 5.2 ## [409] 4.9 4.6 4.7 4.6 4.4 4.9 3.2 4.6 5.1 4.4 3.1 4.2 2.9 3.8 4.5 3.2 4.2 ## [426] 3.3 4.3 5.0 3.9 3.9 5.6 4.2 2.7 3.2 5.7 6.5 5.0 4.4 5.1 4.0 4.4 4.6 ## [443] 5.0 4.7 4.2 4.7 3.9 2.8 4.4 4.3 4.9 5.1 4.5 3.3 5.0 3.9 5.4 4.6 5.5 ## [460] 5.2 3.1 4.6 2.8 4.4 4.6 5.0 4.8 4.2 3.8 2.7 4.8 4.4 4.9 4.7 5.7 5.1 ## [477] 4.4 4.6 3.6 3.2 3.9 3.7 3.5 4.4 4.9 4.2 4.1 5.6 5.1 3.7 4.0 4.2 5.1 ## [494] 3.5 4.0 4.5 4.2 3.5 5.8 5.4 6.1 4.7 5.0 4.8 4.3 4.4 5.5 5.2 4.8 4.7 ## [511] 5.9 3.3 5.8 4.5 3.4 5.4 4.4 5.5 3.3 3.8 5.7 5.1 3.0 4.0 4.4 2.6 4.8 ## [528] 3.6 4.3 3.6 2.7 5.2 3.1 3.8 3.7 3.8 3.4 4.8 4.5 4.8 5.7 3.6 4.7 2.8 ## [545] 4.5 5.5 4.3 4.9 2.4 6.0 5.0 4.9 4.5 3.8 5.1 5.5 3.3 5.8 5.3 3.7 5.1 ## [562] 4.5 3.0 4.4 5.0 4.3 3.8 5.3 6.6 5.7 4.0 4.3 4.5 3.6 4.6 5.4 6.5 4.6 ## [579] 5.9 4.5 5.5 3.9 5.5 5.5 3.5 5.7 4.6 3.8 5.6 5.5 4.8 4.8 4.3 3.4 4.4 ## [596] 3.5 5.9 4.6 6.3 3.5 3.3 3.5 5.3 4.5 5.6 4.6 6.9 4.0 3.3 2.6 5.1 3.1 ## [613] 4.6 4.1 2.9 5.3 2.9 4.1 4.1 4.3 5.2 3.5 3.9 4.3 5.2 5.5 3.2 5.7 4.3 ## [630] 4.5 3.5 4.7 4.8 3.5 5.5 4.7 5.0 3.1 4.6 4.5 4.0 5.7 4.9 5.1 5.0 3.8 ## [647] 5.9 4.0 3.6 3.3 5.2 5.3 3.1 4.5 4.2 4.5 5.5 3.2 4.8 4.3 4.7 3.9 4.2 ## [664] 3.1 4.0 4.9 5.2 5.7 3.4 4.2 5.1 4.6 4.5 4.6 3.3 4.8 5.2 4.5 4.6 4.2 ## [681] 5.2 4.7 5.0 6.1 4.1 5.3 4.2 4.7 4.2 4.3 4.7 4.3 2.9 4.8 4.3 4.3 2.8 ## [698] 3.9 4.2 5.3 4.9 4.1 5.8 4.1 5.0 3.8 4.0 3.7 2.6 3.7 3.4 4.0 5.1 4.7 ## [715] 3.3 4.0 4.4 2.8 3.1 5.5 4.7 5.7 3.0 4.8 3.0 4.1 3.1 5.2 4.6 5.7 5.0 ## [732] 5.4 2.6 4.2 6.1 6.4 4.0 4.9 4.3 5.0 3.9 4.7 5.2 3.4 4.3 4.5 5.3 4.0 ## [749] 2.5 4.4 4.0 4.0 3.8 5.4 3.3 6.1 5.6 3.8 3.7 3.4 4.4 4.9 5.6 4.0 6.0 ## [766] 3.9 4.3 4.7 4.6 3.9 5.8 5.3 4.8 5.2 4.9 3.7 4.3 5.8 3.9 3.8 5.5 4.0 ## [783] 2.7 4.8 4.1 5.5 4.4 6.0 5.2 4.0 5.3 5.1 4.1 3.8 4.8 3.3 3.4 3.9 3.7 ## [800] 1.8 4.8 5.3 5.1 5.9 1.3 4.7 4.7 5.1 6.1 4.3 3.9 5.9 5.7 4.7 3.9 4.5 ## [817] 5.5 5.2 3.6 3.3 7.7 5.8 3.0 5.1 3.0 4.1 3.3 4.0 3.9 4.4 4.1 4.2 3.8 ## [834] 3.3 2.7 5.2 3.8 4.5 3.7 5.1 4.3 4.3 4.2 3.4 4.9 4.5 4.0 6.0 6.0 3.3 ## [851] 4.0 6.4 4.8 5.5 3.1 4.7 4.1 3.6 4.5 4.5 3.1 3.5 3.0 3.9 3.1 5.4 2.6 ## [868] 4.9 5.1 4.1 5.5 4.2 3.3 2.7 3.7 4.1 4.1 4.2 4.7 4.1 5.4 3.5 4.8 5.5 ## [885] 3.3 6.1 5.2 4.4 2.2 4.2 6.3 5.8 5.7 4.7 5.4 5.0 3.4 3.4 3.3 3.3 4.7 ## [902] 4.1 3.3 3.2 4.2 4.7 3.8 5.2 5.6 4.1 4.0 4.3 4.4 4.2 4.2 4.5 4.4 2.2 ## [919] 6.2 4.0 3.9 5.8 4.9 5.0 6.1 3.6 5.3 4.7 4.9 4.4 5.3 4.9 3.1 3.3 4.6 ## [936] 5.4 5.1 4.2 3.4 6.7 3.0 3.9 6.4 4.5 4.8 5.5 3.5 3.8 4.4 3.5 5.6 3.6 ## [953] 5.9 3.9 4.6 4.1 3.6 3.5 4.3 4.3 3.3 5.0 4.7 4.8 4.2 6.4 4.3 4.7 5.6 ## [970] 5.2 5.5 4.2 5.6 5.5 4.3 5.2 5.6 4.1 4.3 5.2 3.7 3.4 4.2 4.2 4.6 3.0 ## [987] 4.0 3.3 3.6 3.8 3.6 4.6 4.4 4.4 4.8 3.8 5.2 4.0 5.1 4.1 ## ## $func.thetastar ## [1] -0.0706 ## ## $jack.boot.val ## [1] 0.44575758 0.30144928 0.15280899 0.07665706 -0.02155689 ## [6] -0.10143678 -0.25187032 -0.31164773 -0.44460227 -0.52026316 ## ## $jack.boot.se ## [1] 0.9064458 ## ## $call ## bootstrap(x = x, nboot = 1000, theta = theta, func = bias) Compare this to ‘bias.boot’ (our result from above). Why might it not be the same? Try running the same section of code several times. See how the value of the bias ($func.thetastar) jumps around? We should not be surprised by this because we can look at the jackknife-after-bootstrap estimate of the standard error of the function (in this case, that function is the bias) and we can see that it is not so small that we wouldn’t expect some variation in these values. Remember, everything we have discussed today are estimates. The statistic as applied to your data will change with new data, as will the standard error, the confidence intervals - everything! All of these values have sampling distributions and are subject to change if you repeated the procedure with new data. Note that we can calculate any function of \\(\\theta^{*}\\). A simple example would be the 72nd percentile: perc72&lt;-function(x) { quantile(x,probs=c(0.72)) } results&lt;-bootstrap(x=x,nboot=1000,theta=theta,func=perc72) results ## $thetastar ## [1] 4.6 2.9 5.4 4.6 4.4 4.6 3.2 4.5 4.0 4.6 5.4 3.6 5.4 4.7 6.1 4.8 5.2 ## [18] 4.7 3.7 5.6 5.6 6.6 5.4 3.7 4.8 5.9 4.1 5.1 4.2 4.1 5.9 4.9 5.0 3.8 ## [35] 4.5 2.8 5.9 6.2 3.8 3.2 2.9 2.8 5.5 3.3 2.8 5.2 3.5 2.8 3.7 3.5 7.0 ## [52] 3.7 6.1 4.8 5.1 3.1 4.8 6.0 4.4 5.5 3.6 4.6 4.1 3.7 5.5 5.6 5.3 4.2 ## [69] 3.9 3.3 4.9 5.1 5.1 5.7 4.2 4.0 3.6 5.2 5.4 4.5 5.2 4.9 5.2 5.5 3.8 ## [86] 6.1 3.7 4.9 5.9 4.3 3.2 3.5 4.3 4.0 5.8 5.1 4.9 5.0 4.5 5.0 4.4 3.5 ## [103] 4.2 4.7 5.8 4.9 3.8 3.4 3.5 2.6 2.7 3.9 3.6 5.7 4.0 3.9 4.1 4.9 5.4 ## [120] 4.8 7.0 5.1 3.4 5.0 4.6 4.2 5.0 6.2 4.5 5.8 3.5 3.5 4.1 4.3 6.5 5.6 ## [137] 5.3 5.0 5.7 4.4 3.3 4.6 3.0 5.8 4.6 4.7 5.1 3.0 3.5 4.4 4.1 6.0 3.6 ## [154] 4.6 4.6 4.5 5.8 5.2 5.7 4.9 4.4 3.8 5.4 5.2 4.4 4.3 4.1 5.0 3.7 5.7 ## [171] 4.8 5.2 3.2 3.9 5.1 4.3 5.3 5.9 2.7 3.1 4.9 4.5 4.3 3.9 5.7 4.9 3.9 ## [188] 6.6 5.1 4.6 6.0 4.1 3.2 5.1 4.6 4.2 4.2 5.1 4.9 2.7 6.6 3.3 4.9 4.2 ## [205] 3.2 4.6 5.7 4.7 2.3 3.7 4.0 3.3 4.1 5.0 7.0 4.3 2.7 4.2 2.6 4.3 3.8 ## [222] 5.4 3.9 3.7 4.0 3.1 3.9 4.8 3.5 4.2 4.1 4.0 4.7 3.9 3.5 6.3 4.7 4.5 ## [239] 4.6 5.8 4.2 5.3 3.9 5.0 5.9 5.7 4.2 3.7 4.4 5.5 3.9 3.9 4.7 4.3 5.8 ## [256] 4.6 5.4 3.4 4.1 4.9 5.6 7.0 3.9 4.5 4.3 3.7 3.8 4.5 6.4 4.3 3.4 2.6 ## [273] 3.4 3.9 3.8 5.2 6.3 4.6 4.3 5.6 4.9 4.7 4.0 4.7 3.9 4.8 4.5 4.2 4.2 ## [290] 3.6 4.6 6.4 4.7 3.3 3.7 5.6 4.7 4.9 5.3 5.7 3.5 4.9 3.9 3.9 3.6 4.6 ## [307] 3.5 4.7 6.0 3.9 3.6 4.0 4.3 5.0 5.7 3.6 5.9 3.0 3.0 2.7 4.8 2.9 5.6 ## [324] 5.0 7.3 3.8 5.7 5.4 3.1 4.5 4.3 5.2 5.4 4.1 3.5 4.6 3.8 3.5 4.8 5.4 ## [341] 4.5 3.7 3.7 4.2 5.2 4.1 3.5 4.9 3.8 4.5 5.2 4.6 4.2 5.4 4.4 6.1 5.1 ## [358] 4.0 3.2 3.7 3.7 3.7 4.5 4.7 4.2 6.5 3.3 4.1 4.2 4.1 3.5 3.8 5.0 4.2 ## [375] 5.2 3.0 3.8 4.5 6.1 6.2 4.2 5.4 5.1 4.3 1.6 4.5 5.3 6.1 5.0 3.9 3.7 ## [392] 4.3 3.9 4.9 3.7 4.0 4.4 4.3 4.8 2.1 3.5 5.2 3.4 3.7 6.1 5.5 5.2 3.5 ## [409] 4.7 3.8 4.9 5.6 2.8 5.1 5.9 5.5 5.5 4.7 4.4 5.8 5.2 4.6 4.9 4.5 5.2 ## [426] 3.7 5.3 4.5 5.3 5.0 4.0 4.7 4.7 5.7 4.4 4.2 6.4 5.7 6.0 3.9 3.5 4.9 ## [443] 5.2 3.7 4.6 4.7 5.3 5.1 3.4 4.5 3.4 5.1 4.5 3.4 3.7 5.5 4.8 4.4 4.6 ## [460] 4.6 4.4 5.6 5.1 4.3 6.5 6.0 5.2 4.5 4.1 4.7 3.5 3.9 5.7 4.0 4.7 3.3 ## [477] 5.2 4.5 3.4 5.2 4.1 5.3 4.5 5.3 4.8 6.5 2.8 4.8 4.3 4.9 4.3 3.3 4.0 ## [494] 3.3 3.3 4.2 6.0 5.9 5.1 4.8 4.6 3.4 5.5 5.4 4.4 6.1 3.1 4.6 6.1 4.1 ## [511] 3.9 7.2 5.5 4.7 4.4 5.0 4.2 3.1 4.4 5.5 6.3 5.1 4.7 4.2 7.2 6.5 3.4 ## [528] 4.4 3.4 4.6 4.4 4.1 4.6 4.2 3.7 4.9 3.5 4.1 5.7 3.6 4.6 3.7 5.4 5.2 ## [545] 3.5 4.7 4.8 3.2 3.3 3.6 2.6 3.7 4.1 4.7 6.5 4.8 4.7 3.3 4.5 5.6 6.1 ## [562] 4.4 4.9 4.8 4.5 4.7 3.9 4.9 4.2 5.3 4.9 4.9 3.1 4.3 4.6 5.3 6.3 4.7 ## [579] 4.5 5.3 3.7 6.8 5.5 4.3 4.6 5.1 5.0 3.8 5.1 5.1 2.4 4.4 4.8 5.7 4.3 ## [596] 4.7 4.1 6.7 5.0 2.5 4.2 3.0 3.5 4.4 4.9 5.1 4.2 5.1 3.4 4.5 4.7 4.1 ## [613] 2.8 5.0 5.5 4.8 5.4 3.7 4.7 5.7 6.7 5.0 4.5 4.2 5.7 4.6 4.6 4.6 4.4 ## [630] 4.1 3.1 4.7 4.6 3.7 6.3 5.8 3.3 4.6 5.1 3.1 4.6 4.6 5.5 6.2 4.0 4.6 ## [647] 5.4 4.2 4.4 4.2 5.1 5.2 4.6 4.4 4.2 5.2 4.2 3.9 5.0 3.5 4.0 5.3 4.7 ## [664] 5.0 5.8 4.3 5.6 4.1 3.7 4.6 5.3 6.0 3.6 4.8 2.8 2.4 4.0 5.5 3.9 3.9 ## [681] 4.3 3.8 3.9 3.5 5.3 4.1 4.5 5.0 5.7 3.6 4.9 4.4 3.9 3.4 5.4 5.7 5.3 ## [698] 5.6 5.4 5.2 4.0 3.3 4.6 6.7 4.9 6.5 4.8 4.8 4.4 5.9 4.2 5.3 4.1 4.2 ## [715] 2.8 4.6 5.7 4.4 4.0 5.6 3.9 4.7 6.1 3.6 4.2 4.3 3.5 3.7 3.0 4.6 3.1 ## [732] 5.7 3.8 2.8 4.1 4.8 4.7 3.6 4.8 4.6 5.6 3.1 5.5 6.1 4.8 4.1 5.2 4.6 ## [749] 4.1 5.6 5.8 3.7 4.2 4.8 5.2 3.8 3.0 5.2 5.7 3.8 3.6 6.3 3.8 3.6 3.7 ## [766] 5.3 4.0 3.4 3.8 4.9 5.0 4.8 5.8 2.9 5.6 3.1 3.0 4.9 5.8 5.1 4.6 3.5 ## [783] 3.1 3.2 4.9 3.4 4.7 4.3 5.8 5.5 4.4 5.5 4.2 6.4 5.5 2.5 5.2 3.7 4.4 ## [800] 4.3 5.7 4.9 4.7 5.0 4.5 5.1 3.7 5.8 3.9 4.4 5.0 3.5 6.0 5.0 4.4 4.3 ## [817] 3.0 3.7 3.8 4.7 3.6 3.6 4.0 4.8 3.7 5.0 4.7 4.4 4.0 4.0 3.6 3.9 6.0 ## [834] 3.8 5.4 3.4 4.7 4.4 6.2 3.6 4.8 4.8 4.5 4.8 5.5 3.5 3.8 3.8 4.0 2.4 ## [851] 4.5 3.6 4.2 6.3 4.7 4.4 4.5 3.0 2.6 3.0 3.6 4.1 4.7 2.8 4.1 4.9 5.4 ## [868] 4.3 4.7 4.5 4.0 4.8 4.2 4.2 4.8 5.6 4.5 3.1 2.5 4.7 4.4 5.5 5.4 4.4 ## [885] 4.3 4.4 3.0 3.6 4.8 5.3 2.9 4.8 5.8 5.8 5.5 4.0 4.5 4.5 3.5 5.0 4.5 ## [902] 4.1 3.9 4.6 2.6 4.1 3.1 5.4 5.2 4.7 4.6 4.5 5.0 4.8 3.8 5.3 5.0 5.5 ## [919] 5.2 5.9 4.8 5.3 3.7 6.0 5.1 3.9 3.4 4.6 4.0 4.5 5.0 3.2 4.8 5.5 2.3 ## [936] 4.3 3.2 3.9 4.3 3.8 3.9 5.4 3.8 4.9 4.1 5.2 4.2 4.8 4.5 5.1 4.0 4.7 ## [953] 5.6 4.0 3.2 4.6 5.5 4.0 2.9 4.7 4.3 3.7 6.0 5.4 3.1 4.0 4.7 5.3 3.9 ## [970] 5.1 3.8 4.0 5.2 4.7 4.9 2.7 4.9 4.8 6.3 4.4 3.8 4.7 3.5 5.1 5.8 6.8 ## [987] 4.7 5.1 5.5 4.5 6.1 4.4 4.3 3.9 5.2 4.1 5.0 6.1 4.0 7.2 ## ## $func.thetastar ## 72% ## 5.1 ## ## $jack.boot.val ## [1] 5.5 5.5 5.4 5.3 5.2 5.1 4.9 4.7 4.7 4.5 ## ## $jack.boot.se ## [1] 1.028786 ## ## $call ## bootstrap(x = x, nboot = 1000, theta = theta, func = perc72) On Tuesday we went over an example in which we bootstrapped the correlation coefficient between LSAT scores and GPA. To do that, we sampled pairs of (LSAT,GPA) data with replacement. Here is a little script that would do something like that using (X,Y) data that are independently drawn from the normal distribution xdata&lt;-matrix(rnorm(30),ncol=2) Everyone’s data is going to be different. With such a small sample size, it would be easy to get a positive or negative correlation by random change, but on average across everyone’s datasets, there should be zero correlation because the two columns are drawn independently. n&lt;-15 theta&lt;-function(x,xdata) { cor(xdata[x,1],xdata[x,2]) } results&lt;-bootstrap(x=1:n,nboot=50,theta=theta,xdata=xdata) #NB: xdata is passed to the theta function, not needed for bootstrap function itself Notice the parameters that get passed to the ‘bootstrap’ function are: (1) the indexes which will be sampled with replacement. This is different that the raw data but the end result is the same because both the indices and the raw data get passed to the function ‘theta’ (2) the number of bootrapped samples (in this case 50) (3) the function to calculate the statistic (4) the raw data. Lets look at a histogram of the bootstrapped statistics \\(\\theta^{*}\\) and draw a vertical line for the statistic as applied to the original data. hist(results$thetastar,breaks=30,col=&quot;pink&quot;) abline(v=cor(xdata[,1],xdata[,2]),lwd=2) 4.5 Parametric bootstrap Let’s do one quick example of a parametric bootstrap. We haven’t introduced distributions yet (except for the Gaussian, or Normal, distribution, which is the most familiar), so lets spend a few minutes exploring the Gamma distribution, just so we have it to work with for testing out parametric bootstrap. All we need to know is that the Gamma distribution is a continuous, non-negative distribution that takes two parameters, which we call “shape” and “rate”. Lets plot a few examples just to see what a Gamma distribution looks like. (Note that the Gamma distribution can be parameterized by “shape” and “rate” OR by “shape” and “scale”, where “scale” is just 1/“rate”. R will allow you to use either (shape,rate) or (shape,scale) as long as you specify which you are providing. Let’s generate some fairly sparse data from a Gamma distribution original.data&lt;-rgamma(10,3,5) and calculate the skew of the data using the R function ‘skewness’ from the ‘moments’ package. library(moments) theta&lt;-skewness(original.data) head(theta) ## [1] 0.03687528 What is skew? Skew describes how assymetric a distribution is. A distribution with a positive skew is a distribution that is “slumped over” to the right, with a right tail that is longer than the left tail. Alternatively, a distribution with negative skew has a longer left tail. Here we are just using it for illustration, as a property of a distribution that you may want to estimate using your data. Lets use ‘fitdistr’ to fit a gamma distribution to these data. This function is an extremely handy function that takes in your data, the name of the distribution you are fitting, and some starting values (for the estimation optimizer under the hood), and it will return the parameter values (and their standard errors). We will learn in a couple weeks how R is doing this, but for now we will just use it out of the box. (Because we generated the data, we happen to know that the data are gamma distributed. In general we wouldn’t know that, and we will see in a second that our assumption about the shape of the data really does make a difference.) library(MASS) fit&lt;-fitdistr(original.data,dgamma,list(shape=1,rate=1)) ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced # fit&lt;-fitdistr(original.data,&quot;gamma&quot;) # The second version would also work. fit ## shape rate ## 1.7456725 2.6363070 ## (0.7185209) (1.2552254) Now lets sample with replacement from this new distribution and calculate the skewness at each step: results&lt;-c() for (i in 1:1000) { x.star&lt;-rgamma(length(original.data),shape=fit$estimate[1],rate=fit$estimate[2]) results&lt;-c(results,skewness(x.star)) } head(results) ## [1] 0.6869693 1.2565702 0.5520709 0.3008216 0.1920771 0.3859516 hist(results,breaks=30,col=&quot;pink&quot;,ylim=c(0,1),freq=F) Now we have the bootstrap distribution for skewness (the \\(\\theta^{*}\\) s), we can compare that to the equivalent non-parametric bootstrap: results2&lt;-bootstrap(x=original.data,nboot=1000,theta=skewness) results2 ## $thetastar ## [1] -0.2985073233 0.2464441624 -0.4765161824 -0.5984916971 ## [5] 0.0124193796 0.3890992365 0.0628915809 0.1561811491 ## [9] 0.7014815291 0.0182763042 -0.0423447548 -0.5978992279 ## [13] 0.3639380642 -0.7422112659 0.3877632798 -0.2706662359 ## [17] -0.4804477981 0.1254434119 -0.3425444962 -0.1631575261 ## [21] 0.0448391020 -0.2709227846 -0.4811533248 0.0215045210 ## [25] -0.0383677867 0.3212035127 0.3448641019 -0.0041380293 ## [29] 0.1247535179 -0.2581440976 0.1359001578 0.0200513255 ## [33] -0.2263504617 0.7477637886 -0.0750344043 0.4207512676 ## [37] -0.3824400995 0.6972052431 0.0328378768 0.3601263030 ## [41] -0.2527958345 1.3342697971 0.0697921008 -0.6821359391 ## [45] -0.4813242056 0.3994446004 -0.4723073930 -0.0500306873 ## [49] -0.3905444491 -1.1738173574 -0.2429346863 -0.2595676842 ## [53] 0.1165925567 0.0270020827 -0.0457578616 0.5423250840 ## [57] 0.2736189952 0.0024869052 0.6259427583 0.5445808389 ## [61] -0.9363163511 0.7554142833 -0.4639111675 -0.4413301193 ## [65] -0.1678132594 0.7817642343 0.9697986412 -0.2576040664 ## [69] -0.2411890195 0.0882535509 0.4485862249 0.6970351413 ## [73] -1.0551090136 0.0444994865 0.1582613440 0.0060373564 ## [77] 0.7081994847 0.2149976503 0.0817267982 -0.8504605911 ## [81] 0.3409701104 -0.4976176705 -0.9112572937 0.2197363810 ## [85] 0.1864971428 1.2403202332 -0.0966809412 -0.1915403227 ## [89] 0.2133825071 0.0211429142 0.2086501659 -0.0924251605 ## [93] 0.4706039322 0.6331704789 -0.3039037115 -0.3387798017 ## [97] -0.7399163279 0.3233670005 -0.0364739797 0.7335927132 ## [101] 0.6233837800 0.0194587164 -0.4396094652 -0.1857222260 ## [105] 0.5801303526 0.1792207098 0.4763991408 -0.5093277007 ## [109] -0.6371878358 0.0367991254 0.0958537237 0.0648831280 ## [113] 0.1150629899 0.0854082989 0.0201975984 0.1744625225 ## [117] 0.0249155363 0.7483065621 0.0358279784 -0.0401060940 ## [121] 0.3201678478 -0.7479979259 -0.1982790839 -0.9668976366 ## [125] 0.3470389429 0.0664068358 0.0560247360 0.5662330433 ## [129] -0.2673128206 -0.5581490287 0.0005477915 0.5506466527 ## [133] -0.4467672412 0.6233837800 0.0731774563 0.2174335803 ## [137] 0.0227391236 -0.7734928842 -0.2450586585 0.4556003954 ## [141] -0.9662208976 0.4411799417 0.2719176630 -0.1487190593 ## [145] -0.0411965470 -0.0521620098 -0.0383679986 -0.4506708173 ## [149] -0.2598434525 0.7212122833 0.1083780047 0.1846331437 ## [153] 0.7700626881 0.1941462005 -0.4039740106 0.0732907877 ## [157] -0.1373689941 -0.3886902670 -0.6327017258 -0.5219786946 ## [161] -0.1003502583 0.4047121023 -0.9687851134 0.2550481308 ## [165] -0.4037043200 -0.5986534527 0.6061880094 0.8775529414 ## [169] -0.9961235212 0.6250373896 0.3160085067 -0.2923881551 ## [173] 0.2714448377 0.0764196133 0.9242764501 0.4802625229 ## [177] 0.2766734887 0.1650258730 -0.0598459419 0.7584719955 ## [181] 0.0732473986 -0.2669182423 0.1924986228 -0.2316311275 ## [185] -0.0559624976 0.0417927325 -0.8486656053 1.0548712724 ## [189] -0.6106285630 0.0927647002 0.5877188892 -0.2767885457 ## [193] 0.5916254825 0.1235967859 0.9430602073 -0.5900466617 ## [197] -0.4706310568 0.2373370410 0.1482320046 -0.1996991400 ## [201] 0.4959812376 0.4682163361 0.1995396591 0.1355149006 ## [205] 0.0001454452 0.0552044990 -0.3189455611 0.4026477817 ## [209] -0.2850626771 0.2603191021 0.2235982208 1.2389670268 ## [213] 0.4738787240 0.2317860931 -0.3709680447 0.0245024266 ## [217] -0.2179720487 0.1235616470 0.5986912323 -0.4439977767 ## [221] 0.2618779589 -1.2210949257 0.7945174265 0.2910320449 ## [225] 0.0212088198 -0.7649149648 -0.8638878374 0.9332026899 ## [229] -0.6286216963 0.1012140088 -0.5167669033 0.8611902650 ## [233] 1.1208595689 -0.2157968735 -0.4479334721 -0.0452679438 ## [237] 0.2861427154 -0.1566134179 0.0345516217 -0.3661612354 ## [241] 0.3432799883 -0.9798572672 0.0054314744 -0.0976817232 ## [245] -0.0233361467 0.1075295890 0.2201789866 0.3108417422 ## [249] 0.4293971988 -0.3972452827 0.4375128822 0.2873944861 ## [253] -0.3344088649 0.8126806812 0.2364579678 0.2382817734 ## [257] -0.9491995889 -0.3059263875 -0.1779185101 -1.0079839877 ## [261] -0.2387648314 0.3667906497 -0.1900355935 0.0954566608 ## [265] 0.1824055392 1.6730239828 0.2695262947 0.0992034236 ## [269] -0.2056400287 0.0711427286 0.5594435478 -0.2446106204 ## [273] 0.4299096645 -0.1140613473 -0.2431245333 0.0287646957 ## [277] -0.2188833832 -0.4608969955 0.1284614449 0.0295386731 ## [281] 0.4939943495 -0.0846423952 -0.3629517275 0.6408816231 ## [285] 0.3757530093 0.6044952341 -0.2466479292 0.2695013802 ## [289] 0.2580202323 -0.2476350066 0.5571893236 -0.2157968735 ## [293] -0.2205333177 1.0792276747 0.7778814646 0.1417436770 ## [297] -0.1426088236 0.1772395373 0.1637473638 -0.0154871574 ## [301] 0.1345746135 0.2373727406 -0.2306253897 0.4116097081 ## [305] 0.4496974813 0.6774209737 0.0339545936 -0.4652174325 ## [309] -0.1099034242 -0.4258548240 0.2419977030 0.8503600890 ## [313] 0.3532436599 -0.5548672328 -0.3284988180 0.0532947011 ## [317] 0.1670886700 -0.1157341424 -0.1866648360 0.0390290441 ## [321] 0.3981793474 -0.3105100425 1.3601981280 0.4045507024 ## [325] -0.3687219857 0.0408241641 -0.7248065239 -0.0317615759 ## [329] -0.2942287050 0.1808599013 0.0717910838 0.0343411807 ## [333] -0.2232589527 0.9983713425 -0.3304947371 -0.0066794075 ## [337] -0.3777253071 0.5331550166 -0.2166085819 -0.4517790618 ## [341] -0.0585210814 -0.0138678244 0.0174857274 0.2059129875 ## [345] 0.4391484670 0.4818988921 -0.6161812212 -0.5921299213 ## [349] 0.0981839505 0.0854133076 -0.2156933016 0.2360929555 ## [353] 0.2190170413 -0.7285229776 0.2268965576 -0.6517533347 ## [357] -0.0843197984 -0.4880902041 -0.0326174226 -0.5476807069 ## [361] 1.0190954215 0.8691551716 0.0318041487 0.1669281255 ## [365] 0.3012852247 0.4517750512 0.4153330421 -0.2748700990 ## [369] 0.1294561665 -0.0231303679 0.1385687699 -0.1772757287 ## [373] -0.8437134217 -0.4909904551 0.8634522604 0.2810624841 ## [377] 0.3472619303 -0.2131034352 -0.4436237750 0.0530450316 ## [381] -0.2093289070 0.2153731750 0.0464515565 -0.1511805772 ## [385] -0.7134534077 0.3566168536 -0.6599589814 0.8235842976 ## [389] 0.2018084943 -0.4027791540 0.0713718835 -0.5398537163 ## [393] -0.0322906640 0.5390179349 0.4266570053 -0.0883644632 ## [397] -0.3813699886 0.9981925636 0.6970311097 0.0556293276 ## [401] -0.4437600764 -0.3445731536 -0.0824720300 -1.0105536586 ## [405] -0.3584281858 0.2955871717 0.1814199417 0.4585633931 ## [409] -0.1191078924 -0.0087300145 0.5219187343 -0.3888905110 ## [413] -0.6488333113 0.1074819689 0.7664429585 0.0041777054 ## [417] 1.2057909704 0.2718787153 0.3741137982 0.3684833569 ## [421] 0.3476068105 -0.1143970571 0.5404691672 0.8227454962 ## [425] -0.4007156186 0.2873272308 0.3944775059 -0.3503981967 ## [429] -0.1415589821 0.1232286527 0.3337266026 -0.5894965694 ## [433] -0.4302517566 0.8824520081 -0.3474081534 0.4932076586 ## [437] -0.3826769546 0.9664384366 -0.8107674092 -0.6256217912 ## [441] -0.9343569240 0.2909572434 0.0186208743 -0.0402943150 ## [445] 0.0487643110 0.3981793474 0.4408035908 0.3852277741 ## [449] -0.0677535655 0.2086786733 -0.4827411992 -0.2825289328 ## [453] -0.1544111522 -0.6317210315 -0.3126403652 -0.3395581298 ## [457] -0.0910275435 0.5175468077 0.3117537478 0.1538712966 ## [461] 0.3597037096 0.4845120896 -0.0714476101 0.3712162232 ## [465] 0.7686712886 0.2506229738 0.2047312196 0.3094690348 ## [469] 0.3214668744 0.1029158970 -0.0205089293 0.4962476357 ## [473] 0.6848453299 0.0080018518 0.2349954930 0.2092596929 ## [477] 0.4834299377 0.4128156792 -0.4523163146 -0.5400397169 ## [481] 0.3982655609 0.0133611461 -1.0541674448 -0.0442556604 ## [485] 0.3267755160 -0.0416000898 -0.5019659088 0.0800026832 ## [489] 0.7839245678 0.4239808854 0.6530132181 0.6256233530 ## [493] 0.0884179364 -0.1495110305 0.0198426496 -0.3330931940 ## [497] 0.7167966785 0.7526652947 -1.2067610085 0.0287139675 ## [501] 0.2446967335 0.8527198930 0.9746011808 0.0766431113 ## [505] 0.6824013132 -0.5647308309 -0.1157341424 0.0977590087 ## [509] 0.4843151885 -0.3649232554 0.3867812923 0.4028917467 ## [513] -0.6923950968 0.6337520833 0.6989321257 0.0080082423 ## [517] 0.3952242183 0.5533095168 -0.2739500914 0.3416363185 ## [521] 0.0109772506 0.8624697167 -0.3846491844 -0.5119217848 ## [525] 0.3292562157 -0.2330854687 -0.0867784255 0.4223154890 ## [529] -0.2121690037 -0.3315642586 -0.6992717899 0.2123417080 ## [533] -1.7009024979 0.0702190106 -0.3177971366 -0.1991157063 ## [537] -0.0735531594 -1.1739378307 0.0710382056 -0.0364562154 ## [541] -0.0017167762 -0.4041127159 -0.4510442082 -0.3687757328 ## [545] -0.7397282947 0.6088486103 -0.0297072223 0.3264831879 ## [549] 1.7455572450 -0.1347422490 -0.3442428836 -0.2261020372 ## [553] 0.6820150043 -0.4629470318 0.4532918096 -0.2207289778 ## [557] 0.2985794598 -0.3486188483 -0.2905826626 0.0318041487 ## [561] 0.5022629146 0.1081843862 0.1388689729 -0.0053783322 ## [565] -0.3497055465 0.6710946670 1.1247561183 -0.2035271260 ## [569] 0.1840576597 -0.6243261518 -0.2485868914 0.4893826806 ## [573] 0.0838751982 -0.4102421562 -0.3686316138 -0.3673059587 ## [577] 0.3237085406 -0.0317615759 1.0558849018 -0.1756332139 ## [581] 0.1868283581 0.6642074716 -0.5578765021 -0.3534067383 ## [585] -0.6236582187 0.5659788624 -0.4025222973 -0.2878461722 ## [589] -0.6508963141 0.6624343248 -0.0949131931 0.3105348542 ## [593] 0.5889581663 -0.1015608443 -0.1645176571 -0.3443350275 ## [597] 0.6437469830 0.5841604198 -0.2573979206 0.0819118223 ## [601] -0.4876008256 0.6322977545 -0.6391137507 -0.2707722839 ## [605] -0.2570058717 -0.0802837143 0.0162417675 0.1008185058 ## [609] -0.0952717802 0.5017141412 -0.2425529683 -0.3005941047 ## [613] 0.1970731085 -0.2467233924 -0.1041023449 0.8246584109 ## [617] -0.5178485818 0.3123661636 -0.2016006255 -0.0882239563 ## [621] 0.7776889065 0.2003379427 -0.3974552370 0.0481090694 ## [625] 0.0343772599 0.4696377109 0.0029492217 -1.2188788168 ## [629] -0.5985589835 -0.1715601503 -0.5659362447 0.4304445978 ## [633] 1.1147415654 0.2901751220 0.7304186065 -0.4560523246 ## [637] -0.1731051111 0.5644311661 0.8621113006 0.5542184973 ## [641] -0.0741278621 0.4866189732 -0.7754237734 0.1332857540 ## [645] 0.6073314348 0.1071137319 0.9194266921 0.2210118153 ## [649] 0.6639575424 -0.8715881169 -0.5180452434 0.3632940943 ## [653] -0.2768844090 1.4388894763 0.0487449648 1.6874240810 ## [657] -0.4055133821 -0.3730269864 0.0110003745 0.4328652970 ## [661] -0.4949805694 -0.4811533248 -0.0376417602 -0.1931458784 ## [665] -0.1192639341 0.9113665102 -0.2736961475 0.1889905051 ## [669] 0.3710628205 0.3046612427 -0.3648379305 0.3017182886 ## [673] 0.0310170500 -0.0557758420 -0.2161533214 -0.8148937357 ## [677] 0.4797531779 -0.2974194164 -0.9562465263 0.4439627553 ## [681] 0.0465317663 0.1594443607 0.5659852529 0.6260719897 ## [685] -0.6273586889 0.3233559670 -0.9368327066 0.1887941572 ## [689] -0.2453865954 -0.1872500201 0.6212458169 0.3895705575 ## [693] -0.4270461258 -1.1240449027 0.7451020784 0.4815448051 ## [697] 0.3123328386 -0.3846491844 -0.1481872973 0.0343772599 ## [701] -0.0308438127 -0.1437254469 0.5698860225 -0.0149653377 ## [705] -0.3962111559 -0.2633578458 0.3719462201 -0.3841942563 ## [709] -0.7804367080 0.4889516043 -0.1049604076 0.5892847454 ## [713] 0.4955956747 -0.2560756655 -0.4434262701 0.0405640174 ## [717] 0.5100423018 0.1213998683 0.3222764652 -0.4297912316 ## [721] -0.0678186671 0.6612618297 -0.0671339621 -0.3998089316 ## [725] 0.2564157735 -0.3871034389 -0.1434582891 -0.5870962232 ## [729] 0.8632957741 0.0119605093 0.4339542450 0.0261725698 ## [733] -0.2798235126 0.6725112952 -1.9750362865 0.4935407718 ## [737] 0.6346921540 -0.7166634456 0.7011370833 0.0492666281 ## [741] 0.4538977283 0.2609081377 0.5396961014 -0.4877584049 ## [745] -0.7276876210 -0.1513914718 0.5150995628 0.4682101811 ## [749] -0.1318909293 0.5651254226 -0.2196536918 0.6068304085 ## [753] -0.0099083178 0.4221835585 -0.3508318462 -0.1086526811 ## [757] -0.3387658952 0.3006213708 -0.0852829820 -0.5600081902 ## [761] -0.7648285885 0.7494130444 -0.5815162166 0.4839957739 ## [765] -0.1416077251 0.0212239031 0.4389748847 1.2024517828 ## [769] 0.2421525622 -0.5374443036 -0.4357098890 0.2288246968 ## [773] 0.7642384199 0.0872274186 -0.0543453504 -0.0515808984 ## [777] 0.4391226553 0.2892073951 0.0250171362 -0.5658695919 ## [781] 1.0968380161 -0.0568885863 1.2847495339 0.2019849592 ## [785] -0.5532708396 -0.2581784518 -0.0350326833 -0.2820478641 ## [789] -0.6519042211 0.7865635865 0.4788153376 -0.3230305955 ## [793] 1.1480261742 0.2565792671 0.6488789820 -0.2845223395 ## [797] -0.0636210713 -0.3292894066 0.4566618488 -0.2175912082 ## [801] -0.3037517069 0.9361980678 0.1330976609 0.3366502198 ## [805] -0.2308779109 -0.0281302568 -0.3195169988 0.9688232919 ## [809] 0.3595831644 -0.4798527209 1.2308762207 -0.0633338555 ## [813] 0.0390290441 -0.0735706893 0.6664340170 0.8091893977 ## [817] -0.0842374706 0.2308913565 -0.7009788150 -1.1902851012 ## [821] 0.1556513940 0.5799654571 -0.1227450704 -0.1305695735 ## [825] 0.7807591847 0.6912025199 0.2138990603 -1.2247273391 ## [829] 0.0635412494 0.6036208647 0.3983256525 -0.3047201739 ## [833] -0.0701216034 1.1539079096 -0.1570152223 0.0529390205 ## [837] 0.7174455266 0.6561665863 0.7825217006 0.2578281515 ## [841] -0.0755679332 0.2165827730 0.6576093804 1.1592200163 ## [845] 0.2166447345 -0.4886388561 -0.2149331355 -0.1132871809 ## [849] 0.1224029397 -0.3076102315 -0.5393776224 -0.9433536177 ## [853] 0.0479829329 -0.0425592639 0.2010134838 -0.5048253009 ## [857] -0.4767760720 0.5609834249 -0.2578005647 0.4600317022 ## [861] 0.5423250840 0.7334877621 -0.3869987579 0.3562060511 ## [865] 0.3071069009 0.2758213361 0.6513642343 -0.3629517275 ## [869] -0.3966496109 0.0147477544 -0.6497672690 0.2745968292 ## [873] -0.5210390555 -0.1476293530 1.5281286042 0.3240594110 ## [877] -0.3092779993 0.2795706759 -0.0455851806 0.2341025080 ## [881] 0.1412514104 0.3838230309 -0.7820370885 0.5291655082 ## [885] -0.6406852243 0.5416175287 -0.0591807393 0.8387578238 ## [889] -0.1010053785 -0.9226836470 0.0432827230 0.2923985929 ## [893] 0.5243725822 0.8336705162 -0.0635203994 0.6058001721 ## [897] 0.6088566491 -0.4398720145 -0.3062876921 0.4883815748 ## [901] 0.3651158933 -0.4023834435 0.9700388228 -0.5590766744 ## [905] -0.0458369954 -0.5368672273 -0.5515277516 -0.0526580692 ## [909] -0.0048037430 0.0560406585 0.3377874447 -0.2830215676 ## [913] -0.9326376700 -1.1272211305 0.8333004663 0.3618588359 ## [917] -1.1239288415 -0.1045345655 -0.6791221930 -0.2849347266 ## [921] 0.3308596138 0.5626578091 -0.3055415510 0.3805769686 ## [925] -0.3028076860 0.2852027412 -0.6842002657 1.0642508535 ## [929] -0.1165538725 -0.0158410112 0.0804550439 -0.2476178412 ## [933] 0.2070707255 -0.3340649522 0.1548645211 0.3168951586 ## [937] -0.1468194149 0.5933476200 0.5545024326 0.2631166748 ## [941] -0.4399568636 0.3502473842 0.1478579139 -0.0754817101 ## [945] 0.4784157760 -0.2598434525 -0.5398763986 0.5494921345 ## [949] -0.9823145911 0.0159009579 -0.1284936212 0.7821894596 ## [953] 0.2212580447 -0.1547420403 0.3091298664 -0.1713647711 ## [957] -0.5701832843 -0.2459568667 -0.3292262165 -0.8291510747 ## [961] 0.2966646263 -0.5813252984 -0.8045527356 -0.0032807587 ## [965] -0.9678526440 -1.1419101147 -0.1932530042 0.4536972002 ## [969] 0.9744984981 0.2868108025 -0.3039037115 0.2946260162 ## [973] 0.0929223672 0.5506466527 0.4190963079 0.7746087126 ## [977] 0.1483270162 0.2608715203 0.2065557099 -0.0027695943 ## [981] 1.0518648616 0.7142962515 0.6681795410 0.5763063262 ## [985] -0.5511759163 0.3169824505 1.8473642373 0.2297263010 ## [989] -0.4676714042 0.3100462864 0.0961655884 -0.7054857677 ## [993] 0.0560406585 0.5669289855 -0.2760023054 0.1995396591 ## [997] -0.5265975370 0.3869910273 0.3169441753 -0.6151743943 ## ## $func.thetastar ## NULL ## ## $jack.boot.val ## NULL ## ## $jack.boot.se ## NULL ## ## $call ## bootstrap(x = original.data, nboot = 1000, theta = skewness) hist(results,breaks=30,col=&quot;pink&quot;,ylim=c(0,1),freq=F) hist(results2$thetastar,breaks=30,border=&quot;purple&quot;,add=T,density=20,col=&quot;purple&quot;,freq=F) What would have happened if we would have fit a normal distribution instead of a gamma distribution? fit2&lt;-fitdistr(original.data,dnorm,start=list(mean=1,sd=1)) ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced fit2 ## mean sd ## 0.6621666 0.4032788 ## (0.1275280) (0.0901740) results.norm&lt;-c() for (i in 1:1000) { x.star&lt;-rnorm(length(original.data),mean=fit2$estimate[1],sd=fit2$estimate[2]) results.norm&lt;-c(results.norm,skewness(x.star)) } head(results.norm) ## [1] 1.0249052 1.3473262 -0.1693246 -0.1950661 0.5518892 0.3147267 hist(results,breaks=30,col=&quot;pink&quot;,ylim=c(0,1),freq=F) hist(results.norm,breaks=30,col=&quot;lightgreen&quot;,freq=F,add=T) hist(results2$thetastar,breaks=30,border=&quot;purple&quot;,add=T,density=20,col=&quot;purple&quot;,freq=F) All three methods (two parametric and one non-parametric) really do give different distributions for the bootstrapped statistic, so the choice of which method is best depends a lot on the situation, how much data you have, and what you might already know about the underlying distribution. Jackknifing is just as easy at bootstrapping. Here we will do a trivial example for illustration. We will write a little function for the mean even though you could put the function in directly with ‘jackknife(x,mean)’ theta&lt;-function(x) { mean(x) } x&lt;-seq(0,9,by=1) results&lt;-jackknife(x=x,theta=theta) results ## $jack.se ## [1] 0.9574271 ## ## $jack.bias ## [1] 0 ## ## $jack.values ## [1] 5.000000 4.888889 4.777778 4.666667 4.555556 4.444444 4.333333 ## [8] 4.222222 4.111111 4.000000 ## ## $call ## jackknife(x = x, theta = theta) Why do we not have to tell the ‘jackknife’ function how many replicates to do? Let’s compare this with what we would have obtained from bootstrapping results2&lt;-bootstrap(x,1000,theta) mean(results2$thetastar)-mean(x) #this is the bias ## [1] 0.0371 sd(results2$thetastar) #the standard deviation of the theta stars is the SE of the statistic (in this case, the mean) ## [1] 0.8890917 Everything we have done to this point used the R package ‘bootstrap’ - now lets compare that with the R package ‘boot’. To avoid any confusion (a.k.a. masking) between the two packages, I recommend detaching the bootstrap package from the workspace with detach(&quot;package:bootstrap&quot;) The ‘boot’ package is now recommended over the ‘bootstrap’ package, but they give the same answers and to some extent it is personal preference which one prefers to use. We will still use the mean as the statistic of interest, but we will have to write a new function for it because the syntax of the ‘boot’ package is slightly different: library(boot) theta&lt;-function(x,index) { mean(x[index]) } boot(x,theta,R=999) ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = x, statistic = theta, R = 999) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 4.5 0.01161161 0.910188 One of the main advantages to the ‘boot’ package over the ‘bootstrap’ package is the nicer formatting of the output. Going back to our original code, lets see how we could reproduce all of these numbers: table(sample(x,size=length(x),replace=T)) ## ## 0 3 4 6 8 9 ## 1 1 2 4 1 1 xmeans&lt;-vector(length=1000) for (i in 1:1000) { xmeans[i]&lt;-mean(sample(x,replace=T)) } mean(x) ## [1] 4.5 bias&lt;-mean(xmeans)-mean(x) se.boot&lt;-sd(xmeans) bias ## [1] 0.0217 se.boot ## [1] 0.9063225 Why do our numbers not agree exactly with those of the boot package? This is because our estimates of bias and standard error are just estimates, and they carry with them their own uncertainties. That is one of the reasons we might bother doing jackknife-after-bootstrap. The ‘boot’ package has a LOT of functionality. If we have time, we will come back to some of these more complex functions later in the semester as we cover topics like regression and glm. "],
["week-3-lecture.html", "5 Week 3 Lecture 5.1 Overview of probability distributions 5.2 Normal (Gaussian) Distribution 5.3 Standard Normal Distribution 5.4 Log-Normal Distribution 5.5 Intermission: Central Limit Theorem 5.6 Poisson Distribution 5.7 Binomial Distribution 5.8 Beta Distribution 5.9 Gamma Distribution 5.10 Some additional notes:", " 5 Week 3 Lecture 5.1 Overview of probability distributions We will cover 7 distributions this week, and several more next week. Please refer to the handout if univariate distributions, and note that the arrows between Normal and Standard Normal need to be reversed. For each distribution, there are five things I want you to know. 1 - Probability density function 2 - General Shape 3 - The expected value \\(E[X]\\) 4 - The variance \\(E[X-E[X]]^2\\) 5 - Relationship to other distributions What is a probability density? if \\(f(x \\mid params)\\) is a probablity density function: \\[ P(a&lt;x&lt;b)=\\int_a^bf(x \\mid params)dx \\] Probablity at a single point is always zero but probablity density is not. The probability density function is not restricted to being \\(\\le1\\) The integral over it’s range is always 1. 5.2 Normal (Gaussian) Distribution The outcome is produced by small number effects acting additively and independently. Normally distributed errors is the most common assumption of linear models. Central Limit theorem! The probability density function of the Normal distribution is given by \\[ f(x \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\] \\[ x \\in \\mathbb{R} \\\\ \\mu \\in \\mathbb{R} \\\\ \\sigma &gt; 0 \\] The shape of the Normal distribution can be illustrated by a few examples The expected value of the Normal distribution is given by \\[ \\begin{align} E[X] &amp;= \\int_{-\\infty}^{\\infty}{X \\cdot f(X)dX} \\\\ &amp;= \\int_{-\\infty}^{\\infty} x \\cdot f(x \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} dx \\\\ &amp;= \\mu \\end{align}\\] The variance of the Normal distribution is given by \\[ \\begin{align} Var[X] &amp;= E[(X- E[X])^2] \\\\ &amp;= E[(X - \\mu)^2] \\\\ &amp;= E[X^2] - \\mu^2 \\\\ &amp;= \\left( \\int_{-\\infty}^{\\infty} x^2 \\cdot f(x \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} dx \\right) - \\mu^2 \\\\ &amp;= \\sigma^2 \\end{align} \\] 5.3 Standard Normal Distribution Raw data rarely fits standard normal. Mostly useful as a theoretical construct in hypothesis testing. The probability density function of the Standard Normal distribution is given by \\[ z = \\frac{x-\\mu}{\\sigma} \\] \\[ f(z \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}z^2} \\] The expected value and variance of the Standard Normal distribution are given by E[X] = 0 Var[X] = 1 Note that the Standard Normal distribution is a linear transformation of the Normal distribution (centered on zero with variance equal to 1). 5.4 Log-Normal Distribution The outcome is produced by small number effects acting multiplicatively and independently. Often used for things where small grows slowly and big grows quickly, such as forest fires or insurence claims. The probability density function of the Log-Normal distribution is given by \\[ \\begin{align} log(X) &amp;\\sim N(\\mu,\\sigma) \\\\ X &amp;\\sim LN(\\mu,\\sigma) \\end{align} \\] \\[ f(x \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}e^{-\\frac{(log(x)-\\mu)^2}{2\\sigma^2}} \\\\ x \\in \\{0,\\infty\\} \\\\ \\mu \\in \\mathbb{R} \\\\ \\sigma &gt; 0 \\] The shape of the Log-Normal distribution can be illustrated with a few examples The expected value and variance of the Log-Normal distribution is given by \\(\\mu\\) is no longer the mean! \\[ E[X] = e^{\\mu + \\frac{\\sigma^2}{2}} \\] \\(\\sigma\\) is no longer the variance! \\[ Var[X] = e^{2(\\mu + \\sigma^2) - (2\\mu + \\sigma^2)} \\] Note that if the log of a variable (X) has a Normal distribution \\[ log(X) \\sim N(\\mu,\\sigma^{2}) \\] than the variable X follows a Log-Normal distribution. NB: Be careful when using the Log-Normal distribution. In particular, keep in mind that the sum of Log-normally distributed variables is NOT Log-Normally distributed. 5.5 Intermission: Central Limit Theorem \\[ X_1,X_2,X_3,...,X_k \\sim N(\\mu,\\sigma^2) \\\\ S_n = \\frac{1}{n} (X_1 + X_2 + X_3,...,X_k) \\\\ \\lim_{n \\to \\infty} S_n \\to N(\\mu,\\frac{\\sigma^2}{n}) \\] X is i.i.d X can be drawn from any distribution! 5.6 Poisson Distribution The Poisson distribution arises principally in 3 situations: 1 - In the description of random spatial point patterns (disease events, complete spatial randomness) 2 - As the frequency distribution of rare but independent events 3 - As the error distribution in linear models of count data The probability mass function of the Poisson distribution is given by \\[ P(x \\mid \\lambda)= \\frac{e^{-\\lambda} \\cdot \\lambda^x}{x!} \\\\ \\lambda&gt;0 \\\\ x \\in \\mathbb{N} \\cup \\{0\\} \\] Note that when variables are discrete (i.e. when the distribution only produces integer numbers), we call the probability density function a probability mass function. The PDF and PMF play the same tole in both cases. The shape of the Poisson distribution is illustrated by a few examples The expected value and variance of the Poisson distribution is given by \\[ \\begin{align} E[X] &amp;= \\sum_{x=1}^{\\infty} x \\frac{e^{-\\lambda} \\cdot \\lambda^x}{x!} \\\\ &amp;= \\lambda \\cdot e^{-\\lambda} \\cdot \\sum_{x=1}^{\\infty} x \\frac{\\lambda^{x-1}}{x!} \\\\ &amp;= \\lambda \\cdot e^{-\\lambda} \\cdot \\sum_{x=1}^{\\infty} \\frac{\\lambda^{x-1}}{(x-1)!}, y = x-1 \\\\ &amp;= \\lambda \\cdot e^{-\\lambda} \\cdot \\sum_{y=0}^{\\infty} \\frac{\\lambda^{y}}{y!} \\\\ &amp;= \\lambda \\cdot e^{-\\lambda} \\cdot e^{\\lambda} \\\\ &amp;= \\lambda\\end{align} \\] \\[ Var[X] = \\lambda \\] The Poisson distribution has the following relationship to the Normal distribution: \\[ \\lim_{\\lambda \\to \\infty} Pois(\\lambda) \\to N(\\lambda, \\lambda) \\] Note that the sum of Poisson distributed variables is itself Poisson distributed. \\[ \\begin{align} X &amp;\\sim Pois(1) \\\\ Y &amp;= \\sum_{i=1}^{\\lambda} X_i \\\\ Y &amp;\\sim Pois(\\lambda) \\end{align} \\] Applying the Central Limit Theorem \\[\\begin{align} \\bar{X} = \\frac{Y}{\\lambda} &amp;\\sim N(1,\\frac{1}{\\lambda}) \\\\ Y &amp;\\sim N(\\lambda, \\lambda) \\end{align} \\] 5.7 Binomial Distribution The probability mass function of the Binomial distribution is given by \\[ P(x \\mid p,n) = \\frac{n!}{x!-(n-x)!}p^x(1-p)^x \\\\ n \\in \\mathbb{N} \\cup \\{0\\} \\\\ x \\in \\{1,2,3,...,n\\} \\\\ p \\in [0,1] \\] The shape of the Binomial distribution is illustrated by the following examples The expected value and variance of the Binomial distribution is given by \\[ \\begin{align} E[X] &amp;= \\sum_{x=1}^n x \\frac{n!}{x!-(n-x)!}p^x(1-p)^x \\\\ &amp;=np \\\\ Var[X] &amp;= np(1-p) \\end{align} \\] The Binomial distribution has the following relationship to the Normal distribution \\[ \\lim_{n \\to \\infty} Binom(n,p) \\to N(np,np(1-p)) \\] 5.8 Beta Distribution One of the few distributions that is restricted to a finite interval (0 and 1). Can be used to model proportions. The probability density function of the Beta distribution is given by \\[ f(x \\mid \\alpha, \\beta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1} \\\\ \\alpha&gt;0 \\\\ \\beta&gt;0 \\\\ x \\in (0,1) \\] Gamma Function: if n is a positive integer \\(\\Gamma(n)=(n-1)!\\) The shape of the Beta distribution is illustrated by the following examples The expected value and variance of the Beta distribution is given by \\[ \\begin{align} E[X] &amp;= \\int_0^1x\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) + \\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}dx \\\\ &amp;= \\frac{\\alpha}{\\alpha + \\beta} \\end{align} \\] \\[ Var[X] = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2 \\cdot(\\alpha + \\beta + 1)} \\] The Beta Distribution’s relationship to the Normal and Uniform distributions are given by \\(Beta(1,1)\\) is the same as \\(Uniform(0,1)\\) \\[ f(x \\mid 1,1) = \\frac{\\Gamma(2)}{\\Gamma(1) + \\Gamma(1)}x^{0}(1-x)^{0} = 1 \\] \\[ \\lim_{\\alpha=\\beta \\to \\infty} \\to N(\\frac{1}{2}, \\frac{1}{8\\alpha + 4}) \\] 5.9 Gamma Distribution Useful for variables that have a positive skew. it is often used to model “waiting times”, such as the time before a device or machine fails. The probability density function of the Gamma distribution (not to be confused with the Gamma Function) is given by \\[ f(x \\mid \\alpha, \\beta) = \\frac{1}{\\beta^\\alpha \\Gamma(\\alpha)} x^{(\\alpha-1)} e^\\frac{-x}{\\beta} \\\\ \\alpha&gt;0 \\\\ \\beta &gt;0 \\\\ x&gt;0\\] The shape of the Gamma distribution is illustrated by the following examples The expected value and variance of the Gamma distribution is given by \\[ \\begin{align} E[X] &amp;= \\int_0^\\infty x\\frac{1}{\\beta^\\alpha \\Gamma(\\alpha)} x^{(\\alpha-1)} e^\\frac{-x}{\\beta} \\\\ &amp;= \\alpha\\beta \\end{align} \\] \\[ Var[X] = \\alpha\\beta^2 \\] The relationship between the Gamma sistribution and the Normal distribution is \\[ \\lim_{\\alpha \\to \\infty} Gamma(\\alpha,\\beta) \\to N(\\alpha \\beta,\\alpha\\beta^2) \\] 5.10 Some additional notes: Please skim through the reading that has been posted “The Algebra of Expectations”. Focus on the “rules”. I want to make sure everyone is clear on the distribution of Normal random variables. If \\[ X\\sim N(\\mu,\\sigma^{2}) \\] then the distribution of a new variable c*X (where c is a constant) is given by \\[ cX\\sim N(c\\mu,c^{2}\\sigma^{2}) \\] The mean is simply multiplied by \\(c\\), and the variance is multiplied by \\(c^{2}\\). To understand why this is, let’s first go into a more detailed derivation of the variance of the distribution of Normal random variables than we did in class. To begin with, you will need to know that one property of expectations is the following: \\[ E[ g(x) ] = \\int g(x) f(x) \\ dx \\] With this knowledge, let’s approach the derivation of the variance, where we are essentially defining \\(g(x) = (X-E[X])^2\\) and using some basic properties of algebra involving integrals. Note that in this derivation we are treating \\(E[X]\\) as a constanct, and not as a function of \\(x\\). \\[ \\begin{eqnarray} Var[X]&amp;=&amp;E[(X-E[X])^{2}] \\\\ &amp;=&amp;\\int (x - E(X))^2 f(x) \\ dx \\\\ &amp;=&amp;\\int (x^2 -2xE(X) + E(X)^2 ) f(x) \\ dx \\\\ &amp;=&amp;\\int x^2 f(x) \\ dx - 2E(X) \\int x f(x) \\ dx + \\int E(X)^2 f(x) \\ dx \\\\ &amp;=&amp; E(X^2) -2 E(X)^2 + E(X)^2 \\\\ &amp;=&amp; E(X^2) - E(X)^2 \\end{eqnarray} \\] "],
["week-3-lab-handout.html", "6 Week 3 Lab Handout 6.1 The Central Limit Theorem 6.2 Exploring the univariate distributions with R 6.3 Standard deviation vs. Standard error", " 6 Week 3 Lab Handout In lab today, we will dive into using R to understand the properties of the univariate distributions, but first we’ll take a short detour to discuss the Central Limit Theorem (or CLT). 6.1 The Central Limit Theorem QUESTION: Why is the normal distribution so fundamental to statistics? ANSWER: The central-limit theorem. Let X1, X2,…, Xn be independently and identically distributed random variables with mean \\(\\mu\\) and finite, non-zero variance \\(\\sigma^{2}\\), \\[ X_{1},X_{2},...,X_{n} \\sim N(\\mu,\\sigma^{2}) \\] and the average of these variable \\(S_{n}\\) be defined as \\[ S_{n} = \\frac{1}{n}(X_{1}+X_{2}+X_{3}+...+X_{n}) \\] Then the Central Limit Theorem states: \\[ \\lim_{n\\rightarrow\\infty} S_{n} \\rightarrow N(\\mu,\\frac{\\sigma^{2}}{n}) \\] Here I have illustrated the CLT using the normal distribution, but the variables X can be drawn from ANY distribution (as long as the X are i.i.d. from a distribution with finite mean and variance), which is remarkable. For example, X could be drawn from a Bernoulli! The CLT is a very general statement, but do not forget the requirements that the mean and standard deviation exist (i.e. are finite). The Cauchy distribution, which is used all the time in atomic physics, has NO MEAN and NO SD – therefore, the CLT would not apply. IMPORTANT SIDE NOTE: The Central Limit Theorem tells us something very important about how well we can estimate the mean of a set of random i.i.d. numbers. Our uncertainty of the mean is given by the variance of \\(S_{n}\\) \\[ \\mbox{variance of estimate of } \\mu = \\frac{s^{2}}{n} \\] where \\[ s^{2} = \\frac{(X-\\bar{X})^{2}}{n-1} \\] is our unbiased estimate of \\(\\sigma^{2}\\). Therefore, we define the STANDARD ERROR of our estimate of \\(\\mu\\) as \\[ \\mbox{s.e. of } \\mu = \\sqrt{\\frac{s^{2}}{n}} \\] Our uncertainty regarding our estimate of \\(\\mu\\) goes down as the \\(\\sqrt{n}\\). DO NOT CONFUSE STANDARD ERROR AND STANDARD DEVIATION. Standard errors are just the standard deviation of a parameter estimate, it expresses uncertainty about the estimate. Standard deviations of a population simply reflect the spread in values. As you increase sample size, standard errors (i.e. standard deviations of the parameter estimate) get smaller and smaller, but standard deviations of the population values do not get smaller with increasing sample size. 6.2 Exploring the univariate distributions with R As a review of last week’s lecture, we can ask a number of things about a statistical distribution: Look at the probability density function: What is the probability of obtaining X (discrete) or a number in the interval (X1,X2) (continuous)? Look at the cumulative probability: What is the probability of obtaining \\(X &lt; X^{*}\\)? Look at the quantiles of the distributions: The inverse of the cumulative distribution - What is \\(X^{*}\\) such that the cumulative probability of obtaining \\(X &lt; X^{*}\\) is the specified quantile? Quantiles can have any size: Quartiles, deciles, percentiles, etc. Look at samples from the distribution: What does the distribution “look like”? There are four basic functions in R: d = probability density function p = cumulative probability q = quantiles of the distribution r = random numbers generated from the distribution We combine these letters with the function names to make all the function calls: For example, Normal distribution: dnorm, pnorm, qnorm, rnorm Log-normal distribution: dlnorm, plnorm, qlnorm, rlnorm Poisson: dpois, ppois, qpois, rpois First we’ll play around with the normal distribution because we know what the answers should be. Then we’ll move onto distributions we may be less familiar with: First, lets draw a couple of random values from the standard normal. We can take 100 random draws from the Standard Normal N(0,1) using the R function ‘rnorm’. data&lt;-rnorm(100,mean=0,sd=1) head(data) ## [1] 0.7745610 -1.0104007 -0.7723421 0.4762443 -1.3725375 0.6413420 Note that you could have left off the “mean” and “sd” since R knows the order of inputs, that is you could simply write head(rnorm(100,0,1)) ## [1] 1.3683642 0.9964899 1.6813507 0.7522076 1.8147232 -1.2430761 or even head(rnorm(100)) ## [1] 0.4173142 -0.1352296 0.8432931 2.0125309 -0.1155821 0.7242720 since mean=0, sd=1 is the default. Until you are 100% comfortable with R, its better to leave all the options spelled out. Make a histogram of data hist(data) Play around with the hist command using different numbers of ‘breaks’ or try leaving that option off altogether. You will get a sense for how many breaks you need for the histogram to “look right” but I prefer to use more breaks than R defaults to. Also, compare this last plot with this one: hist(data,freq=FALSE) To really play around with these distributions, lets combine these commands into a single command: hist(rnorm(100,mean=0,sd=1),col=&quot;plum4&quot;) Play around with different means and sd and convince yourself that ‘rnorm’ really does work. You can look at the graphics options by doing ?hist and you can explore the list of named colors by typing colors() What happens if you add the flag “plot=F”? hist(rnorm(1000,mean=0,sd=1),plot=F) ## $breaks ## [1] -3.5 -3.0 -2.5 -2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 ## ## $counts ## [1] 3 4 21 39 98 158 191 169 157 95 44 17 4 ## ## $density ## [1] 0.006 0.008 0.042 0.078 0.196 0.316 0.382 0.338 0.314 0.190 0.088 ## [12] 0.034 0.008 ## ## $mids ## [1] -3.25 -2.75 -2.25 -1.75 -1.25 -0.75 -0.25 0.25 0.75 1.25 1.75 ## [12] 2.25 2.75 ## ## $xname ## [1] &quot;rnorm(1000, mean = 0, sd = 1)&quot; ## ## $equidist ## [1] TRUE ## ## attr(,&quot;class&quot;) ## [1] &quot;histogram&quot; Note that you can assign that to a variable and then use those results later in a calculation or another plot. Next, lets play around with pnorm quantiles&lt;-seq(-3.5,3.5,0.01) #These are the quantiles density&lt;-dnorm(quantiles,mean=0,sd=1) #dnorm gives the pdf for a given quantile plot(quantiles,density,type=&quot;l&quot;,ylab=&quot;Probability density&quot;) This is the probability density function for the standard normal. (We are getting a little ahead of ourselves because we won’t discuss graphics until next week, but this syntax is fairly straightforward.) Let’s use the same vector ‘quantiles’ and try the function pnorm: cumulative&lt;-pnorm(quantiles,mean=0,sd=1) plot(quantiles,cumulative,type=&quot;l&quot;,ylab=&quot;Probability&quot;) This gives us the cumulative distribution function! Finally, lets look at qnorm probability&lt;-seq(0,1,0.001) quantiles&lt;-qnorm(probability,mean=0,sd=1) plot(probability,quantiles,type=&quot;l&quot;,ylab=&quot;Quantiles&quot;) This plots the quantiles for each probability between 0 and 1. In other words, what value \\(Y^{*}\\) is associated with the cumulative probability of \\(X^{*}\\). Lets make sure this makes sense by plotting on top of this line another representing the quantiles for a normal distribution with smaller variance quantiles2&lt;- qnorm(probability,mean=0,sd=0.2) plot(probability,quantiles,type=&quot;l&quot;,ylab=&quot;Quantiles&quot;) lines(probability,quantiles2,col=&quot;red&quot;) Notice that because the variance of the new distribution is smaller, you get from a cumulative probability of 0 to 1 over a smaller range of values. Let’s try some discrete distributions next: count&lt;-rpois(500,lambda=3) table(count) ## count ## 0 1 2 3 4 5 6 7 8 11 ## 25 87 116 104 86 44 22 11 4 1 mean(count) ## [1] 2.894 var(count) ## [1] 2.972709 6.3 Standard deviation vs. Standard error Many people struggle with the distinction between the standard deviation of a sample (or a population), and the standard error of the mean of the sample (or population). The standard deviation is a measure of the average spread of the data. Since the standard deviation is a measure of the average spread of the data, adding more data does not appreciably change the standard deviation. (Make sure this makes sense!) The standard error can be understood as follows: If you repeated your experiment many times, and calculated the mean of each of the samples (one sample from each “experiment”), the standard deviation of the means would represent the uncertainty in the estimate of the mean coming from any one sample. The standard deviation of those means is called the standard error of the mean (or SEM). The SEM decreases as the size of each sample increases because each sample is now more representative of the underlying distribution. More precisely, the standard error is the standard deviation of the sampling distribution of a statistic. Standard errors can be calculated for any statistic. For example, if we fit a Beta(\\(\\alpha\\),\\(\\beta\\)) distribution to a dataset, we want to estimate the parameter values \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) AND thier standard errors, which we might denote s.e.\\(_{\\hat{\\alpha}}\\) and s.e.\\(_{\\hat{\\beta}}\\). We can use the Poisson distribution to illustrate the difference between a standard deviation of a distribution and the standard deviation of the mean: First we want to plot the distribution for three different sample sizes: par(mfrow=c(3,1)) #Use ?par to see what this command does - more on this later sample1&lt;-rpois(1000,lambda=3) sample2&lt;-rpois(10000,lambda=3) sample3&lt;-rpois(100000,lambda=3) hist(sample1) hist(sample2) hist(sample3) sd(sample1) ## [1] 1.76407 sd(sample2) ## [1] 1.721567 sd(sample3) ## [1] 1.732815 Notice that the standard deviation has not appreciably changed as we have increased the sample size. Now lets run some code to calculate the standard error of the mean: sample.size&lt;-1000 means&lt;-c() for (i in 1:2000) { means&lt;-c(means,mean(rpois(sample.size,lambda=3))) } hist(means) s.e.1&lt;-sqrt(var(rpois(sample.size,lambda=3))/sample.size) s.e.2&lt;-sd(means) s.e.1 ## [1] 0.05304493 s.e.2 ## [1] 0.05413985 Note that the number of experiments I looped through (2000 in this case) is not relevant. It just has to be big enough that you get a sense of what the distribution of means looks like. Now go back and modify the code so that sample.size=10000 - how does that change the result? On Tuesday we discussed the probability mass function for the binomial. While an individual flip of the coin can be thought of as a success/failure, the binomial is answering the question “How many succesess do I expect if I try n times.” We can plot this for varying numbers of trials assuming p=0.5 par(mfrow=c(3,3)) p=0.5 plot(seq(0,10),dbinom(x=seq(0,10),size=1,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=1&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),1*p,1*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=2,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=2&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),2*p,2*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=3,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=3&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),3*p,3*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=4,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=4&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),4*p,4*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=5,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=5&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),5*p,5*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=6,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=6&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),6*p,6*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=7,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=7&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),7*p,7*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=8,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=8&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),8*p,8*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=9,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=9&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),9*p,9*p*(1-p)),col=&quot;red&quot;,lwd=2) and we can see how this might change for p=0.1 par(mfrow=c(3,3)) p=0.1 plot(seq(0,10),dbinom(x=seq(0,10),size=1,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=1&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),1*p,1*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=2,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=2&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),2*p,2*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=3,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=3&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),3*p,3*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=4,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=4&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),4*p,4*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=5,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=5&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),5*p,5*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=6,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=6&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),6*p,6*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=7,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=7&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),7*p,7*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=8,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=8&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),8*p,8*p*(1-p)),col=&quot;red&quot;,lwd=2) plot(seq(0,10),dbinom(x=seq(0,10),size=9,prob=p),typ=&quot;h&quot;,lwd=5,xlab=&quot;# of successes&quot;,ylab=&quot;&quot;,main=&quot;n=9&quot;,ylim=c(0,0.6)) lines(seq(0,10,0.1),dnorm(seq(0,10,0.1),9*p,9*p*(1-p)),col=&quot;red&quot;,lwd=2) If you look back at our notes from Tuesday, we see that the gamma and the Poisson distributions look quite similar (ignoring that one is discrete and the other continuous). We can use R to show us the differences are: First we will draw from the Poisson distribution, then we will use R’s very handy function ‘fitdistr’ to fit the gamma distribution to that data and compare. We haven’t yet covered HOW this function works, but for now let’s just take for granted that this function is able to find the parameter estimates that will give you the best fit to your data. First, install the library ‘MASS’. library(MASS) #this loads the library into the workspace sample.pois&lt;-rpois(1000,lambda=20) fit&lt;-fitdistr(sample.pois,&quot;gamma&quot;,start=list(shape=20,scale=1)) ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced ## Warning in densfun(x, parm[1], parm[2], ...): NaNs produced fit ## shape scale ## 19.24087854 1.03290154 ## ( 0.85293487) ( 0.04638881) (Sometimes you get a warnings message about NAs when using fitdistr. The best explanation I can find says that this means R “encountered some difficulties during fitting”. I can find no difference in the fits when you get the warning and when you don’t, and the same sample.pois will sometimes give a warning and sometimes not, so it appears independent of the data itself. Do not ignore warnings() in R but don’t be paralized by them, especially in a context where R is searching parameter space during an optimization. Be sure to search around for an explanation and make sure you are confident that R is still giving reasonable answers.) Notice that we are fitting a gamma distribution to this data, and we specify what distribution we want to fit using the name of the distribution in quotes. Remember that rgamma can take as inputs shape,scale or shape,rate=1/scale. I am using scale as the input because it is consistent with the way I introduced the gamma distribution in class. Be aware that some people will use rate and some will use scale and you always have to check. When we print the object fit, we get the estimates and the standard errors, but at first it isn’t obvious how to extract the estimates (and errors) so we can use them in other calculations. We start by using the function names to “get inside” this object and see what it is made up of. names(fit) ## [1] &quot;estimate&quot; &quot;sd&quot; &quot;vcov&quot; &quot;loglik&quot; &quot;n&quot; We now look at fit$estimate ## shape scale ## 19.240879 1.032902 and notice that we can pull out the two estimates as fit$estimate[1] ## shape ## 19.24088 fit$estimate[2] ## scale ## 1.032902 Now we want to plot the data, and the best fit line: x.vals&lt;- seq(from=5,to=40,by=1) hist(sample.pois,breaks=x.vals) lines(x.vals,dgamma(x.vals,shape=fit$estimate[1],scale=fit$estimate[2])*1000,col=&quot;blue&quot;) Two things to note here: 1. I created x.vals just as a mechanism for plotting a relatively smooth line for the best-fit distribution 2. I multiplied for 1000 because I had originally drawn 1000 values, and this puts my best-fit line on the same scale as the histogram. NOTE: We can guess at starting values by making sure the mean and variance of the gamma match the mean and variance of the data. This method is an example of “moment matching”. In other words, we take two distributions and get a close fit between them by requiring that they have the same mean and, if possible, the same variance. We can see that if we generate data from a Poisson, it can be fit very well by a gamma distribution. It can also be fit quite well by a Normal distribution hist(sample.pois,breaks=x.vals) fit2&lt;-fitdistr(sample.pois,&quot;normal&quot;) lines(x.vals,dnorm(x.vals,mean=fit2$estimate[1],sd=fit2$estimate[2])*1000,col=&quot;red&quot;) So we have shown that given certain parameters, a Gamma distribution can approximate a Poisson, and we have shown that the Normal can approximate the draws from a Poisson distribution. This latter fact shouldn’t come as a surprise because \\[ \\lim_{\\lambda\\rightarrow\\infty} Pois(\\lambda)\\rightarrow N(\\lambda,\\lambda) \\] The function ‘fitdistr’ is one of the MOST HANDY functions that exist for probabilities in R. Notice that ‘fitdistr’ also gives the estimated standard errors in parentheses. The next few weeks will be dedicated to learning more about the interpretation and creation of these standard errors or, equivalently, confidence intervals. Finally, I want to introduce the idea of a QQ-plot. A QQ-plot has the quantiles of two distributions plotted against one another. If the two distributions are quite similar, the QQ-plot will fall roughly on the 1:1 line. We can compare the Poisson data to the gamma distribution fit using a QQ-plot of the original Poisson sample and an equally sized sample from our best-fit gamma distribution. qqplot(x=sample.pois, y=rgamma(1000,shape=fit$estimate[1],scale=fit$estimate[2])) abline(a=0,b=1,col=&quot;red&quot;,lwd=2) "]
]
