<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Week 2 Lecture | Biometry Lecture and Lab Notes</title>
  <meta name="description" content="3 Week 2 Lecture | Biometry Lecture and Lab Notes" />
  <meta name="generator" content="bookdown 0.14 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Week 2 Lecture | Biometry Lecture and Lab Notes" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Week 2 Lecture | Biometry Lecture and Lab Notes" />
  
  
  

<meta name="author" content="Heather Lynch" />


<meta name="date" content="2019-11-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="week-1-lab-handout.html"/>
<link rel="next" href="week-2-lab.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Biometry Lecture and Lab Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="week-1-lecture.html"><a href="week-1-lecture.html"><i class="fa fa-check"></i><b>1</b> Week 1 Lecture</a><ul>
<li class="chapter" data-level="1.1" data-path="week-1-lecture.html"><a href="week-1-lecture.html#reading-material"><i class="fa fa-check"></i><b>1.1</b> Reading Material</a></li>
<li class="chapter" data-level="1.2" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-outline"><i class="fa fa-check"></i><b>1.2</b> Basic Outline</a></li>
<li class="chapter" data-level="1.3" data-path="week-1-lecture.html"><a href="week-1-lecture.html#todays-agenda"><i class="fa fa-check"></i><b>1.3</b> Today’s Agenda</a></li>
<li class="chapter" data-level="1.4" data-path="week-1-lecture.html"><a href="week-1-lecture.html#basic-probability-theory"><i class="fa fa-check"></i><b>1.4</b> Basic Probability Theory</a></li>
<li class="chapter" data-level="1.5" data-path="week-1-lecture.html"><a href="week-1-lecture.html#multiple-events"><i class="fa fa-check"></i><b>1.5</b> Multiple events</a></li>
<li class="chapter" data-level="1.6" data-path="week-1-lecture.html"><a href="week-1-lecture.html#conditionals"><i class="fa fa-check"></i><b>1.6</b> Conditionals</a></li>
<li class="chapter" data-level="1.7" data-path="week-1-lecture.html"><a href="week-1-lecture.html#bayes-theorem"><i class="fa fa-check"></i><b>1.7</b> Bayes Theorem</a></li>
<li class="chapter" data-level="1.8" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-few-foundational-ideas"><i class="fa fa-check"></i><b>1.8</b> A few foundational ideas</a></li>
<li class="chapter" data-level="1.9" data-path="week-1-lecture.html"><a href="week-1-lecture.html#overview-of-univariate-distributions"><i class="fa fa-check"></i><b>1.9</b> Overview of Univariate Distributions</a></li>
<li class="chapter" data-level="1.10" data-path="week-1-lecture.html"><a href="week-1-lecture.html#what-can-you-ask-of-a-distribution"><i class="fa fa-check"></i><b>1.10</b> What can you ask of a distribution?</a></li>
<li class="chapter" data-level="1.11" data-path="week-1-lecture.html"><a href="week-1-lecture.html#a-brief-introduction-to-scientific-method"><i class="fa fa-check"></i><b>1.11</b> A Brief Introduction to Scientific Method</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="week-1-lab-handout.html"><a href="week-1-lab-handout.html"><i class="fa fa-check"></i><b>2</b> Week 1 Lab Handout</a><ul>
<li class="chapter" data-level="2.1" data-path="week-1-lab-handout.html"><a href="week-1-lab-handout.html#using-r-like-a-calculator"><i class="fa fa-check"></i><b>2.1</b> Using R like a calculator</a></li>
<li class="chapter" data-level="2.2" data-path="week-1-lab-handout.html"><a href="week-1-lab-handout.html#the-basic-data-structures-in-r"><i class="fa fa-check"></i><b>2.2</b> The basic data structures in R</a></li>
<li class="chapter" data-level="2.3" data-path="week-1-lab-handout.html"><a href="week-1-lab-handout.html#writing-functions-in-r"><i class="fa fa-check"></i><b>2.3</b> Writing functions in R</a></li>
<li class="chapter" data-level="2.4" data-path="week-1-lab-handout.html"><a href="week-1-lab-handout.html#writing-loops-and-ifelse"><i class="fa fa-check"></i><b>2.4</b> Writing loops and if/else</a></li>
<li class="chapter" data-level="2.5" data-path="week-1-lab-handout.html"><a href="week-1-lab-handout.html#a-short-diversion-bias-in-estimators"><i class="fa fa-check"></i><b>2.5</b> (A short diversion) Bias in estimators</a></li>
<li class="chapter" data-level="2.6" data-path="week-1-lab-handout.html"><a href="week-1-lab-handout.html#lesson-6-some-practice-writing-r-code"><i class="fa fa-check"></i><b>2.6</b> Lesson #6: Some practice writing R code</a></li>
<li class="chapter" data-level="2.7" data-path="week-1-lab-handout.html"><a href="week-1-lab-handout.html#a-few-final-notes"><i class="fa fa-check"></i><b>2.7</b> A few final notes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="week-2-lecture.html"><a href="week-2-lecture.html"><i class="fa fa-check"></i><b>3</b> Week 2 Lecture</a><ul>
<li class="chapter" data-level="3.1" data-path="week-2-lecture.html"><a href="week-2-lecture.html#hypothesis-testing-and-p-values"><i class="fa fa-check"></i><b>3.1</b> Hypothesis testing and p-values</a></li>
<li class="chapter" data-level="3.2" data-path="week-2-lecture.html"><a href="week-2-lecture.html#permutation-tests"><i class="fa fa-check"></i><b>3.2</b> Permutation tests</a></li>
<li class="chapter" data-level="3.3" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parameter-estimation"><i class="fa fa-check"></i><b>3.3</b> Parameter estimation</a></li>
<li class="chapter" data-level="3.4" data-path="week-2-lecture.html"><a href="week-2-lecture.html#method-1-non-parametric-bootstrap"><i class="fa fa-check"></i><b>3.4</b> Method #1: Non-parametric bootstrap</a></li>
<li class="chapter" data-level="3.5" data-path="week-2-lecture.html"><a href="week-2-lecture.html#parametric-bootstrap"><i class="fa fa-check"></i><b>3.5</b> Parametric bootstrap</a></li>
<li class="chapter" data-level="3.6" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife"><i class="fa fa-check"></i><b>3.6</b> Jackknife</a></li>
<li class="chapter" data-level="3.7" data-path="week-2-lecture.html"><a href="week-2-lecture.html#jackknife-after-bootstrap"><i class="fa fa-check"></i><b>3.7</b> Jackknife-after-bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="week-2-lab.html"><a href="week-2-lab.html"><i class="fa fa-check"></i><b>4</b> Week 2 Lab</a><ul>
<li class="chapter" data-level="4.1" data-path="week-2-lab.html"><a href="week-2-lab.html#basics-of-bootstrap-and-jackknife"><i class="fa fa-check"></i><b>4.1</b> Basics of bootstrap and jackknife</a></li>
<li class="chapter" data-level="4.2" data-path="week-2-lab.html"><a href="week-2-lab.html#calculating-bias-and-standard-error"><i class="fa fa-check"></i><b>4.2</b> Calculating bias and standard error</a></li>
<li class="chapter" data-level="4.3" data-path="week-2-lab.html"><a href="week-2-lab.html#parametric-bootstrap-1"><i class="fa fa-check"></i><b>4.3</b> Parametric bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="week-3-lecture.html"><a href="week-3-lecture.html"><i class="fa fa-check"></i><b>5</b> Week 3 Lecture</a><ul>
<li class="chapter" data-level="5.1" data-path="week-3-lecture.html"><a href="week-3-lecture.html#overview-of-probability-distributions"><i class="fa fa-check"></i><b>5.1</b> Overview of probability distributions</a></li>
<li class="chapter" data-level="5.2" data-path="week-3-lecture.html"><a href="week-3-lecture.html#normal-gaussian-distribution"><i class="fa fa-check"></i><b>5.2</b> Normal (Gaussian) Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="week-3-lecture.html"><a href="week-3-lecture.html#standard-normal-distribution"><i class="fa fa-check"></i><b>5.3</b> Standard Normal Distribution</a></li>
<li class="chapter" data-level="5.4" data-path="week-3-lecture.html"><a href="week-3-lecture.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.4</b> Log-Normal Distribution</a></li>
<li class="chapter" data-level="5.5" data-path="week-3-lecture.html"><a href="week-3-lecture.html#intermission-central-limit-theorem"><i class="fa fa-check"></i><b>5.5</b> Intermission: Central Limit Theorem</a></li>
<li class="chapter" data-level="5.6" data-path="week-3-lecture.html"><a href="week-3-lecture.html#poisson-distribution"><i class="fa fa-check"></i><b>5.6</b> Poisson Distribution</a></li>
<li class="chapter" data-level="5.7" data-path="week-3-lecture.html"><a href="week-3-lecture.html#binomial-distribution"><i class="fa fa-check"></i><b>5.7</b> Binomial Distribution</a></li>
<li class="chapter" data-level="5.8" data-path="week-3-lecture.html"><a href="week-3-lecture.html#beta-distribution"><i class="fa fa-check"></i><b>5.8</b> Beta Distribution</a></li>
<li class="chapter" data-level="5.9" data-path="week-3-lecture.html"><a href="week-3-lecture.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9</b> Gamma Distribution</a></li>
<li class="chapter" data-level="5.10" data-path="week-3-lecture.html"><a href="week-3-lecture.html#some-additional-notes"><i class="fa fa-check"></i><b>5.10</b> Some additional notes:</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="week-3-lab-handout.html"><a href="week-3-lab-handout.html"><i class="fa fa-check"></i><b>6</b> Week 3 Lab Handout</a><ul>
<li class="chapter" data-level="6.1" data-path="week-3-lab-handout.html"><a href="week-3-lab-handout.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>6.1</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="6.2" data-path="week-3-lab-handout.html"><a href="week-3-lab-handout.html#exploring-the-univariate-distributions-with-r"><i class="fa fa-check"></i><b>6.2</b> Exploring the univariate distributions with R</a></li>
<li class="chapter" data-level="6.3" data-path="week-3-lab-handout.html"><a href="week-3-lab-handout.html#standard-deviation-vs.standard-error"><i class="fa fa-check"></i><b>6.3</b> Standard deviation vs. Standard error</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Biometry Lecture and Lab Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="week-2-lecture" class="section level1">
<h1><span class="header-section-number">3</span> Week 2 Lecture</h1>
<div id="hypothesis-testing-and-p-values" class="section level2">
<h2><span class="header-section-number">3.1</span> Hypothesis testing and p-values</h2>
<p>There are few topics in statistics more controversial than the various philosophies behind null hypothesis testing. Over the next two weeks we will learn about the two paradigms (Fisher vs. Neyman-Pearson), the hybrid approach mostly commonly used in ecology, and criticisms of the whole enterprise. Bayesian statistics takes an entirely different approach than either Fisher or Neyman-Pearson, and the Bayesian approach resolves many of the inconsistencies involved with frequentist statistics, but at the expense of increased computation (and the use of prior information…).</p>
<p>We frame decision-making in terms of a null and an alternative hypothesis.</p>
<p><span class="math inline">\(H_{0}\)</span> vs. <span class="math inline">\(H_{A}\)</span></p>
<p>To take Karl Popper’s famous example:</p>
<p><span class="math inline">\(H_{0}\)</span>: There are no vultures in the park.</p>
<p><span class="math inline">\(H_{A}\)</span>: There are vultures in the park.</p>
<p>Note that the data may reject the null hypothesis (for example, finding vultures in the park), or the data may fail to reject the null hypothesis, but it can never prove the null hypothesis. We cannot prove there are no vultures in the park. We can only say that we were not able to find any vultures in the park, and therefore cannot reject the null hypothesis.</p>
<p>Fisher’s original context for developing significance testing was agricultural experiments that could be easily replicated. Fisher’s threshold of 0.05 was an arbitrary threshold for an effect to be considered worthy of continued experimentation. Any experiment that failed to reach this threshold would not be pursued. Experiments that gave “significant” results would be subject to additional experiments. These additional experiments may prove the original effect to be a fluke (and experiments would cease) or the additional experiments may provide confirmatory evidence that the effect was real.</p>
<p>Null hypothesis testing (as I will teach it) involves 6 steps.</p>
<p><strong>Step #1</strong>: Specify a null hypothesis <span class="math inline">\(H_{0}\)</span> (Note that I do not include specification of the alternative hypothesis <span class="math inline">\(H_{A}\)</span> here. While the alternative hypothesis is useful as a mental construct, the basic approach deals only with <span class="math inline">\(H_{0}\)</span> and does not require a <span class="math inline">\(H_{A}\)</span>.</p>
<p><strong>Step #2</strong>: Specific an appropriate test statistic T. A test statistic is some summary of your data that pertains to the null hypothesis. For testing simple hypotheses, there are test statistics known to be ideal in certain situations. However, even in these simple cases, there are other test statistics that could be used. In more complex situations, YOU will have to determine the most appropriate test statistic.</p>
<p>generic=T=f(X)</p>
<p>specific <span class="math inline">\(T^{*}\)</span>=T(<span class="math inline">\(X_{1}\)</span>,<span class="math inline">\(X_{2}\)</span>,…,<span class="math inline">\(X_{n}\)</span>)</p>
<p>We will introduce many more test statistics in the weeks to come but a simple example of a test statistic would be the use of <span class="math inline">\(\bar{X}\)</span>̅ (the mean of your sample) as a measure of the mean of the normally distributed population from which the sample was derived.</p>
<p><strong>Step #3</strong>: Determine the distribution of the test statistic under the null hypothesis <span class="math inline">\(H_{0}\)</span>. A test statistic is a statistical quantity that has a statistical distribution.</p>
<p><span class="math inline">\(f(T│H_{0})\)</span></p>
<p>Notice that this is the probability of obtaining the test statistic T GIVEN the null distribution, it is NOT <span class="math inline">\(f(H_{0}│T)\)</span>.</p>
<p>The test statistic and its distribution under the null hypothesis is the statistical test.</p>
<p>Test = Test statistic + Distribution of test statistic under <span class="math inline">\(H_{0}\)</span></p>
<p><strong>Step #4</strong>: Collect data and calculate T* Collect data by taking random samples from your population and calculate the test statistic from the sample data.</p>
<p><strong>Step #5</strong>: Calculate a p-value</p>
<p>Calculate the probability that you would get a value for the test statistic as large or larger than that obtained with the data under the null hypothesis</p>
<p><span class="math inline">\(P(T^{*}│H_{0})\)</span>=p-value</p>
<p><strong>Step #6</strong>: Interpret the p-value</p>
<p>Use the p-value to determine whether to reject the null hypothesis (or, alternatively, to decide that the null hypothesis cannot be rejected)</p>
<p>These steps apply for both parametric and non-parametric statistics. Here we are introducing hypothesis testing through the lens of randomization procedures, but the same steps will be used again when we get into statistics involving parametric distributions (i.e. statistical distributions of known form and described by a finite number of parameters) and their properties. As you will see in a few weeks, most standard statistical tests involve a test statistic with a known distribution under the null hypothesis; here the distribution under the null hypothesis needs to be generated by randomization (randomization test). (We are starting with the randomization-based procedures because there is no math involved and it is more intuitive.)</p>
<p>The basic idea underlying all statistical tests: What is the probability that I would get a test statistic as large or larger (as produced by the data) if the null hypothesis was true (this is the “p-value”). To answer this question we need (1) a test statistic and (2) a distribution under the null hypothesis.</p>
<p>p-value = <span class="math inline">\(P(data|H_{0})\)</span></p>
<p>Remember – the p-value is a statement about the probability of getting your data if the null hypothesis were true. It is NOT a statement about the probability that the null hypothesis is true. This logic can go wrong!!</p>
<p>Example: If a person is an American, he is probably not a member of Congress. This person is a member of Congress. Therefore he is probably not an American.</p>
<p>Let’s draw a null distribution. In order to interpret the statistical test, we need to know whether we want a one-tailed test or a two-tailed test. In a one-tailed test, we would reject the null hypothesis only if the test statistic is larger than expected under in the null in one direction (5<span class="math inline">\(%\)</span> in one tail). In a two-tailed test, we would reject the null if the test statistic is larger in either direction (2.5<span class="math inline">\(%\)</span> in both tails).</p>
<p>EXAMPLE: Let’s say I’m looking at the change in auto accident mortalities after a ban is enacted on driving while texting. We would expect that auto accident mortality would decrease after a ban on texting while driving. Let’s say, for arguments sake, that our test statistic T is the change in accident deaths</p>
<p><span class="math inline">\(H_{0}\)</span>: T=0 (no change in deaths)</p>
<p><span class="math inline">\(H_{A}\)</span>: T&lt;0 (decline in deaths)</p>
<p>Another possible formulation of the null and alternative hypotheses is</p>
<p><span class="math inline">\(H_{0}\)</span>: T=0 (no change in deaths)</p>
<p><span class="math inline">\(H_{A}\)</span>: T <span class="math inline">\(\neq\)</span> 0 (increase or decline in deaths)</p>
<p>Why does it matter?</p>
<p>Consider the first case. To reject the null hypothesis, you would have to show that the measured decline T^*was so large as to be very unlikely to have occurred by random chance assuming there was no true change in death rate. Therefore, you would require</p>
<p><span class="math inline">\(P(T \geq T^{*}│H_{0})&lt;0.05\)</span></p>
<p>to be true for you to decide to reject the null hypothesis. This is a one-tailed test.</p>
<p>Consider the second case. To reject the null hypothesis, you would accept values of <span class="math inline">\(T^{*}\)</span> as significant if they were either very large or very small, and would divide the 5% critical region between the two tails</p>
<p><span class="math inline">\(P(T \geq T^{*}│H_{0})&lt;0.025\)</span></p>
<p><span class="math inline">\(P(T \leq T^{*}│H_{0})&lt;0.025\)</span></p>
<p>Notice that it now becomes a <em>more stringent test</em>. If <span class="math inline">\(T^{*}\)</span> is large, it now has to be even larger to qualify as “significant”. This is a two-tailed test.</p>
<p>TWO KEY POINTS:</p>
<ol style="list-style-type: decimal">
<li>If you are using a one-tailed test, you have to be willing to accept a result that is opposite in sign of what was expected as being PURELY BY CHANCE!! In other words, if traffic deaths went UP after the texting ban, you would have to be willing to accept that that was by pure chance and you would then fail to reject the null hypothesis of NO CHANGE. This is in fact what happened, by the way: Texting bans actually increase traffic deaths – WHY?</li>
<li>Before using the more “lenient” one-tailed test, make sure you really believe that results opposite to what you expect are only random</li>
</ol>
<p>You cannot do a one-tailed test, find the answer to have the wrong sign and then do a two-tailed test. While probably quite common, this is not statistically valid. You cannot use the data to generate the test! Not all tests are created equal!! Tests differ in their power to detect differences, and their efficiency. The balance between power and efficiency depends on the specific situation; we will discuss this more next week.</p>
<p>We are going to introduce the idea of hypothesis testing through the practice of permutation tests, because it allows us to get into the flow of testing hypotheses without the burden of a lot of complicated mathematics.</p>
</div>
<div id="permutation-tests" class="section level2">
<h2><span class="header-section-number">3.2</span> Permutation tests</h2>
<p>Let’s say we have two random samples drawn from possibly different probability distributions F and G,</p>
<p><span class="math inline">\(F \rightarrow z=\{z_{1},z_{2},...,z_{n}\}\)</span></p>
<p><span class="math inline">\(G \rightarrow y=\{y_{1},y_{2},...,y_{m}\}\)</span></p>
<p>Having observed z and y, we wish to test the null hypothesis <span class="math inline">\(H_{0}\)</span> of no difference between F and G, <span class="math inline">\(H_{0}:F=G\)</span>. Note that the equality F=G means that the two distributions are exactly the same across their entire distribution, not just that their means are the same. If <span class="math inline">\(H_{0}\)</span> is true, than there is no probabilistic difference between drawing random values from F and drawing random values from G.</p>
<p><em>What are some possible test statistics that we might use in this case?</em></p>
<p>There are many test statistics that we could use to test this null hypothesis but lets use the difference in means as the test statistic. If there is a large difference in their means, than we can probably reject the null hypothesis that they represent the same underlying distribution.</p>
<p><span class="math inline">\(T=E[F]-E[G]=\bar{x}-\bar{y}\)</span></p>
<p>The way to do this is to lump all the data together with their “label” (and here I will use the example from the problem set and test whether males and females have different blood sugar distributions), and to randomly permute the labels so that the gender identity of the data points is randomized. In other words, if there are 10 males and 15 females, then you would randomly select 10 of the blood sugar levels and label them “male” and the remaining 15 would be labeled “female”.</p>
<p>NOTE: We are not sampling with replacement here. We are simply permuting the gender labels to “erase” the correlation between gender and blood sugar.</p>
<p>We then calculate the mean of the “fake male” group and the mean of the “fake female” group and take the difference. That is the result of ONE permutation. If we do that many many times (say, 10000 times) then the distribution of those differences reflects the distribution under the null hypothesis of no correlation between gender and blood sugar.</p>
<p>We will do an example like this in the problem set.</p>
</div>
<div id="parameter-estimation" class="section level2">
<h2><span class="header-section-number">3.3</span> Parameter estimation</h2>
<p>Hypothesis testing is a bit like the game 50-questions (Are you red? Are you blue? Each question is a null hypothesis to be rejected…).</p>
<p>Parameter estimation appears at first glance more direct, it just asks “What are you?”, and in doing so it estimates the value of a parameter (mean growth rate of a fish population, for example) and provides a measure of uncertainty about that estimate.</p>
<p>As a reminder, estimators are tools that produce estimates of population statistics from sample statistics.</p>
<p>The basic outline of “statistical inference”: Data = sample <span class="math inline">\(\rightarrow\)</span> sample statistics <span class="math inline">\(\rightarrow\)</span> ESTIMATOR <span class="math inline">\(\rightarrow\)</span> population parameters</p>
<p>We generally use the word “statistic” when discussing the data, and “parameter” when discussing the underlying distribution.</p>
<p>An “estimator” or “point estimate” is a statistic (that is, a function of the data) that is used to infer the value of an unknown parameter in a statistical model. If the parameter is denoted <span class="math inline">\(\theta\)</span> then the estimator is typically written by adding a “hat” over the symbol: <span class="math inline">\(\hat{\theta}\)</span>. Being a function of the data, the estimator is itself a random variable; a particular realization of this random variable is called the “estimate”. Sometimes the words “estimator” and “estimate” are used interchangeably, but I will try and be consistent in using the word “estimator” for the function in the generic, and the word “estimate” for the result of applying that function to the data at hand.</p>
<p>Example:</p>
<p><span class="math display">\[
X \sim N(\mu,\sigma^{2})
\]</span></p>
<p>We define the “estimator” for <span class="math inline">\(\mu\)</span> as</p>
<p><span class="math display">\[
\frac{1}{n}\sum_{i=1}^{n}X_{i} 
\]</span></p>
<p>Therefore, the “estimate” <span class="math inline">\(\hat{\mu}\)</span> is</p>
<p><span class="math display">\[
\hat{\mu}=\bar{X} =\frac{1}{n}\sum_{i=1}^{n}X_{i} 
\]</span></p>
<p>Estimators are IMPERFECT tools.</p>
<ol style="list-style-type: decimal">
<li><p>Bias: As <span class="math inline">\(n \rightarrow \infty\)</span>, sample statistic does not converge to the population parameter</p></li>
<li><p>Standard error: Each individual estimate may be too low or too high from the true value (this can occur even if the long run average value is correct, i.e. unbiased)</p></li>
</ol>
<p>Why are estimators associated with a standard error? If you were to do your experiment all over again, say 1000 times, the value of your estimate would be different each time. Your 1000 estimates would have a statistical distribution with some spread, and the spread of these 1000 estimates is quantified by the standard error.</p>
<p>How do we estimate the bias and variance (related to standard error) of an estimator?</p>
<p>While there are other methods that we will discuss in a few weeks, now we are going to introduce the idea through two non-parametric approaches: bootstrap and jackknife.</p>
<p>First we need to stop and discuss what it means to sample from an empirical distribution.</p>
<p>Let’s say I have a bunch of lotto balls in an urn</p>
<p><span class="math inline">\(X=\{X_{1},X_{2},X_{3},...,X_{n}\}\)</span></p>
<p>and I want to draw sets of 5 lotto numbers from that urn. I can sample with replacement or without replacement. If you sample with replacement, we may get some numbers more than once. It also means that if you draw n balls out of an urn with n numbers, there are some numbers you will never draw.</p>
<p><strong>STOP: Do you understand sample-with-replacement and sample-without-replacement?</strong></p>
</div>
<div id="method-1-non-parametric-bootstrap" class="section level2">
<h2><span class="header-section-number">3.4</span> Method #1: Non-parametric bootstrap</h2>
<p>The basic idea behind bootstrap sampling is that even if we don’t know what the distribution is that underlies the data, we can “pull ourselves up by our bootstraps” and generate the distribution by resampling WITH REPLACEMENT from the data itself.</p>
<p>Say we have original data drawn from an unknown distribution G</p>
<p><span class="math inline">\(X=\{X_{1},X_{2},X_{3},...,X_{n}\}\)</span></p>
<p><span class="math display">\[
X \sim G()
\]</span></p>
<p>We don’t know the underlying distribution, but we can substitute the empirical distribution <span class="math inline">\(\hat{G}\)</span> which is defined by <span class="math inline">\(\{X_{1},X_{2},X_{3},...,X_{n}\}\)</span>. In other words, we model the underlying “true” unknown distribution as a multinomial where every value in X is given a probability <span class="math inline">\(\frac{1}{n}\)</span> of occurring.</p>
<p>Let’s say we want to compute a statistic of the probability distribution <span class="math inline">\(\theta=f(G)\)</span>, which could be the mean or the median or the standard error of the standard deviation (anything at all!!).</p>
<p>BTW: <span class="math inline">\(\theta\)</span> is analogous to the test statistic T used for hypothesis testing, and it will be used in the same way. However, I will use the symbol <span class="math inline">\(\theta\)</span> to be consistent with the Efron and Tibshirani and other literature on the bootstrap.</p>
<p>The “plug-in” principle states that for every parameter of the underlying distribution, we can estimate that function by simply plugging in the empirical distribution</p>
<p><span class="math display">\[
\hat{\theta}=f(\hat{G})=f(X)
\]</span></p>
<p>This is exactly what we would do intuitively. If we have a bunch of numbers and we want to know the mean of the distribution from whence they came, we would use as the best estimate the mean of those numbers. The “plug-in” principle simply formalizes the idea that these summary statistics can be used to make inference about the generating distribution.</p>
<p>In the development to follow, we will assume that we have NO other information about a distribution other than a single sample from that distribution.</p>
<p>Summary statistics are easy enough to compute, but we don’t have any way of knowing how accurate those summary statistics might be. The bootstrap gives us a way to calculate the accuracy of our summary statistics <span class="math inline">\(\hat{\theta}\)</span>.</p>
<p>The bootstrap works NO MATTER HOW COMPLICATED THE FUNCTION, IT IS COMPLETELY AUTOMATIC, AND REQUIRES NO THEORETICAL CALCULATIONS.</p>
<p>First we need the idea of a bootstrap sample. A bootstrap sample is any sample drawn randomly WITH REPLACEMENT from the empirical distribution.</p>
<p>BOOTSTRAP = SAMPLE WITH REPLACEMENT</p>
<p><span class="math inline">\(X^{*}=\{\mbox{n values drawn with replacement from } X\}\)</span></p>
<p>n = size of the bootstrap sample = size of the original dataset</p>
<p>We draw k such bootstrap samples:</p>
<p><span class="math display">\[
X_{1}^{*}=\{\mbox{n values drawn with replacement from } X\}
\]</span> <span class="math display">\[
X_{2}^{*}=\{\mbox{n values drawn with replacement from } X\}
\]</span></p>
<p>etc.</p>
<p><span class="math display">\[
X_{k}^{*}=\{\mbox{n values drawn with replacement from } X\}
\]</span></p>
<p><strong>Important</strong>: Because we are sampling WITH REPLACEMENT, some of the original values will be represented more than once in any given bootstrap sample and others not at all.</p>
<p>We calculate our statistic of interest on each bootstrap sample:</p>
<p><span class="math display">\[
\theta_{1}^{*}=f(X_{1}^{*})
\]</span> <span class="math display">\[
\theta_{2}^{*}=f(X_{2}^{*})
\]</span></p>
<p>etc.</p>
<p><span class="math display">\[
\theta_{k}^{*}=f(X_{k}^{*})
\]</span></p>
<p>We will number the different bootstrap sample statistics as</p>
<p><span class="math display">\[
\theta_{1}^{*},θ_{2}^{*},θ_{3}^{*},...,θ_{k}^{*}
\]</span></p>
<p>k = number of bootstrap samples, you can choose the number of bootstrap samples, more sample = better estimates</p>
<p>Now that we have our collection of k bootstrapped estimates of the statistic, what do we do with them?</p>
<p>Remember: The goal was to calculate the bias and standard error of our estimator.</p>
<p><span class="math display">\[
\widehat{Bias_{boot}}=\left( \frac{1}{k}\sum_{i=1}^{k}\theta_{i}^{*}\right)-\hat{\theta}
\]</span></p>
<p>In other words, the Bias of our estimator is simply the mean of the bootstrapped sample statistics minus the statistic as calculated for the original data. (For unbiased estimators, our estimate of bias goes to zero as the sample size n gets very large.)</p>
<p>We can also use these bootstrapped statistics to calculate the standard error of the estimator:</p>
<p><span class="math display">\[
\widehat{se_{boot}}=\sqrt{\frac{1}{k-1}\sum_{i=1}^{k}(\theta_{i}^{*}-\bar{\theta^{*}})^{2}}
\]</span></p>
<p><em>This is just the standard deviation of the distribution of <span class="math inline">\(\theta\)</span>.</em> This is a really important point that is worth dwelling on for a bit. Our uncertainty about the value is captured by how much variation there is when I draw a different sub-sample of the data, which mimics re-doing the experiment altogether. In this case, I call the standard deviation of those <span class="math inline">\(\theta^{*}\)</span> values a standard error, because they represent my uncertainty (my potential error) about <span class="math inline">\(\hat{\theta}\)</span>. Do not confuse standard deviation and standard error. A standard deviation is a statistic (something calculated from data) about the spread of the data. A standard error is the standard deviation of my estimates, and therefore is a measure of how uncertain I am about my estimate.</p>
<p>We will work through 2 examples, one using hand calculations, and one using R pseudocode.</p>
<p><strong>Example 1</strong>:</p>
<p><em>Data</em>: 10 pennies that the students have</p>
<p><em>Test statistic</em>: Median</p>
<p>Lets say we are trying to find the median age of all pennies in circulation. We can’t figure this out exactly, because we can’t collect all the pennies in circulation, but we each have a sample of 10 pennies. The median age of the pennies in our sample is a reasonable estimate for the median age of all pennies in circulation.</p>
<p>What is our uncertainty about that number? How far might our estimate of the median age be from the true median age? In this case, we don’t know the underlying distribution of penny ages. (Let’s brainstorm this for a bit. Do we have any guesses what this distribution might look like? What might be a reasonable distribution to describe the shape of penny age?)</p>
<p>Let’s use bootstrapped samples to calculate the s.e. associated with that estimate.</p>
<p>Procedure: 1. Sample WITH REPLACEMENT a group of 10 pennies. (To sample with replacement you will have to sample one penny, write down the age, and repeat that 10 times.) 2. Calculate the median age from that sample of pennies. 3. Repeat</p>
<p>Over time you will gather a collection of median estimates, each one of which was calculated using a different bootstrapped dataset. They can be used to calculate the Bias and the Variance of the estimator.</p>
<p>We actually have two primary mechanisms for generating confidence intervals for the statistic.</p>
<p>Method #1: We can use the following normal approximation:</p>
<p><span class="math display">\[
\hat{\theta^{*}} \sim N(\hat{\theta},\hat{se}^{2})
\]</span></p>
<p><strong>STOP</strong>: How do we construct a confidence interval from this?</p>
<p><span class="math display">\[
\hat{\theta}_{LL}=\hat{\theta}-1.96*\hat{se}
\]</span> <span class="math display">\[
\hat{\theta}_{UL}=\hat{\theta}+1.96*\hat{se}
\]</span></p>
<p>Remember that 95<span class="math inline">\(\%\)</span> of the probability for a Standard Normal distribution lies between (-1.96<span class="math inline">\(\sigma\)</span>,+1.96<span class="math inline">\(\sigma\)</span>)? Here we are using the same principle, capturing 95<span class="math inline">\(\%\)</span> of the probability of the distribution by assuming the distribution of <span class="math inline">\(\theta^{*}\)</span> is Normal and pulling out the lower limit and the upper limit lying 1.95 times the standard deviation below and above (respectively) the estimate.</p>
<p><strong>OR</strong></p>
<p>Method #2: we could do away with normal approximations altogether and simply take the quantiles directly from the distribution of <span class="math inline">\(\hat{\theta}^{*}\)</span>:</p>
<p><span class="math display">\[
\theta_{LL} = \mbox{2.5th percentile of } \hat{\theta}^{*}
\]</span> <span class="math display">\[
\theta_{UL} = \mbox{97.5th percentile of } \hat{\theta}^{*}
\]</span></p>
<p>Notice that (by construction) 95<span class="math inline">\(%\)</span> of the <span class="math inline">\(\hat{\theta}^{*}\)</span> values fall in the interval <span class="math inline">\((\theta_{LL},\theta_{UL})\)</span>.</p>
<p>NB: If you are going to go through the trouble of doing the bootstrap sampling, I don’t know why you would make a normal approximation at the very end to construct the CIs. I recommend Method #2.</p>
<p><strong>Example 2</strong>:</p>
<p>Knowing how to draw bootstrap replicates gets more complicated when you have multivariate datasets. For example, lets start with a dataset comparing average LSAT scores and GPA for the incoming classes for 15 law schools</p>
<div class="figure">
<img src="LSAT_info.png" />

</div>
<p>Lets say we want to estimate the true correlation coefficient between LSAT scores and GPA. We haven’t covered this yet, but one estimator for the true correlation coefficient is Pearson’s product moment correlation coefficient r</p>
<p><span class="math display">\[
r=\frac{cov(a,b)}{\sqrt{var(a)×var(b)}}
\]</span></p>
<p>Therefore, in this case</p>
<p><span class="math display">\[
\hat{r} = \frac{cov(LSAT,GPA)}{\sqrt{var(LSAT)*var(GPA)}}
\]</span></p>
<p>(In R, we would write this as r.est = cor.test(LSAT,GPA)$estimate.)</p>
<p>If LSAT and GPA both come from a normal distribution, then we could use the theory of normal distributions to calculate the standard error of <span class="math inline">\(\hat{r}\)</span>. (We will learn this in Week 9.) But, we know LSAT and GPA can’t be from normal distributions. At the very least, GPA is bounded on (0,4), so it cannot be Normally distributed. So, how do we calculate the standard error of <span class="math inline">\(\hat{r}\)</span>?</p>
<p>Here we sample with replacement from the bivariate PAIRS of data. In other words, we sample</p>
<p><span class="math display">\[
X_{1}^{*}=(LSAT_{i},GPA_{i}), \mbox{where i=sample with replacement 1...n}
\]</span> <span class="math display">\[
X_{2}^{*}=(LSAT_{i},GPA_{i}), \mbox{where i=sample with replacement 1...n}
\]</span></p>
<p>and so forth, and then calculate the correlation of each simulated dataset.</p>
<p><strong>Question</strong>: Why not sample with replacement from the two datasets independently? What question would that be answering?</p>
<p>If we do this many times, say k=10,000 times, then we can draw a histogram of these bootstrapped correlation coefficients.</p>
<p>We can calculate the standard error of our estimate for the correlation coefficient</p>
<p><span class="math display">\[
\hat{se}_{boot} = \sqrt{\left(\frac{1}{k-1}\right)\sum_{i=1}^{k}(r_{i}^{*}-\bar{r^{*}})^{2}}
\]</span></p>
<p>Therefore, using R, we would calculate the parametric correlation coefficient as: r.est ± 1.96*s.e.boot (VERSION 1)</p>
<p>Even better, we can calculate the 95th percentile confidence interval of this distribution: quantile(all.cor,c(0.025,0.975)) (VERSION 2)</p>
<p>Note that while VERSION 1 is common, VERSION 2 is preferred because there is no guarantee that the distribution of bootstrap statistics is even vaguely Normal.</p>
<p>Bootstrapping can deal with even more complex cases, and is particularly useful when dealing with spatial or temporal autocorrelation. Take for instance a time series of hormone levels:</p>
<div class="figure">
<img src="Hormone_timeseries.png" />

</div>
<p>If you wanted to do some time series analysis of this data, say to calculate the correlation between each datapoint and the last datapoint, you would have a difficult time doing so because of the complex temporal autocorrelation. Bootstrap can help in this case, but its not at all obvious how to bootstrap from this time series and preserve the essential temporal autocorrelation structure of the data. One approach would be to do a “moving blocks” bootstrap.</p>
<div class="figure">
<img src="Moving_blocks.png" />

</div>
<p>This is more advanced, but it makes the point that a) bootstrap can be enormously useful in a variety of complicated analyses and b) you have to think carefully about what to sample in order to preserve the essential element of the data.</p>
<p>R has numerous functions for doing bootstrapping, although bootstrapping is so easy its often just as easy (and more transparent) to simply write your own code to do it. We will go over some examples in lab.</p>
<p>Note that the procedure we have described is called the non-parametric bootstrap estimate because it is based only on the non-parametric empirical distribution G ̂. If we had assumed some kind of distributional form for G, it would be considered a parametric bootstrap.</p>
</div>
<div id="parametric-bootstrap" class="section level2">
<h2><span class="header-section-number">3.5</span> Parametric bootstrap</h2>
<p>The parametric bootstrap is similar to the non-parametric bootstrap except that instead of drawing our bootstrap samples from the original data, we fit a distribution to the data first, and then draw our samples from that. We haven’t covered how to fit a distribution to data yet, nor have we introduced any of the univariate distributions, so I won’t show you how to do a parametric bootstrap now but we’ll get some practice in the Week 3 problem set.</p>
<p>Why would we ever do a parametric bootstrap? We might use a parametric distribution if our original sample size was so small that we did not think it could “stand in” for the underlying parametric distribution. For example, if your dataset for coin age just so happens not to have any coins made in 1990, you may be uncomfortable having all your bootstrapped datasets also be missing coins made in 1990. (Remember: Bootstrapping is, in some way, supposed to mimic redoing your experiment. Do you really think that you’d never get a coin made in 1990?) To get around this problem, you might do a parametric bootstrap. Note that, if you use MLEs to get the parameters for the parametric bootstrap, those parameter estimates assume large sample sizes (the formula are asymptotically correct for large sample sizes) and so you have to be a little caution that your parametric bootstrap might not be capturing the true underlying distribution. While parametric bootstrap is often done when sample sizes are too small, occasionally it may also be used when you have some strong theoretical justification for a particular distribution but the statistics you are interested in have no simple formula. (In other words, maybe the distribution is known, but the statistical properties of the specific parameter you are interested in is not known but could be derived through parametric bootstrapping.)</p>
</div>
<div id="jackknife" class="section level2">
<h2><span class="header-section-number">3.6</span> Jackknife</h2>
<p>Jackknifing is another method of assessing bias and standard error of sample statistics. Jackknife can also be used to establish the influence of each datapoint in your dataset. The procedure simply involves leaving out each datapoint and recalculating the statistic of interest.</p>
<p>If your dataset involves the set</p>
<p><span class="math display">\[
\{x_{1},x_{2},x_{3}\}
\]</span></p>
<p>then the jackknife samples are</p>
<p><span class="math display">\[
\{x_{1},x_{2}\},\{x_{1},x_{3}\},\{x_{2},x_{3}\}
\]</span></p>
<p>The traditional notation is that the estimate based on the dataset when the ith element is removed is (<span class="math inline">\(\widehat{\theta_{(i)}}\)</span>).</p>
<p>The jackknife estimate of bias is given by</p>
<p><span class="math display">\[
\widehat{Bias_{jack}}=(n-1)(\hat{\theta_{(.)}}-\hat{\theta})
\]</span> where</p>
<p><span class="math display">\[
\hat{\theta}_{(.)}=\frac{1}{n}\sum_{i=1}^{n}\hat{\theta}_{(i)}
\]</span></p>
<p>You can convince yourself of this formula by working out the case where <span class="math inline">\(\hat{\theta}\)</span> is the mean. You can also see intuitively why you would have to multiply the jackknife estimate of bias by (n-1) since the deviation of the jackknifed samples from the full sample is much smaller than the standard deviation of the bootstrapped samples. (<strong>DOES EVERYONE SEE WHY?</strong>)</p>
<p>The jackknife estimate of standard error is given by</p>
<p><span class="math display">\[
\hat{se}_{jack}=\sqrt{\frac{n-1}{n}\sum_{i=1}^{n}(\hat{\theta}_{(i)}-\hat{\theta}_{(.)})^{2}}
\]</span></p>
<p>With the pennies example, we had 10 pennies and we have only 10 possible jackknifed samples. Do you see why? Note that while bootstrapping can involve simulating an arbitrarily large number of pseudosamples (k), there are only n possible jackknife replicates for a dataset of size n.</p>
<p><strong>Exercise</strong>: Use your pennies to calculate <span class="math inline">\(\widehat{Bias}_{jack}\)</span> and <span class="math inline">\(\widehat{se}_{jack}\)</span>.</p>
<p>Both bootstrap and jackknife can estimate the standard error of a statistic, and in this way, their use can often be interchangeable. However, the jackknife can ONLY compute the bias and standard error whereas the bootstrap calculates the entire distribution of the statistic from which the standard error can be inferred. Bootstrapping is often more computer intensive, but with modern computers this is hardly a drawback.</p>
</div>
<div id="jackknife-after-bootstrap" class="section level2">
<h2><span class="header-section-number">3.7</span> Jackknife-after-bootstrap</h2>
<p>Jackknife-after-bootstrap is one method of assessing the standard error of bootstrap statistics. For example, jackknife-after-bootstrap can give us the standard error of the bootstrap standard error:</p>
<p><span class="math display">\[
\widehat{se}_{jack}(\widehat{se}_{boot})
\]</span></p>
<p>To do this there are two steps:</p>
<ol style="list-style-type: decimal">
<li>Leave out data point i and use the remaining data in a bootstrap analysis to calculate (s.e.) ̂_(boot(i))</li>
<li>Define</li>
</ol>
<p><span class="math display">\[
\widehat{se}_{jack}(\widehat{se}_{boot})=\sqrt{\left(\frac{n-1}{n}\right)\sum_{i=1}^{n}(\hat{se}_{boot(i)}-\hat{se}_{boot(.)})^{2}}
\]</span></p>
<p>Notice that because there are always some bootstrap samples that do not include i, you do not actually have to do any extra computation to do jackknife-after-bootstrap, but the precise details of using the bootstrap samples you already have are a bit complicated.</p>
<p>In R, this can be done using the ‘jack.after.boot’ function in the package “boot”.</p>
<p><strong>Discuss: Why are hypothesis testing and parameter estimation two sides of the same coin?</strong></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="week-1-lab-handout.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="week-2-lab.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
